{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ThreatMatrix Centralized Documentation","text":"<p>Welcome to the ThreatMatrix Centralized Documentation. Here you will be able to find all documentation for all projects under ThreatMatrix.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Intel Owl is an Open Source Intelligence, or OSINT solution, to get Threat Intelligence data about a specific digital artifact from a single API at scale. It integrates a high number of services available online and a lot of cutting-edge malware analysis tools. It is for everyone who needs a single point to query for info about a specific file or observable. If you are a Security Analyst, do not waste any more time in performing enrichment tasks! ThreatMatrix saves your time and allows you to concentrate on more serious tasks.</p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li> Introduction  More</li> </ul> <ul> <li> Installation  More</li> </ul> <ul> <li> Contribute  More</li> </ul> <ul> <li> Usage  More</li> </ul> <ul> <li> Advanced Usage  More</li> </ul> <ul> <li> Advanced Configuration  More</li> </ul> <p>Need more help?</p> <p>We are doing our best to keep this documentation complete, accurate and up to date.</p> <p>If you still have questions or you find something which is not sufficiently explained, join the ThreatMatrix channel under HoneyNet Community on Slack.</p> <p> </p>"},{"location":"Guide-docstrings/","title":"Docstrings guide","text":""},{"location":"Guide-docstrings/#implementing-docstrings-in-python-code","title":"Implementing Docstrings in Python Code","text":"<p>When you write or modify Python code in the codebase, it's important to add or update the docstrings accordingly. If you wish to display these docstrings in the documentation, follow these steps.</p> <p>Suppose the docstrings are located in the following path: <code>docs/Submodules/ThreatMatrix/api_app/analyzers_manager/classes</code>, and you want to show the description of a class, such as BaseAnalyzerMixin.</p> <p>To include this in the documentation, use the following command:</p> <pre><code>:::docs.Submodules.ThreatMatrix.api_app.analyzers_manager.classes.BaseAnalyzerMixin\n</code></pre> <p>Warning</p> Make sure your path is correct and syntax is correct. If you face any issues even path is correct then read the Submodules Guide."},{"location":"Guide-docstrings/#this-is-how-it-would-look-in-documentation","title":"This is how it would look in documentation:","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract Base class for Analyzers. Never inherit from this branch, always use either one of ObservableAnalyzer or FileAnalyzer classes.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/analyzers_manager/classes.py</code> <pre><code>class BaseAnalyzerMixin(Plugin, metaclass=ABCMeta):\n    \"\"\"\n    Abstract Base class for Analyzers.\n    Never inherit from this branch,\n    always use either one of ObservableAnalyzer or FileAnalyzer classes.\n    \"\"\"\n\n    HashChoices = HashChoices\n    ObservableTypes = ObservableTypes\n    TypeChoices = TypeChoices\n\n    @classmethod\n    @property\n    def config_exception(cls):\n        \"\"\"Returns the AnalyzerConfigurationException class.\"\"\"\n        return AnalyzerConfigurationException\n\n    @property\n    def analyzer_name(self) -&gt; str:\n        \"\"\"Returns the name of the analyzer.\"\"\"\n        return self._config.name\n\n    @classmethod\n    @property\n    def report_model(cls):\n        \"\"\"Returns the AnalyzerReport model.\"\"\"\n        return AnalyzerReport\n\n    @classmethod\n    @property\n    def config_model(cls):\n        \"\"\"Returns the AnalyzerConfig model.\"\"\"\n        return AnalyzerConfig\n\n    def get_exceptions_to_catch(self):\n        \"\"\"\n        Returns additional exceptions to catch when running *start* fn\n        \"\"\"\n        return (\n            AnalyzerConfigurationException,\n            AnalyzerRunException,\n        )\n\n    def _validate_result(self, result, level=0, max_recursion=190):\n        \"\"\"\n        function to validate result, allowing to store inside postgres without errors.\n\n        If the character \\u0000 is present in the string, postgres will throw an error\n\n        If an integer is bigger than max_int,\n        Mongodb is not capable to store and will throw an error.\n\n        If we have more than 200 recursion levels, every encoding\n        will throw a maximum_nested_object exception\n        \"\"\"\n        if level == max_recursion:\n            logger.info(\n                f\"We have reached max_recursion {max_recursion} level. \"\n                f\"The following object will be pruned {result} \"\n            )\n            return None\n        if isinstance(result, dict):\n            for key, values in result.items():\n                result[key] = self._validate_result(\n                    values, level=level + 1, max_recursion=max_recursion\n                )\n        elif isinstance(result, list):\n            for i, _ in enumerate(result):\n                result[i] = self._validate_result(\n                    result[i], level=level + 1, max_recursion=max_recursion\n                )\n        elif isinstance(result, str):\n            return result.replace(\"\\u0000\", \"\")\n        elif isinstance(result, int) and result &gt; 9223372036854775807:  # max int 8bytes\n            result = 9223372036854775807\n        return result\n\n    def after_run_success(self, content):\n        \"\"\"\n        Handles actions after a successful run.\n\n        Args:\n            content (any): The content to process after a successful run.\n        \"\"\"\n        super().after_run_success(self._validate_result(content, max_recursion=15))\n</code></pre>"},{"location":"Guide-docstrings/#docs.Submodules.ThreatMatrix.api_app.analyzers_manager.classes.BaseAnalyzerMixin.analyzer_name","title":"<code>analyzer_name: str</code>  <code>property</code>","text":"<p>Returns the name of the analyzer.</p>"},{"location":"Guide-docstrings/#docs.Submodules.ThreatMatrix.api_app.analyzers_manager.classes.BaseAnalyzerMixin.config_exception","title":"<code>config_exception</code>  <code>classmethod</code> <code>property</code>","text":"<p>Returns the AnalyzerConfigurationException class.</p>"},{"location":"Guide-docstrings/#docs.Submodules.ThreatMatrix.api_app.analyzers_manager.classes.BaseAnalyzerMixin.config_model","title":"<code>config_model</code>  <code>classmethod</code> <code>property</code>","text":"<p>Returns the AnalyzerConfig model.</p>"},{"location":"Guide-docstrings/#docs.Submodules.ThreatMatrix.api_app.analyzers_manager.classes.BaseAnalyzerMixin.report_model","title":"<code>report_model</code>  <code>classmethod</code> <code>property</code>","text":"<p>Returns the AnalyzerReport model.</p>"},{"location":"Guide-docstrings/#docs.Submodules.ThreatMatrix.api_app.analyzers_manager.classes.BaseAnalyzerMixin.after_run_success","title":"<code>after_run_success(content)</code>","text":"<p>Handles actions after a successful run.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>any</code> <p>The content to process after a successful run.</p> required Source code in <code>docs/Submodules/ThreatMatrix/api_app/analyzers_manager/classes.py</code> <pre><code>def after_run_success(self, content):\n    \"\"\"\n    Handles actions after a successful run.\n\n    Args:\n        content (any): The content to process after a successful run.\n    \"\"\"\n    super().after_run_success(self._validate_result(content, max_recursion=15))\n</code></pre>"},{"location":"Guide-docstrings/#docs.Submodules.ThreatMatrix.api_app.analyzers_manager.classes.BaseAnalyzerMixin.get_exceptions_to_catch","title":"<code>get_exceptions_to_catch()</code>","text":"<p>Returns additional exceptions to catch when running start fn</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/analyzers_manager/classes.py</code> <pre><code>def get_exceptions_to_catch(self):\n    \"\"\"\n    Returns additional exceptions to catch when running *start* fn\n    \"\"\"\n    return (\n        AnalyzerConfigurationException,\n        AnalyzerRunException,\n    )\n</code></pre>"},{"location":"Guide-documentation/","title":"Setting Up the New Documentation Site Locally","text":"<p>To set up and run the documentation site on your local machine, please follow the steps below:</p>"},{"location":"Guide-documentation/#1-create-a-virtual-environment","title":"1. Create a Virtual Environment","text":"<p>To create a virtual environment named <code>venv</code> in your project directory, use the following command:</p> <pre><code>python3 -m venv venv\n</code></pre>"},{"location":"Guide-documentation/#2-activate-the-virtual-environment","title":"2. Activate the Virtual Environment","text":"<p>Activate the virtual environment to ensure that all dependencies are installed locally within your project directory.</p> <p>On Linux/MacOS:</p> <pre><code>source venv/bin/activate\n</code></pre> <p>On Windows:</p> <pre><code>venv\\Scripts\\activate\n</code></pre>"},{"location":"Guide-documentation/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>To install all the necessary Python packages listed in requirements.txt, run:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Please run these commands to update and fetch the local Submodules.</p> <pre><code>git submodule foreach --recursive 'git fetch --all'\ngit submodule update --init --remote --recursive --depth 1\ngit submodule sync --recursive\ngit submodule update --remote --recursive\n</code></pre>"},{"location":"Guide-documentation/#4-serve-the-documentation-locally","title":"4. Serve the Documentation Locally","text":"<p>Start a local development server to preview the documentation in your web browser. The server will automatically reload whenever you make changes to the documentation files.</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"Guide-documentation/#5-make-changes-and-review","title":"5. Make Changes and Review","text":"<p>As you edit the documentation, you can view your changes in real-time through the local server. This step ensures everything looks as expected before deploying.</p>"},{"location":"Guide-documentation/#6-push-changes-to-github","title":"6. Push Changes to GitHub","text":"<p>Once you are satisfied with your changes, commit and push them to the GitHub repository. The documentation will be automatically deployed via GitHub Actions, making it live on the documentation site.</p>"},{"location":"Submodules/","title":"Submodules","text":""},{"location":"Submodules/#implementing-docstrings-in-threatmatrix-documentation","title":"Implementing Docstrings in ThreatMatrix Documentation","text":"<p>In the ThreatMatrix documentation site, we use Git submodules to manage multiple repositories as child repositories. This allows us to fetch updated code (including docstrings and API specs) automatically, reducing redundant work for developers.</p>"},{"location":"Submodules/#current-submodules","title":"Current Submodules","text":"<p>There are four submodules under the khulnasoft:</p> <ol> <li>ThreatMatrix</li> <li>ThreatPot</li> <li>pythreatmatrix</li> <li>GoThreatMatrix</li> </ol> <p>These submodules are updated whenever we push new changes to our documentation site, here's the Github Action file.</p>"},{"location":"Submodules/#making-changes-to-documentation","title":"Making Changes to Documentation","text":"<p>When you make changes to the ThreatMatrix codebase, it typically does not update automatically in the github repository of documentation site.</p> <p>While development if you want to update the submodules to latest changes you can do the following:</p> <pre><code>git submodule foreach --recursive 'git fetch --all'\ngit submodule update --init --remote --recursive --depth 1\ngit submodule sync --recursive\ngit submodule update --remote --recursive\n</code></pre> <p>However, if you need to test changes immediately, you can do the following:</p>"},{"location":"Submodules/#add-custom-submodules-for-testing","title":"Add Custom Submodules for Testing:","text":"<p>Point the submodule in <code>.gitmodules</code> to your fork of the repository to check the updates instantly.</p>"},{"location":"Submodules/#update-submodules","title":"Update Submodules:","text":"<p>After modifying <code>.gitmodules</code>, run the following command to fetch the latest changes:</p> <pre><code>git submodule update --remote --merge\n</code></pre> <p>This ensures that your documentation reflects the most recent code changes.</p>"},{"location":"GoThreatMatrix/","title":"index","text":"<p> Go-ThreatMatrix Repository</p>"},{"location":"GoThreatMatrix/#go-threatmatrix","title":"go-threatmatrix","text":"<p> go-threatmatrix is a client library/SDK that allows developers to easily automate and integrate ThreatMatrix with their own set of tools!</p>"},{"location":"GoThreatMatrix/#table-of-contents","title":"Table of Contents","text":"<ul> <li>go-threatmatrix</li> <li>Getting Started<ul> <li>Pre requisites</li> <li>Installation</li> <li>Usage</li> <li>Examples</li> </ul> </li> <li>Contribute</li> <li>License</li> <li>Links</li> <li>FAQ<ul> <li>Generate API key<ul> <li>v4.0 and above</li> <li>v4.0 below</li> </ul> </li> </ul> </li> </ul>"},{"location":"GoThreatMatrix/#getting-started","title":"Getting Started","text":""},{"location":"GoThreatMatrix/#pre-requisites","title":"Pre requisites","text":"<ul> <li>Go 1.17+</li> </ul>"},{"location":"GoThreatMatrix/#installation","title":"Installation","text":"<p>Use go get to retrieve the SDK to add it to your GOPATH workspace, or project's Go module dependencies.</p> <pre><code>$ go get github.com/khulnasoft/go-threatmatrix\n</code></pre>"},{"location":"GoThreatMatrix/#usage","title":"Usage","text":"<p>This library was built with ease of use in mind! Here are some quick examples to get you started. If you need more example you can go to the examples directory</p> <p>To start using the go-threatmatrix library you first need to import it:</p> <pre><code>import \"github.com/khulnasoft/go-threatmatrix/gothreatmatrix\"\n</code></pre> <p>Construct a new <code>ThreatMatrixClient</code>, then use the various services to easily access different parts of Threatmatrix's REST API. Here's an example of getting all jobs:</p> <pre><code>clientOptions := gothreatmatrix.ThreatMatrixClientOptions{\n    Url:         \"your-cool-URL-goes-here\",\n    Token:       \"your-super-secret-token-goes-here\",\n    // This is optional\n    Certificate: \"your-optional-certificate-goes-here\",\n}\n\nthreatmatrix := gothreatmatrix.NewThreatMatrixClient(\n    &amp;clientOptions,\n    nil\n)\n\nctx := context.Background()\n\n// returns *[]Jobs or an ThreatMatrixError!\njobs, err := threatmatrix.JobService.List(ctx)\n</code></pre> <p>For easy configuration and set up we opted for <code>options</code> structs. Where we can customize the client API or service endpoint to our liking! For more information go here. Here's a quick example!</p> <pre><code>// ...Making the client and context!\n\ntagOptions = gothreatmatrix.TagParams{\n  Label: \"NEW TAG\",\n  Color: \"#ffb703\",\n}\n\ncreatedTag, err := threatmatrix.TagService.Create(ctx, tagOptions)\nif err != nil {\n    fmt.Println(err)\n} else {\n    fmt.Println(createdTag)\n}\n</code></pre>"},{"location":"GoThreatMatrix/#examples","title":"Examples","text":"<p>The examples directory contains a couple for clear examples, of which one is partially listed here as well:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/khulnasoft/go-threatmatrix/gothreatmatrix\"\n)\n\nfunc main(){\n    threatmatrixOptions := gothreatmatrix.ThreatMatrixClientOptions{\n        Url:         \"your-cool-url-goes-here\",\n        Token:       \"your-super-secret-token-goes-here\",\n        Certificate: \"your-optional-certificate-goes-here\",\n    }\n\n    client := gothreatmatrix.NewThreatMatrixClient(\n        &amp;threatmatrixOptions,\n        nil,\n    )\n\n    ctx := context.Background()\n\n    // Get User details!\n    user, err := client.UserService.Access(ctx)\n    if err != nil {\n        fmt.Println(\"err\")\n        fmt.Println(err)\n    } else {\n        fmt.Println(\"USER Details\")\n        fmt.Println(*user)\n    }\n}\n</code></pre> <p>For complete usage of go-threatmatrix, see the full package docs.</p>"},{"location":"GoThreatMatrix/#contribute","title":"Contribute","text":"<p>If you want to follow the updates, discuss, contribute, or just chat then please join our slack channel we'd love to hear your feedback!</p>"},{"location":"GoThreatMatrix/#license","title":"License","text":"<p>Licensed under the GNU AFFERO GENERAL PUBLIC LICENSE.</p>"},{"location":"GoThreatMatrix/#links","title":"Links","text":"<ul> <li>Threatmatrix</li> <li>Documentation</li> <li>API documentation</li> <li>Examples</li> </ul>"},{"location":"GoThreatMatrix/#faq","title":"FAQ","text":""},{"location":"GoThreatMatrix/#generate-api-key","title":"Generate API key","text":"<p>You need a valid API key to interact with the ThreatMatrix server.</p>"},{"location":"GoThreatMatrix/#v40-and-above","title":"v4.0 and above","text":"<p>You can get an API by doing the following:</p> <ol> <li>Log / Signin into threatmatrix</li> <li>At the upper right click on your profile from the drop down select <code>API Access/ Sessions</code></li> <li>Then generate an API key or see it!</li> </ol>"},{"location":"GoThreatMatrix/#v40-below","title":"v4.0 below","text":"<p>Keys should be created from the admin interface of ThreatMatrix: you have to go in the Durin section (click on <code>Auth tokens</code>) and generate a key there.</p>"},{"location":"Submodules/CyberPot/","title":"CyberPot - The All In One Multi Honeypot Platform","text":"<p>CyberPot is the all in one, optionally distributed, multiarch (amd64, arm64) honeypot plattform, supporting 20+ honeypots and countless visualization options using the Elastic Stack, animated live attack maps and lots of security tools to further improve the deception experience. </p>"},{"location":"Submodules/CyberPot/#tldr","title":"TL;DR","text":"<ol> <li>Meet the system requirements. The CyberPot installation needs at least 8-16 GB RAM, 128 GB free disk space as well as a working (outgoing non-filtered) internet connection.</li> <li>Download or use a running, supported distribution.</li> <li>Install the ISO with as minimal packages / services as possible (<code>ssh</code> required)</li> <li>Install <code>curl</code>: <code>$ sudo [apt, dnf, zypper] install curl</code> if not installed already</li> <li>Run installer as non-root from <code>$HOME</code>:</li> </ol> <pre><code>env bash -c \"$(curl -sL https://github.com/khulnasoft/cyberpot/raw/master/install.sh)\"\n</code></pre> <ul> <li>Follow instructions, read messages, check for possible port conflicts and reboot</li> </ul> <ul> <li>CyberPot - The All In One Multi Honeypot Platform</li> <li>TL;DR</li> <li>Disclaimer</li> <li>Technical Concept<ul> <li>Technical Architecture</li> <li>Services</li> <li>User Types</li> </ul> </li> <li>System Requirements<ul> <li>Running in a VM</li> <li>Running on Hardware</li> <li>Running in a Cloud</li> <li>Required Ports</li> </ul> </li> <li>System Placement</li> <li>Installation<ul> <li>Choose your distro</li> <li>Raspberry Pi 4 (8GB) Support</li> <li>Get and install CyberPot</li> <li>macOS &amp; Windows</li> <li>Installation Types<ul> <li>Standard / HIVE</li> <li>Distributed</li> </ul> </li> <li>Uninstall CyberPot</li> </ul> </li> <li>First Start<ul> <li>Standalone First Start</li> <li>Distributed Deployment<ul> <li>Planning and Certificates</li> <li>Deploying Sensors</li> </ul> </li> <li>Community Data Submission</li> <li>Opt-In HPFEEDS Data Submission</li> </ul> </li> <li>Remote Access and Tools<ul> <li>SSH</li> <li>CyberPot Landing Page</li> <li>Kibana Dashboard</li> <li>Attack Map</li> <li>Cyberchef</li> <li>Elasticvue</li> <li>Spiderfoot</li> </ul> </li> <li>Configuration<ul> <li>CyberPot Config File</li> <li>Customize CyberPot Honeypots and Services</li> </ul> </li> <li>Maintenance<ul> <li>General Updates</li> <li>Update Script</li> <li>Daily Reboot</li> <li>Known Issues<ul> <li>Docker Images Fail to Download</li> <li>CyberPot Networking Fails</li> </ul> </li> <li>Start CyberPot</li> <li>Stop CyberPot</li> <li>CyberPot Data Folder</li> <li>Log Persistence</li> <li>Factory Reset</li> <li>Show Containers</li> <li>Blackhole</li> <li>Add Users to Nginx (CyberPot WebUI)</li> <li>Import and Export Kibana Objects<ul> <li>Export</li> <li>Import</li> </ul> </li> </ul> </li> <li>Troubleshooting<ul> <li>Logs</li> <li>RAM and Storage</li> </ul> </li> <li>Contact<ul> <li>Issues</li> <li>Discussions</li> </ul> </li> <li>Licenses</li> <li>Credits<ul> <li>The developers and development communities of</li> </ul> </li> <li> <p>Testimonials </p> <p></p> </li> </ul>"},{"location":"Submodules/CyberPot/#disclaimer","title":"Disclaimer","text":"<ul> <li>You install and run CyberPot within your responsibility. Choose your deployment wisely as a system compromise can never be ruled out.</li> <li>For fast help research the Issues and Discussions.</li> <li>The software is designed and offered with best effort in mind. As a community and open source project it uses lots of other open source software and may contain bugs and issues. Report responsibly.</li> <li>Honeypots - by design - should not host any sensitive data. Make sure you don't add any.</li> <li>By default, your data is submitted to Sicherheitstacho. You can disable this in the config (<code>~/cyberpot/docker-compose.yml</code>) by removing the <code>ewsposter</code> section. But in this case sharing really is caring!   </li> </ul>"},{"location":"Submodules/CyberPot/#technical-concept","title":"Technical Concept","text":"<p>CyberPot's main components have been moved into the <code>cyberpotinit</code> Docker image allowing CyberPot to now support multiple Linux distributions, even macOS and Windows (although both limited to the feature set of Docker Desktop). CyberPot uses docker and docker compose to reach its goal of running as many honeypots and tools as possible simultaneously and thus utilizing the host's hardware to its maximum. </p> <p>CyberPot offers docker images for the following honeypots ...</p> <ul> <li>adbhoney,</li> <li>ciscoasa,</li> <li>citrixhoneypot,</li> <li>conpot,</li> <li>cowrie,</li> <li>ddospot,</li> <li>dicompot,</li> <li>dionaea,</li> <li>elasticpot,</li> <li>endlessh,</li> <li>glutton,</li> <li>hellpot,</li> <li>heralding,</li> <li>honeypots,</li> <li>honeytrap,</li> <li>ipphoney,</li> <li>log4pot,</li> <li>mailoney,</li> <li>medpot,</li> <li>redishoneypot,</li> <li>sentrypeer,</li> <li>snare,</li> <li>tanner,</li> <li>wordpot</li> </ul> <p>... alongside the following tools ...</p> <ul> <li>Autoheal a tool to automatically restart containers with failed healthchecks.</li> <li>Cyberchef a web app for encryption, encoding, compression and data analysis.</li> <li>Elastic Stack to beautifully visualize all the events captured by CyberPot.</li> <li>Elasticvue a web front end for browsing and interacting with an Elasticsearch cluster.</li> <li>Fatt a pyshark based script for extracting network metadata and fingerprints from pcap files and live network traffic.</li> <li>CyberPot-Attack-Map a beautifully animated attack map for CyberPot.</li> <li>P0f is a tool for purely passive traffic fingerprinting.</li> <li>Spiderfoot an open source intelligence automation tool.</li> <li>Suricata a Network Security Monitoring engine.</li> </ul> <p>... to give you the best out-of-the-box experience possible and an easy-to-use multi-honeypot system. </p>"},{"location":"Submodules/CyberPot/#technical-architecture","title":"Technical Architecture","text":"<p>The source code and configuration files are fully stored in the CyberPot GitHub repository. The docker images are built and preconfigured for the CyberPot environment.</p> <p>The individual Dockerfiles and configurations are located in the docker folder. </p>"},{"location":"Submodules/CyberPot/#services","title":"Services","text":"<p>CyberPot offers a number of services which are basically divided into five groups:</p> <ol> <li>System services provided by the OS<ul> <li>SSH for secure remote access.</li> </ul> </li> <li>Elastic Stack<ul> <li>Elasticsearch for storing events.</li> <li>Logstash for ingesting, receiving and sending events to Elasticsearch.</li> <li>Kibana for displaying events on beautifully rendered dashboards.</li> </ul> </li> <li>Tools<ul> <li>NGINX provides secure remote access (reverse proxy) to Kibana, CyberChef, Elasticvue, GeoIP AttackMap, Spiderfoot and allows for CyberPot sensors to securely transmit event data to the CyberPot hive.</li> <li>CyberChef a web app for encryption, encoding, compression and data analysis.</li> <li>Elasticvue a web front end for browsing and interacting with an Elasticsearch cluster.</li> <li>CyberPot Attack Map a beautifully animated attack map for CyberPot.</li> <li>Spiderfoot an open source intelligence automation tool.</li> </ul> </li> <li>Honeypots<ul> <li>A selection of the 23 available honeypots based on the selected <code>docker-compose.yml</code>.</li> </ul> </li> <li>Network Security Monitoring (NSM)    _ Fatt a pyshark based script for extracting network metadata and fingerprints from pcap files and live network traffic.    _ P0f is a tool for purely passive traffic fingerprinting. * Suricata a Network Security Monitoring engine.    </li> </ol>"},{"location":"Submodules/CyberPot/#user-types","title":"User Types","text":"<p>During the installation and during the usage of CyberPot there are two different types of accounts you will be working with. Make sure you know the differences of the different account types, since it is by far the most common reason for authentication errors.</p> Service Account Type Username / Group Description SSH OS <code>&lt;OS_USERNAME&gt;</code> The user you chose during the installation of the OS. Nginx BasicAuth <code>&lt;WEB_USER&gt;</code> <code>&lt;web_user&gt;</code> you chose during the installation of CyberPot. CyberChef BasicAuth <code>&lt;WEB_USER&gt;</code> <code>&lt;web_user&gt;</code> you chose during the installation of CyberPot. Elasticvue BasicAuth <code>&lt;WEB_USER&gt;</code> <code>&lt;web_user&gt;</code> you chose during the installation of CyberPot. Geoip Attack Map BasicAuth <code>&lt;WEB_USER&gt;</code> <code>&lt;web_user&gt;</code> you chose during the installation of CyberPot. Spiderfoot BasicAuth <code>&lt;WEB_USER&gt;</code> <code>&lt;web_user&gt;</code> you chose during the installation of CyberPot. CyberPot OS <code>cyberpot</code> <code>cyberpot</code> this user / group is always reserved by the CyberPot services. CyberPot Logs BasicAuth <code>&lt;LS_WEB_USER&gt;</code> <code>LS_WEB_USER</code> are automatically managed. <p></p>"},{"location":"Submodules/CyberPot/#system-requirements","title":"System Requirements","text":"<p>Depending on the supported Linux distro images, hive / sensor, installing on real hardware, in a virtual machine or other environments there are different kind of requirements to be met regarding OS, RAM, storage and network for a successful installation of CyberPot (you can always adjust <code>~/cyberpot/docker-compose.yml</code> and <code>~/cyberpot/.env</code>to your needs to overcome these requirements). </p> CyberPot Type RAM Storage Description Hive 16GB 256GB SSD As a rule of thumb, the more sensors &amp; data, the more RAM and storage is needed. Sensor 8GB 128GB SSD Since honeypot logs are persisted (~/cyberpot/data) for 30 days, storage depends on attack volume. <p>CyberPot does require ...</p> <ul> <li>an IPv4 address via DHCP or statically assigned</li> <li>a working, non-proxied, internet connection   ... for a successful installation and operation.    If you need proxy support or otherwise non-standard features, you should check the docs of the supported Linux distro images and / or the Docker documentation. </li> </ul>"},{"location":"Submodules/CyberPot/#running-in-a-vm","title":"Running in a VM","text":"<p>All of the supported Linux distro images will run in a VM which means CyberPot will just run fine. The following were tested / reported to work:</p> <ul> <li>UTM (Intel &amp; Apple Silicon)</li> <li>VirtualBox</li> <li>VMWare Fusion and VMWare Workstation</li> <li>KVM is reported to work as well.</li> </ul> <p>Some configuration / setup hints:</p> <ul> <li>While Intel versions run stable, Apple Silicon (arm64) support has known issues which in UTM may require switching <code>Display</code> to <code>Console Only</code> during initial installation of the OS and afterwards back to <code>Full Graphics</code>.</li> <li>During configuration you may need to enable promiscuous mode for the network interface in order for fatt, suricata and p0f to work properly.</li> <li>If you want to use a wifi card as a primary NIC for CyberPot, please be aware that not all network interface drivers support all wireless cards. In VirtualBox e.g. you have to choose the \"MT SERVER\" model of the NIC.   </li> </ul>"},{"location":"Submodules/CyberPot/#running-on-hardware","title":"Running on Hardware","text":"<p>CyberPot is only limited by the hardware support of the supported Linux distro images. It is recommended to check the HCL (hardware compatibility list) and test the supported distros with CyberPot before investing in dedicated hardware. </p>"},{"location":"Submodules/CyberPot/#running-in-a-cloud","title":"Running in a Cloud","text":"<p>CyberPot is tested on and known to run on ...</p> <ul> <li>Telekom OTC using the post install method   ... others may work, but remain untested.</li> </ul> <p>Some users report working installations on other clouds and hosters, i.e. Azure and GCP. Hardware requirements may be different. If you are unsure you should research issues and discussions and run some functional tests. With CyberPot 24.04.0 and forward we made sure to remove settings that were known to interfere with cloud based installations. </p>"},{"location":"Submodules/CyberPot/#required-ports","title":"Required Ports","text":"<p>Besides the ports generally needed by the OS, i.e. obtaining a DHCP lease, DNS, etc. CyberPot will require the following ports for incoming / outgoing connections. Review the CyberPot Architecture for a visual representation. Also some ports will show up as duplicates, which is fine since used in different editions.</p> Port Protocol Direction Description 80, 443 tcp outgoing CyberPot Management: Install, Updates, Logs (i.e. OS, GitHub, DockerHub, Sicherheitstacho, etc. 64294 tcp incoming CyberPot Management: Sensor data transmission to hive (through NGINX reverse proxy) to 127.0.0.1:64305 64295 tcp incoming CyberPot Management: Access to SSH 64297 tcp incoming CyberPot Management Access to NGINX reverse proxy 5555 tcp incoming Honeypot: ADBHoney 5000 udp incoming Honeypot: CiscoASA 8443 tcp incoming Honeypot: CiscoASA 443 tcp incoming Honeypot: CitrixHoneypot 80, 102, 502, 1025, 2404, 10001, 44818, 47808, 50100 tcp incoming Honeypot: Conpot 161, 623 udp incoming Honeypot: Conpot 22, 23 tcp incoming Honeypot: Cowrie 19, 53, 123, 1900 udp incoming Honeypot: Ddospot 11112 tcp incoming Honeypot: Dicompot 21, 42, 135, 443, 445, 1433, 1723, 1883, 3306, 8081 tcp incoming Honeypot: Dionaea 69 udp incoming Honeypot: Dionaea 9200 tcp incoming Honeypot: Elasticpot 22 tcp incoming Honeypot: Endlessh 21, 22, 23, 25, 80, 110, 143, 443, 993, 995, 1080, 5432, 5900 tcp incoming Honeypot: Heralding 21, 22, 23, 25, 80, 110, 143, 389, 443, 445, 631, 1080, 1433, 1521, 3306, 3389, 5060, 5432, 5900, 6379, 6667, 8080, 9100, 9200, 11211 tcp incoming Honeypot: qHoneypots 53, 123, 161, 5060 udp incoming Honeypot: qHoneypots 631 tcp incoming Honeypot: IPPHoney 80, 443, 8080, 9200, 25565 tcp incoming Honeypot: Log4Pot 25 tcp incoming Honeypot: Mailoney 2575 tcp incoming Honeypot: Medpot 6379 tcp incoming Honeypot: Redishoneypot 5060 tcp/udp incoming Honeypot: SentryPeer 80 tcp incoming Honeypot: Snare (Tanner) 8090 tcp incoming Honeypot: Wordpot <p>Ports and availability of SaaS services may vary based on your geographical location.</p> <p>For some honeypots to reach full functionality (i.e. Cowrie or Log4Pot) outgoing connections are necessary as well, in order for them to download the attacker's malware. Please see the individual honeypot's documentation to learn more by following the links to their repositories.</p> <p></p>"},{"location":"Submodules/CyberPot/#system-placement","title":"System Placement","text":"<p>It is recommended to get yourself familiar with how CyberPot and the honeypots work before you start exposing towards the internet. For a quickstart run a CyberPot installation in a virtual machine.  Once you are familiar with how things work you should choose a network you suspect intruders in or from (i.e. the internet). Otherwise CyberPot will most likely not capture any attacks (unless you want to prove a point)! For starters it is recommended to put CyberPot in an unfiltered zone, where all TCP and UDP traffic is forwarded to CyberPot's network interface. To avoid probing for CyberPot's management ports you should put CyberPot behind a firewall and forward all TCP / UDP traffic in the port range of 1-64000 to CyberPot while allowing access to ports &gt; 64000 only from trusted IPs and / or only expose the ports relevant to your use-case. If you wish to catch malware traffic on unknown ports you should not limit the ports you forward since glutton and honeytrap dynamically bind any TCP port that is not occupied by other honeypot daemons and thus give you a better representation of the risks your setup is exposed to. </p>"},{"location":"Submodules/CyberPot/#installation","title":"Installation","text":"<p>Download one of the supported Linux distro images, follow the TL;DR instructions or <code>git clone</code> the CyberPot repository and run the installer <code>~/cyberpot/install.sh</code>. Running CyberPot on top of a running and supported Linux system is possible, but a clean installation is recommended to avoid port conflicts with running services. The CyberPot installer will require direct access to the internet as described here. </p>"},{"location":"Submodules/CyberPot/#choose-your-distro","title":"Choose your distro","text":"<p>Steps to Follow:</p> <ol> <li>Download a supported Linux distribution from the list below.</li> <li>During installation choose a minimum, netinstall or server version that will only install essential packages.</li> <li>Never install a graphical desktop environment such as Gnome or KDE. CyberPot will fail to work with it due to port conflicts.</li> <li>Make sure to install SSH, so you can connect to the machine remotely.</li> </ol> Distribution Name x64 arm64 Alma Linux OS 9.4 Boot ISO download download Debian 12 Network Install download download Fedora Server 40 Network Install download download OpenSuse Tumbleweed Network Image download download Rocky Linux OS 9.4 Boot ISO download download Ubuntu 24.04 Live Server download download <p></p>"},{"location":"Submodules/CyberPot/#raspberry-pi-4-8gb-support","title":"Raspberry Pi 4 (8GB) Support","text":"Distribution Name arm64 Raspberry Pi OS (64Bit, Lite) download"},{"location":"Submodules/CyberPot/#get-and-install-cyberpot","title":"Get and install CyberPot","text":"<ol> <li>Clone the GitHub repository: <code>$ git clone https://github.com/khulnasoft/cyberpot</code> or follow the TL;DR and skip this section.</li> <li>Change into the cyberpot/ folder: <code>$ cd cyberpot</code></li> <li>Run the installer as non-root: <code>$ ./install.sh</code>:<ul> <li>\u26a0\ufe0f Depending on your Linux distribution of choice the installer will:<ul> <li>Change the SSH port to <code>tcp/64295</code></li> <li>Disable the DNS Stub Listener to avoid port conflicts with honeypots</li> <li>Set SELinux to Monitor Mode</li> <li>Set the firewall target for the public zone to ACCEPT</li> <li>Add Docker's repository and install Docker</li> <li>Install recommended packages</li> <li>Remove packages known to cause issues</li> <li>Add the current user to the docker group (allow docker interaction without <code>sudo</code>)</li> <li>Add <code>dps</code> and <code>dpsw</code> aliases (<code>grc docker ps -a</code>, <code>watch -c \"grc --colour=on docker ps -a</code>)</li> <li>Add <code>la</code>, <code>ll</code> and <code>ls</code> aliases (for <code>exa</code>, a improved <code>ls</code> command)</li> <li>Add <code>mi</code> (for <code>micro</code>, a great alternative to <code>vi</code> and / or <code>nano</code>)</li> <li>Display open ports on the host (compare with CyberPot required ports)</li> <li>Add and enable <code>cyberpot.service</code> to <code>/etc/systemd/system</code> so CyberPot can automatically start and stop</li> </ul> </li> </ul> </li> <li>Follow the installer instructions, you will have to enter your user (<code>sudo</code> or <code>root</code>) password at least once</li> <li>Check the installer messages for errors and open ports that might cause port conflicts</li> <li>Reboot: <code>$ sudo reboot</code> </li> </ol>"},{"location":"Submodules/CyberPot/#macos-windows","title":"macOS &amp; Windows","text":"<p>Sometimes it is just nice if you can spin up a CyberPot instance on macOS or Windows, i.e. for development, testing or just the fun of it. As Docker Desktop is rather limited not all honeypot types or CyberPot features are supported. Also remember, by default the macOS and Windows firewall are blocking access from remote, so testing is limited to the host. For production it is recommended to run CyberPot on Linux. To get things up and running just follow these steps:</p> <ol> <li>Install Docker Desktop for macOS or Windows.</li> <li>Clone the GitHub repository: <code>git clone https://github.com/khulnasoft/cyberpot</code> (in Windows make sure the code is checked out with <code>LF</code> instead of <code>CRLF</code>!)</li> <li>Go to: <code>cd ~/cyberpot</code></li> <li>Copy <code>cp compose/mac_win.yml ./docker-compose.yml</code></li> <li>Create a <code>WEB_USER</code> by running <code>~/cyberpot/genuser.sh</code> (macOS) or <code>~/cyberpot/genuserwin.ps1</code> (Windows)</li> <li>Adjust the <code>.env</code> file by changing <code>CYBERPOT_OSTYPE=linux</code> to either <code>mac</code> or <code>win</code>:    <pre><code># OSType (linux, mac, win)\n#  Most docker features are available on linux\nCYBERPOT_OSTYPE=mac\n</code></pre></li> <li>You have to ensure on your own there are no port conflicts keeping CyberPot from starting up.</li> <li>Start CyberPot: <code>docker compose up</code> or <code>docker compose up -d</code> if you want CyberPot to run in the background.</li> <li>Stop CyberPot: <code>CTRL-C</code> (it if was running in the foreground) and / or <code>docker compose down -v</code> to stop CyberPot entirely.</li> </ol>"},{"location":"Submodules/CyberPot/#installation-types","title":"Installation Types","text":""},{"location":"Submodules/CyberPot/#standard-hive","title":"Standard / HIVE","text":"<p>With CyberPot Standard / HIVE all services, tools, honeypots, etc. will be installed on to a single host which also serves as a HIVE endpoint. Make sure to meet the system requirements. You can adjust <code>~/cyberpot/docker-compose.yml</code> to your personal use-case or create your very own configuration using <code>~/cyberpot/compose/customizer.py</code> for a tailored CyberPot experience to your needs. Once the installation is finished you can proceed to First Start. </p>"},{"location":"Submodules/CyberPot/#distributed","title":"Distributed","text":"<p>The distributed version of CyberPot requires at least two hosts</p> <ul> <li>the CyberPot HIVE, the standard installation of CyberPot (install this first!),</li> <li>and a CyberPot SENSOR, which will host only the honeypots, some tools and transmit log data to the HIVE.</li> <li>The SENSOR will not start before finalizing the SENSOR installation as described in Distributed Deployment.   </li> </ul>"},{"location":"Submodules/CyberPot/#uninstall-cyberpot","title":"Uninstall CyberPot","text":"<p>Uninstallation of CyberPot is only available on the supported Linux distros. To uninstall CyberPot run <code>~/cyberpot/uninstall.sh</code> and follow the uninstaller instructions, you will have to enter your password at least once. Once the uninstall is finished reboot the machine <code>sudo reboot</code> </p>"},{"location":"Submodules/CyberPot/#first-start","title":"First Start","text":"<p>Once the CyberPot Installer successfully finishes, the system needs to be rebooted (<code>sudo reboot</code>). Once rebooted you can log into the system using the user you setup during the installation of the system. Logins are according to the User Types:</p> <ul> <li>user: [<code>&lt;OS_USERNAME&gt;</code>]</li> <li>pass: [password]</li> </ul> <p>You can login via SSH to access the command line: <code>ssh -l &lt;OS_USERNAME&gt; -p 64295 &lt;your.ip&gt;</code>:</p> <ul> <li>user: [<code>&lt;OS_USERNAME&gt;</code>]</li> <li>pass: [password, ssh key recommended]</li> </ul> <p>You can also login from your browser and access the CyberPot WebUI and tools: <code>https://&lt;your.ip&gt;:64297</code></p> <ul> <li>user: [<code>&lt;WEB_USER&gt;</code>]</li> <li>pass: [password] </li> </ul>"},{"location":"Submodules/CyberPot/#standalone-first-start","title":"Standalone First Start","text":"<p>There is not much to do except to login and check via <code>dps.sh</code> if all services and honeypots are starting up correctly and login to Kibana and / or Geoip Attack Map to monitor the attacks. </p>"},{"location":"Submodules/CyberPot/#distributed-deployment","title":"Distributed Deployment","text":""},{"location":"Submodules/CyberPot/#planning-and-certificates","title":"Planning and Certificates","text":"<p>The distributed deployment involves planning as CyberPot Init will only create a self-signed certificate for the IP of the HIVE host which usually is suitable for simple setups. Since logstash will check for a valid certificate upon connection, a distributed setup involving HIVE to be reachable on multiple IPs (i.e. RFC 1918 and public NAT IP) and maybe even a domain name will result in a connection error where the certificate cannot be validated as such a setup needs a certificate with a common name and SANs (Subject Alternative Name). Before deploying any sensors make sure you have planned out domain names and IPs properly to avoid issues with the certificate. For more details see issue #1543. Adjust the example to your IP / domain setup and follow the commands to change the certificate of HIVE:</p> <pre><code>sudo systemctl stop cyberpot\n\nsudo openssl req \\\n    -nodes \\\n    -x509 \\\n    -sha512 \\\n    -newkey rsa:8192 \\\n    -keyout \"$HOME/cyberpot/data/nginx/cert/nginx.key\" \\\n    -out \"$HOME/cyberpot/data/nginx/cert/nginx.crt\" \\\n    -days 3650 \\\n    -subj '/C=AU/ST=Some-State/O=Internet Widgits Pty Ltd' \\\n    -addext \"subjectAltName = IP:192.168.1.200, IP:1.2.3.4, DNS:my.primary.domain, DNS:my.secondary.domain\"\n\nsudo chmod 774 $HOME/cyberpot/data/nginx/cert/*\nsudo chown cyberpot:cyberpot $HOME/cyberpot/data/nginx/cert/*\n\nsudo systemctl start cyberpot\n</code></pre> <p>The CyberPot configuration file (<code>.env</code>) does allow to disable the SSL verification for logstash connections from SENSOR to the HIVE by setting <code>LS_SSL_VERIFICATION=none</code>. For security reasons this is only recommended for lab or test environments. If you choose to use a valid certificate for the HIVE signed by a CA (i.e. Let's Encrypt), logstash, and therefore the SENSOR, should have no problems to connect and transmit its logs to the HIVE.</p>"},{"location":"Submodules/CyberPot/#deploying-sensors","title":"Deploying Sensors","text":"<p>Once you have rebooted the SENSOR as instructed by the installer you can continue with the distributed deployment by logging into HIVE and go to <code>cd ~/cyberpot</code> folder. Make sure you understood the Planning and Certificates before continuing with the actual deployment.</p> <p>If you have not done already generate a SSH key to securely login to the SENSOR and to allow <code>Ansible</code> to run a playbook on the sensor:</p> <ol> <li>Run <code>ssh-keygen</code>, follow the instructions and leave the passphrase empty:    <pre><code>Generating public/private rsa key pair.\nEnter file in which to save the key (/home/&lt;your_user&gt;/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/&lt;your_user&gt;/.ssh/id_rsa\nYour public key has been saved in /home/&lt;your_user&gt;/.ssh/id_rsa.pub\n</code></pre></li> <li> <p>Deploy the key to the SENSOR by running <code>ssh-copy-id -p 64295 &lt;SENSOR_SSH_USER&gt;@&lt;SENSOR_IP&gt;)</code>:</p> <pre><code>/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/home/&lt;your_user&gt;/.ssh/id_rsa.pub\"\nThe authenticity of host '[&lt;SENSOR_IP&gt;]:64295 ([&lt;SENSOR_IP&gt;]:64295)' can't be stablished.\nED25519 key fingerprint is SHA256:naIDxFiw/skPJadTcgmWZQtgt+CdfRbUCoZn5RmkOnQ.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\n&lt;your_user&gt;@172.20.254.124's password:\n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh -p '64295' '&lt;your_user&gt;@&lt;SENSOR_IP&gt;'\"\nand check to make sure that only the key(s) you wanted were added.\n</code></pre> </li> <li> <p>As suggested follow the instructions to test the connection <code>ssh -p '64295' '&lt;your_user&gt;@&lt;SENSOR_IP&gt;'</code>.</p> </li> <li>Once the key is successfully deployed run <code>./deploy.sh</code> and follow the instructions.    </li> </ol>"},{"location":"Submodules/CyberPot/#removing-sensors","title":"Removing Sensors","text":"<p>Identify the <code>CYBERPOT_HIVE_USER</code> ENV on the SENSOR in the <code>$HOME/cyberpot/.env</code> config (it is a base64 encoded string). Now identify the same string in the <code>LS_WEB_USER</code> ENV on the HIVE in the <code>$HOME/cyberpot/.env</code> config. Remove the string and restart CyberPot. Now you can safely delete the SENSOR machine.</p>"},{"location":"Submodules/CyberPot/#community-data-submission","title":"Community Data Submission","text":"<p>CyberPot is provided in order to make it accessible to everyone interested in honeypots. By default, the captured data is submitted to a community backend. This community backend uses the data to feed Sicherheitstacho. You may opt out of the submission by removing the <code># Ewsposter service</code> from <code>~/cyberpot/docker-compose.yml</code> by following these steps:</p> <ol> <li>Stop CyberPot services: <code>systemctl stop cyberpot</code></li> <li>Open <code>~/cyberpot/docker-compose.yml</code>: <code>micro ~/cyberpot/docker-compose.yml</code></li> <li>Remove the following lines, save and exit micro (<code>CTRL+Q</code>):</li> </ol> <pre><code># Ewsposter service\n  ewsposter:\n    container_name: ewsposter\n    restart: always\n    depends_on:\n      cyberpotinit:\n        condition: service_healthy\n    networks:\n     - ewsposter_local\n    environment:\n     - EWS_HPFEEDS_ENABLE=false\n     - EWS_HPFEEDS_HOST=host\n     - EWS_HPFEEDS_PORT=port\n     - EWS_HPFEEDS_CHANNELS=channels\n     - EWS_HPFEEDS_IDENT=user\n     - EWS_HPFEEDS_SECRET=secret\n     - EWS_HPFEEDS_TLSCERT=false\n     - EWS_HPFEEDS_FORMAT=json\n    image: ${CYBERPOT_REPO}/ewsposter:${CYBERPOT_VERSION}\n    pull_policy: ${CYBERPOT_PULL_POLICY}\n    volumes:\n     - ${CYBERPOT_DATA_PATH}:/data\n     - ${CYBERPOT_DATA_PATH}/ews/conf/ews.ip:/opt/ewsposter/ews.ip\n</code></pre> <ol> <li>Start CyberPot services: <code>systemctl start cyberpot</code></li> </ol> <p>It is encouraged not to disable the data submission as it is the main purpose of the community approach - as you all know sharing is caring \ud83d\ude0d </p>"},{"location":"Submodules/CyberPot/#opt-in-hpfeeds-data-submission","title":"Opt-In HPFEEDS Data Submission","text":"<p>As an Opt-In it is possible to share CyberPot data with 3<sup>rd</sup> party HPFEEDS brokers.</p> <ol> <li>Follow the instructions here to stop the CyberPot services and open <code>~/cyberpot/docker-compose.yml</code>.</li> <li>Scroll down to the <code>ewsposter</code> section and adjust the HPFEEDS settings to your needs.</li> <li>If you need to add a CA certificate add it to <code>~/cyberpot/data/ews/conf</code> and set <code>EWS_HPFEEDS_TLSCERT=/data/ews/conf/&lt;your_ca.crt&gt;</code>.</li> <li>Start CyberPot services: <code>systemctl start cyberpot</code>.    </li> </ol>"},{"location":"Submodules/CyberPot/#remote-access-and-tools","title":"Remote Access and Tools","text":"<p>Remote access to your host / CyberPot is possible with SSH (on <code>tcp/64295</code>) and some services and tools come with CyberPot to make some of your research tasks a lot easier. </p>"},{"location":"Submodules/CyberPot/#ssh","title":"SSH","text":"<p>According to the User Types you can login via SSH to access the command line: <code>ssh -l &lt;OS_USERNAME&gt; -p 64295 &lt;your.ip&gt;</code>:</p> <ul> <li>user: [<code>&lt;OS_USERNAME&gt;</code>]</li> <li>pass: [password] </li> </ul>"},{"location":"Submodules/CyberPot/#cyberpot-landing-page","title":"CyberPot Landing Page","text":"<p>According to the User Types you can open the CyberPot Landing Page from your browser via <code>https://&lt;your.ip&gt;:64297</code>:</p> <ul> <li>user: [<code>&lt;WEB_USER&gt;</code>]</li> <li>pass: [password]</li> </ul> <p> </p>"},{"location":"Submodules/CyberPot/#kibana-dashboard","title":"Kibana Dashboard","text":"<p>On the CyberPot Landing Page just click on <code>Kibana</code> and you will be forwarded to Kibana. You can select from a large variety of dashboards and visualizations all tailored to the CyberPot supported honeypots.</p> <p> </p>"},{"location":"Submodules/CyberPot/#attack-map","title":"Attack Map","text":"<p>On the CyberPot Landing Page just click on <code>Attack Map</code> and you will be forwarded to the Attack Map. Since the Attack Map utilizes web sockets you may need to re-enter the <code>&lt;WEB_USER&gt;</code> credentials.</p> <p> </p>"},{"location":"Submodules/CyberPot/#cyberchef","title":"Cyberchef","text":"<p>On the CyberPot Landing Page just click on <code>Cyberchef</code> and you will be forwarded to Cyberchef.</p> <p> </p>"},{"location":"Submodules/CyberPot/#elasticvue","title":"Elasticvue","text":"<p>On the CyberPot Landing Page just click on <code>Elastivue</code> and you will be forwarded to Elastivue.</p> <p> </p>"},{"location":"Submodules/CyberPot/#spiderfoot","title":"Spiderfoot","text":"<p>On the CyberPot Landing Page just click on <code>Spiderfoot</code> and you will be forwarded to Spiderfoot.</p> <p> </p>"},{"location":"Submodules/CyberPot/#configuration","title":"Configuration","text":""},{"location":"Submodules/CyberPot/#cyberpot-config-file","title":"CyberPot Config File","text":"<p>CyberPot offers a configuration file providing variables not only for the docker services (i.e. honeypots and tools) but also for the docker compose environment. The configuration file is hidden in <code>~/tpoce/.env</code>. There is also an example file (<code>env.example</code>) which holds the default configuration. Before the first start run <code>~/cyberpot/genuser.sh</code> or setup the <code>WEB_USER</code> manually as described here.</p>"},{"location":"Submodules/CyberPot/#customize-cyberpot-honeypots-and-services","title":"Customize CyberPot Honeypots and Services","text":"<p>In <code>~/cyberpot/compose</code> you will find everything you need to adjust the CyberPot Standard / HIVE installation:</p> <pre><code>customizer.py\nmac_win.yml\nmini.yml\nmobile.yml\nraspberry_showcase.yml\nsensor.yml\nstandard.yml\ncyberpot_services.yml\n</code></pre> <p>The <code>.yml</code> files are docker compose files, each representing a different set of honeypots and tools with <code>cyberpot_services.yml</code> being a template for <code>customizer.py</code> to create a customized docker compose file. To activate a compose file follow these steps:</p> <ol> <li>Stop CyberPot with <code>systemctl stop cyberpot</code>.</li> <li>Copy the docker compose file <code>cp ~/cyberpot/compose/&lt;dockercompose.yml&gt; ~/cyberpot/docker-compose.yml</code>.</li> <li>Start CyberPot with <code>systemctl start cyberpot</code>.</li> </ol> <p>To create your customized docker compose file:</p> <ol> <li>Go to <code>cd ~/cyberpot/compose</code>.</li> <li>Run <code>python3 customizer.py</code>.</li> <li>The script will guide you through the process of creating your own <code>docker-compose.yml</code>. As some honeypots and services occupy the same ports it will check if any port conflicts are present and notify regarding the conflicting services. You then can resolve them manually by adjusting <code>docker-compose-custom.yml</code> or re-run the script.</li> <li>Stop CyberPot with <code>systemctl stop cyberpot</code>.</li> <li>Copy the custom docker compose file: <code>cp docker-compose-custom.yml ~/cyberpot</code> and <code>cd ~/cyberpot</code>.</li> <li>Check if everything works by running <code>docker-compose -f docker-compose-custom.yml up</code>. In case of errors follow the Docker Compose Specification for mitigation. Most likely it is just a port conflict you can adjust by editing the docker compose file.</li> <li>If everything works just fine press <code>CTRL-C</code> to stop the containers and run <code>docker-compose -f docker-compose-custom.yml down -v</code>.</li> <li>Replace docker compose file with the new and successfully tested customized docker compose file <code>mv ~/cyberpot/docker-compose-custom.yml ~/cyberpot/docker-compose.yml</code>.</li> <li>Start CyberPot with <code>systemctl start cyberpot</code>.    </li> </ol>"},{"location":"Submodules/CyberPot/#maintenance","title":"Maintenance","text":"<p>CyberPot is designed to be low maintenance. Since almost everything is provided through docker images there is basically nothing you have to do but let it run. We will upgrade the docker images regularly to reduce the risks of compromise; however you should read this section closely. Should an update fail, opening an issue or a discussion will help to improve things in the future, but the offered solution will always be to perform a fresh install as we simply cannot provide any support for lost data! </p>"},{"location":"Submodules/CyberPot/#general-updates","title":"General Updates","text":"<p>CyberPot security depends on the updates provided for the supported Linux distro images. Make sure to review the OS documentation and ensure updates are installed regularly by the OS. By default (<code>~/cyberpot/.env</code>) <code>CYBERPOT_PULL_POLICY=always</code> will ensure that at every CyberPot start docker will check for new docker images and download them before creating the containers. </p>"},{"location":"Submodules/CyberPot/#update-script","title":"Update Script","text":"<p>CyberPot releases are offered through GitHub and can be pulled using <code>~/cyberpot/update.sh</code>. If you made any relevant changes to the CyberPot config files make sure to create a backup first! Updates may have unforeseen consequences. Create a backup of the machine or the files most valuable to your work!</p> <p>The update script will ...</p> <ul> <li>mercilessly overwrite local changes to be in sync with the CyberPot master branch</li> <li>create a full backup of the <code>~/cyberpot</code> folder</li> <li>update all files in <code>~/cyberpot</code> to be in sync with the CyberPot master branch</li> <li>restore your custom <code>ews.cfg</code> from <code>~/cyberpot/data/ews/conf</code> and the CyberPot configuration (<code>~/cyberpot/.env</code>).</li> </ul>"},{"location":"Submodules/CyberPot/#daily-reboot","title":"Daily Reboot","text":"<p>By default CyberPot will add a daily reboot including some cleaning up. You can adjust this line with <code>sudo crontab -e</code></p> <pre><code>#Ansible: CyberPot Daily Reboot\n42 2 * * * bash -c 'systemctl stop cyberpot.service &amp;&amp; docker container prune -f; docker image prune -f; docker volume prune -f; /usr/sbin/shutdown -r +1 \"CyberPot Daily Reboot\"'\n</code></pre>"},{"location":"Submodules/CyberPot/#known-issues","title":"Known Issues","text":"<p>The following issues are known, simply follow the described steps to solve them. </p>"},{"location":"Submodules/CyberPot/#docker-images-fail-to-download","title":"Docker Images Fail to Download","text":"<p>Some time ago Docker introduced download rate limits. If you are frequently downloading Docker images via a single or shared IP, the IP address might have exhausted the Docker download rate limit. Login to your Docker account to extend the rate limit.</p> <pre><code>sudo su -\ndocker login\n</code></pre>"},{"location":"Submodules/CyberPot/#cyberpot-networking-fails","title":"CyberPot Networking Fails","text":"<p>CyberPot is designed to only run on machines with a single NIC. CyberPot will try to grab the interface with the default route, however it is not guaranteed that this will always succeed. At best use CyberPot on machines with only a single NIC.</p>"},{"location":"Submodules/CyberPot/#start-cyberpot","title":"Start CyberPot","text":"<p>The CyberPot service automatically starts and stops on each reboot (which occurs once on a daily basis as setup in <code>sudo crontab -l</code> during installation).  If you want to manually start the CyberPot service you can do so via <code>systemctl start cyberpot</code> and observe via <code>dpsw</code> the startup of the containers. </p>"},{"location":"Submodules/CyberPot/#stop-cyberpot","title":"Stop CyberPot","text":"<p>The CyberPot service automatically starts and stops on each reboot (which occurs once on a daily basis as setup in <code>sudo crontab -l</code> during installation).  If you want to manually stop the CyberPot service you can do so via <code>systemctl stop cyberpot</code> and observe via <code>dpsw</code> the shutdown of the containers. </p>"},{"location":"Submodules/CyberPot/#cyberpot-data-folder","title":"CyberPot Data Folder","text":"<p>All persistent log files from the honeypots, tools and CyberPot related services are stored in <code>~/cyberpot/data</code>. This includes collected artifacts which are not transmitted to the Elastic Stack. </p>"},{"location":"Submodules/CyberPot/#log-persistence","title":"Log Persistence","text":"<p>All log data stored in the CyberPot Data Folder will be persisted for 30 days by default.  Elasticsearch indices are handled by the <code>cyberpot</code> Index Lifecycle Policy which can be adjusted directly in Kibana (make sure to \"Include managed system policies\").  </p> <p>By default the <code>cyberpot</code> Index Lifecycle Policy keeps the indices for 30 days. This offers a good balance between storage and speed. However you may adjust the policy to your needs.  </p>"},{"location":"Submodules/CyberPot/#factory-reset","title":"Factory Reset","text":"<p>All log data stored in the CyberPot Data Folder (except for Elasticsearch indices, of course) can be erased by running <code>clean.sh</code>. Sometimes things might break beyond repair and it has never been easier to reset a CyberPot to factory defaults (make sure to enter <code>cd ~/cyberpot</code>).</p> <ol> <li>Stop CyberPot using <code>systemctl stop cyberpot</code>.</li> <li>Move / Backup the <code>~/cyberpot/data</code> folder to a safe place (this is optional, just in case).</li> <li>Delete the <code>~/cyberpot/data</code> folder using <code>sudo rm -rf ~/cyberpot/data</code>.</li> <li>Reset CyberPot to the last fetched commit:</li> </ol> <pre><code>cd ~/cyberpot/\ngit reset --hard\n</code></pre> <ol> <li>Now you can run <code>~/cyberpot/install.sh</code>.    </li> </ol>"},{"location":"Submodules/CyberPot/#show-containers","title":"Show Containers","text":"<p>You can show all CyberPot relevant containers by running <code>dps</code> or <code>dpsw [interval]</code>. The <code>interval (s)</code> will re-run <code>dps.sh</code> periodically. </p>"},{"location":"Submodules/CyberPot/#blackhole","title":"Blackhole","text":"<p>Blackhole will run CyberPot in kind of a stealth mode manner without permanent visits of publicly known scanners and thus reducing the possibility of being exposed. While this is of course always a cat and mouse game the blackhole feature is null routing all requests from known mass scanners while still catching the events through Suricata.  The feature is activated by setting <code>CYBERPOT_BLACKHOLE=DISABLED</code> in <code>~/cyberpot/.env</code>, then run <code>systemctl stop cyberpot</code> and <code>systemctl start cyberpot</code> or <code>sudo reboot</code>.  Enabling this feature will drastically reduce attackers visibility and consequently result in less activity. However as already mentioned it is neither a guarantee for being completely stealth nor will it prevent fingerprinting of some honeypot services. </p>"},{"location":"Submodules/CyberPot/#add-users-to-nginx-cyberpot-webui","title":"Add Users to Nginx (CyberPot WebUI)","text":"<p>Nginx (CyberPot WebUI) allows you to add as many <code>&lt;WEB_USER&gt;</code> accounts as you want (according to the User Types). To add a new user run <code>~/cyberpot/genuser.sh</code>. To remove users open <code>~/cyberpot/.env</code>, locate <code>WEB_USER</code> and remove the corresponding base64 string (to decode: <code>echo &lt;base64_string&gt; | base64 -d</code>, or open CyberChef and load \"From Base64\" recipe). For the changes to take effect you need to restart CyberPot using <code>systemctl stop cyberpot</code> and <code>systemctl start cyberpot</code> or <code>sudo reboot</code>. </p>"},{"location":"Submodules/CyberPot/#import-and-export-kibana-objects","title":"Import and Export Kibana Objects","text":"<p>Some CyberPot updates will require you to update the Kibana objects. Either to support new honeypots or to improve existing dashboards or visualizations. Make sure to export first so you do not loose any of your adjustments.</p>"},{"location":"Submodules/CyberPot/#export","title":"Export","text":"<ol> <li>Go to Kibana</li> <li>Click on \"Stack Management\"</li> <li>Click on \"Saved Objects\"</li> <li>Click on \"Export  objects\" <li>Click on \"Export all\"    This will export a NDJSON file with all your objects. Always run a full export to make sure all references are included.</li>"},{"location":"Submodules/CyberPot/#import","title":"Import","text":"<ol> <li>Download the NDJSON file and unzip it.</li> <li>Go to Kibana</li> <li>Click on \"Stack Management\"</li> <li>Click on \"Saved Objects\"</li> <li>Click on \"Import\" and leave the defaults (check for existing objects and automatically overwrite conflicts) if you did not make personal changes to the Kibana objects.</li> <li>Browse for NDJSON file    When asked: \"If any of the objects already exist, do you want to automatically overwrite them?\" you answer with \"Yes, overwrite all\".    </li> </ol>"},{"location":"Submodules/CyberPot/#troubleshooting","title":"Troubleshooting","text":"<p>Generally CyberPot is offered as is without any commitment regarding support. Issues and discussions can be opened, but be prepared to include basic necessary info, so the community is able to help. </p>"},{"location":"Submodules/CyberPot/#logs","title":"Logs","text":"<ul> <li>Check if your containers are running correctly: <code>dps</code></li> <li>Check if your system resources are not exhausted: <code>htop</code>, <code>docker stats</code></li> <li>Check if there is a port conflict:</li> </ul> <pre><code>systemctl stop cyberpot\ngrc netstat -tulpen\nmi ~/cyberpot/docker-compose.yml\ndocker-compose -f ~/cyberpot/docker-compose.yml up\nCTRL+C\ndocker-compose -f ~/cyberpot/docker-compose.yml down -v\n</code></pre> <ul> <li>Check individual container logs: <code>docker logs -f &lt;container_name&gt;</code></li> <li>Check <code>cyberpotinit</code> log: <code>cat ~/cyberpot/data/cyberpotinit.log</code> </li> </ul>"},{"location":"Submodules/CyberPot/#ram-and-storage","title":"RAM and Storage","text":"<p>The Elastic Stack is hungry for RAM, specifically <code>logstash</code> and <code>elasticsearch</code>. If the Elastic Stack is unavailable, does not receive any logs or simply keeps crashing it is most likely a RAM or storage issue. While CyberPot keeps trying to restart the services / containers run <code>docker logs -f &lt;container_name&gt;</code> (either <code>logstash</code> or <code>elasticsearch</code>) and check if there are any warnings or failures involving RAM.</p> <p>Storage failures can be identified easier via <code>htop</code>. </p>"},{"location":"Submodules/CyberPot/#contact","title":"Contact","text":"<p>CyberPot is provided as is open source without any commitment regarding support (see the disclaimer).</p> <p>If you are a security researcher and want to responsibly report an issue please get in touch with our CERT. </p>"},{"location":"Submodules/CyberPot/#issues","title":"Issues","text":"<p>Please report issues (errors) on our GitHub Issues, but troubleshoot first. Issues not providing information to address the error will be closed or converted into discussions.</p> <p>Use the search function first, it is possible a similar issue has been addressed or discussed already, with the solution just a search away. </p>"},{"location":"Submodules/CyberPot/#discussions","title":"Discussions","text":"<p>General questions, ideas, show &amp; tell, etc. can be addressed on our GitHub Discussions.</p> <p>Use the search function, it is possible a similar discussion has been opened already, with an answer just a search away. </p>"},{"location":"Submodules/CyberPot/#licenses","title":"Licenses","text":"<p>The software that CyberPot is built on uses the following licenses. GPLv2: conpot, dionaea, honeytrap, suricata GPLv3: adbhoney, elasticpot, ewsposter, log4pot, fatt, heralding, ipphoney, redishoneypot, sentrypeer, snare, tanner Apache 2 License: cyberchef, dicompot, elasticsearch, logstash, kibana, docker MIT license: autoheal, ciscoasa, ddospot, elasticvue, glutton, hellpot, maltrail  Unlicense: endlessh  Other: citrixhoneypot, cowrie, mailoney, Elastic License, Wordpot  AGPL-3.0: honeypots Public Domain (CC): Harvard Dataverse </p>"},{"location":"Submodules/CyberPot/#credits","title":"Credits","text":"<p>Without open source and the development community we are proud to be a part of, CyberPot would not have been possible! Our thanks are extended but not limited to the following people and organizations:</p>"},{"location":"Submodules/CyberPot/#the-developers-and-development-communities-of","title":"The developers and development communities of","text":"<ul> <li>adbhoney</li> <li>ciscoasa</li> <li>citrixhoneypot</li> <li>conpot</li> <li>cowrie</li> <li>ddospot</li> <li>dicompot</li> <li>dionaea</li> <li>docker</li> <li>elasticpot</li> <li>elasticsearch</li> <li>elasticvue</li> <li>endlessh</li> <li>ewsposter</li> <li>fatt</li> <li>glutton</li> <li>hellpot</li> <li>heralding</li> <li>honeypots</li> <li>honeytrap</li> <li>ipphoney</li> <li>kibana</li> <li>logstash</li> <li>log4pot</li> <li>mailoney</li> <li>maltrail</li> <li>medpot</li> <li>p0f</li> <li>redishoneypot</li> <li>sentrypeer</li> <li>spiderfoot</li> <li>snare</li> <li>tanner</li> <li>suricata</li> <li>wordpot</li> </ul> <p>The following companies and organizations</p> <ul> <li>docker</li> <li>elastic.io</li> <li>honeynet project</li> </ul> <p>... and of course *you* for joining the community! </p> <p>Thank you for playing \ud83d\udc96</p>"},{"location":"Submodules/CyberPot/#testimonials","title":"Testimonials","text":"<p>One of the greatest feedback we have gotten so far is by one of the Conpot developers: \"[...] I highly recommend CyberPot which is ... it's not exactly a swiss army knife .. it's more like a swiss army soldier, equipped with a swiss army knife. Inside a tank. A swiss tank. [...]\"  And from @robcowart (creator of ElastiFlow): \"#CyberPot is one of the most well put together turnkey honeypot solutions. It is a must-have for anyone wanting to analyze and understand the behavior of malicious actors and the threat they pose to your organization.\" Thank you!</p> <p></p>"},{"location":"Submodules/CyberPot/CHANGELOG/","title":"Release Notes / Changelog","text":"<p>CyberPot 24.04.0 marks probably the largest change in the history of the project. While most of the changes have been made to the underlying platform some changes will be standing out in particular - a CyberPot ISO image will no longer be provided with the benefit that CyberPot will now run on multiple Linux distributions (Alma Linux, Debian, Fedora, OpenSuse, Raspbian, Rocky Linux, Ubuntu), Raspberry Pi (optimized) and macOS / Windows (limited).</p>"},{"location":"Submodules/CyberPot/CHANGELOG/#new-features","title":"New Features","text":"<ul> <li>Distributed Installation is now using NGINX reverse proxy instead of SSH to transmit HIVE_SENSOR logs to HIVE</li> <li><code>deploy.sh</code>, will make the deployment of sensor much easier and will automatically take care of the configuration. You only have to install the CyberPot sensor.</li> <li>CyberPot Init is the foundation for running CyberPot on multiple Linux distributions and will also ensure to restart containers with failed healthchecks using autoheal</li> <li>CyberPot Installer is now mostly Ansible based providing a universal playbook for the most common Linux distributions</li> <li>CyberPot Uninstaller allows to uninstall CyberPot, while not recommended for general usage, this comes in handy for testing purposes</li> <li>CyberPot Customizer (<code>compose/customizer.py</code>) is here to assist you in the creation of a customized <code>docker-compose.yml</code></li> <li>CyberPot Landing Page has been redesigned and simplified   </li> <li>Kibana Dashboards, Objects fully refreshed in favor of Lens based objects   </li> <li>Wordpot is added as new addition to the available honeypots within CyberPot and will run on <code>tcp/8080</code> by default.</li> <li>Raspberry Pi is now supported using a dedicated <code>mobile.yml</code> (why this is called mobile will be revealed soon!)</li> <li>GeoIP Attack Map is now aware of connects / disconnects and thus eliminating required reloads</li> <li>Docker, where possible, will now be installed directly from the Docker repositories to avoid any incompatibilities</li> <li><code>.env</code> now provides a single configuration file for the CyberPot related settings</li> <li><code>genuser.sh</code> can now be used to add new users to the CyberPot Landing Page as part of the CyberPot configuration file (<code>.env</code>)</li> </ul>"},{"location":"Submodules/CyberPot/CHANGELOG/#updates","title":"Updates","text":"<ul> <li>Honeypots and tools were updated to their latest pushed code and / or releases</li> <li>Where possible Docker Images will now use Alpine 3.19</li> <li>Updates will be provided continuously through Docker Images updates</li> </ul>"},{"location":"Submodules/CyberPot/CHANGELOG/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>There is no option to migrate a previous installation to CyberPot 24.04.0, you can try to transfer the old <code>data</code> folder to the new CyberPot installation, but a working environment depends on too many other factors outside of our control and a new installation is simply faster.</li> <li>Most of the support scripts were moved into the CyberPot Init image and are no longer available directly on the host.</li> <li>Cockpit is no longer available as part of CyberPot itself. However, where supported, you can simply install the <code>cockpit</code> package.</li> </ul> <p>... and many others from the CyberPot community by opening valued issues and discussions, suggesting ideas and thus helping to improve CyberPot!</p>"},{"location":"Submodules/CyberPot/SECURITY/","title":"Security Policy","text":""},{"location":"Submodules/CyberPot/SECURITY/#supported-versions","title":"Supported Versions","text":"Version Supported 24.04"},{"location":"Submodules/CyberPot/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>We prioritize the security of CyberPot highly. Often, vulnerabilities in CyberPot components stem from upstream dependencies, including honeypots, Docker images, tools, or packages. We are committed to working together to resolve any issues effectively.</p> <p>Please follow these steps before reporting a potential vulnerability:</p> <ol> <li>Verify that the behavior you've observed isn't already documented as a normal aspect or unrelated issue of CyberPot. For example, Cowrie may initiate outgoing connections, or CyberPot might open all possible TCP ports\u2014a feature enabled by Honeytrap.</li> <li>Clearly identify which component is vulnerable (e.g., a specific honeypot, Docker image, tool, package) and isolate the issue.</li> <li>Provide a detailed description of the issue, including log and, if available, debug files. Include all steps necessary to reproduce the vulnerability. If you have a proposed solution, hotfix, or patch, please be prepared to submit a pull request (PR).</li> <li>Check whether the vulnerability is already known upstream. If there is an existing fix or patch, include that information in your report.</li> </ol> <p>This approach ensures a thorough and efficient resolution process.</p> <p>We aim to respond as quickly as possible. If you believe the issue poses an immediate threat to the entire CyberPot community, you can expedite the process by responsibly alerting our CERT.</p>"},{"location":"Submodules/CyberPot/docker/deprecated/elasticpot.old/","title":"Index","text":""},{"location":"Submodules/CyberPot/docker/deprecated/elasticpot.old/#elasticpot","title":"elasticpot","text":"<p>elasticpot is a simple elastic search honeypot.</p> <p>This dockerized version is part of the CyberPot community honeypot of Deutsche Telekom AG.</p> <p>The <code>Dockerfile</code> contains the blueprint for the dockerized elasticpot and will be used to setup the docker image.</p> <p>The <code>docker-compose.yml</code> contains the necessary settings to test elasticpot using <code>docker-compose</code>. This will ensure to start the docker container with the appropriate permissions and port mappings.</p>"},{"location":"Submodules/CyberPot/docker/deprecated/elasticpot.old/#elasticpot-dashboard","title":"ElasticPot Dashboard","text":""},{"location":"Submodules/CyberPot/docker/deprecated/glastopf/","title":"Index","text":""},{"location":"Submodules/CyberPot/docker/deprecated/glastopf/#glastopf-deprecated","title":"glastopf (deprecated)","text":"<p>glastopf is a python web application honeypot.</p> <p>This dockerized version is part of the CyberPot community honeypot of Deutsche Telekom AG.</p> <p>The <code>Dockerfile</code> contains the blueprint for the dockerized glastopf and will be used to setup the docker image.</p> <p>The <code>docker-compose.yml</code> contains the necessary settings to test glastopf using <code>docker-compose</code>. This will ensure to start the docker container with the appropriate permissions and port mappings.</p>"},{"location":"Submodules/CyberPot/docker/deprecated/glastopf/#glastopf-dashboard","title":"Glastopf Dashboard","text":""},{"location":"Submodules/GoThreatMatrix/","title":"Go-ThreatMatrix","text":"<p>go-threatmatrix is a client library/SDK that allows developers to easily automate and integrate ThreatMatrix with their own set of tools!</p>"},{"location":"Submodules/GoThreatMatrix/#table-of-contents","title":"Table of Contents","text":"<ul> <li>go-threatmatrix</li> <li>Getting Started     - Pre requisites     - Installation     - Usage     - Examples</li> <li>Contribute</li> <li>License</li> <li>Links</li> <li>FAQ     - Generate API key         - v4.0 and above         - v4.0 below</li> </ul>"},{"location":"Submodules/GoThreatMatrix/#getting-started","title":"Getting Started","text":""},{"location":"Submodules/GoThreatMatrix/#pre-requisites","title":"Pre requisites","text":"<ul> <li>Go 1.17+</li> </ul>"},{"location":"Submodules/GoThreatMatrix/#installation","title":"Installation","text":"<p>Use go get to retrieve the SDK to add it to your GOPATH workspace, or project's Go module dependencies.</p> <pre><code>$ go get github.com/khulnasoft/go-threatmatrix\n</code></pre>"},{"location":"Submodules/GoThreatMatrix/#usage","title":"Usage","text":"<p>This library was built with ease of use in mind! Here are some quick examples to get you started. If you need more example you can go to the examples directory</p> <p>To start using the go-threatmatrix library you first need to import it: <pre><code>import \"github.com/khulnasoft/go-threatmatrix/gothreatmatrix\"\n</code></pre> Construct a new <code>ThreatMatrixClient</code>, then use the various services to easily access different parts of Threatmatrix's REST API. Here's an example of getting all jobs:</p> <p><pre><code>clientOptions := gothreatmatrix.ThreatMatrixClientOptions{\n    Url:         \"your-cool-URL-goes-here\",\n    Token:       \"your-super-secret-token-goes-here\",\n    // This is optional\n    Certificate: \"your-optional-certificate-goes-here\",\n}\n\nthreatmatrix := gothreatmatrix.NewThreatMatrixClient(\n    &amp;clientOptions,\n    nil\n)\n\nctx := context.Background()\n\n// returns *[]Jobs or an ThreatMatrixError!\njobs, err := threatmatrix.JobService.List(ctx)\n</code></pre> For easy configuration and set up we opted for <code>options</code> structs. Where we can customize the client API or service endpoint to our liking! For more information go here. Here's a quick example!</p> <pre><code>// ...Making the client and context!\n\ntagOptions = gothreatmatrix.TagParams{\n  Label: \"NEW TAG\",\n  Color: \"#ffb703\",\n}\n\ncreatedTag, err := threatmatrix.TagService.Create(ctx, tagOptions)\nif err != nil {\n    fmt.Println(err)\n} else {\n    fmt.Println(createdTag)\n}\n</code></pre>"},{"location":"Submodules/GoThreatMatrix/#examples","title":"Examples","text":"<p>The examples directory contains a couple for clear examples, of which one is partially listed here as well:</p> <p><pre><code>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/khulnasoft/go-threatmatrix/gothreatmatrix\"\n)\n\nfunc main(){\n    threatmatrixOptions := gothreatmatrix.ThreatMatrixClientOptions{\n        Url:         \"your-cool-url-goes-here\",\n        Token:       \"your-super-secret-token-goes-here\",\n        Certificate: \"your-optional-certificate-goes-here\",\n    }   \n\n    client := gothreatmatrix.NewThreatMatrixClient(\n        &amp;threatmatrixOptions,\n        nil,\n    )\n\n    ctx := context.Background()\n\n    // Get User details!\n    user, err := client.UserService.Access(ctx)\n    if err != nil {\n        fmt.Println(\"err\")\n        fmt.Println(err)\n    } else {\n        fmt.Println(\"USER Details\")\n        fmt.Println(*user)\n    }\n}\n</code></pre> For complete usage of go-threatmatrix, see the full package docs.</p>"},{"location":"Submodules/GoThreatMatrix/#contribute","title":"Contribute","text":"<p>If you want to follow the updates, discuss, contribute, or just chat then please join our slack channel we'd love to hear your feedback!</p>"},{"location":"Submodules/GoThreatMatrix/#license","title":"License","text":"<p>Licensed under the GNU AFFERO GENERAL PUBLIC LICENSE.</p>"},{"location":"Submodules/GoThreatMatrix/#links","title":"Links","text":"<ul> <li>Threatmatrix</li> <li>Documentation</li> <li>API documentation</li> <li>Examples</li> </ul>"},{"location":"Submodules/GoThreatMatrix/#faq","title":"FAQ","text":""},{"location":"Submodules/GoThreatMatrix/#generate-api-key","title":"Generate API key","text":"<p>You need a valid API key to interact with the ThreatMatrix server.</p>"},{"location":"Submodules/GoThreatMatrix/#v40-and-above","title":"v4.0 and above","text":"<p>You can get an API by doing the following: 1. Log / Signin into threatmatrix 2. At the upper right click on your profile from the drop down select <code>API Access/ Sessions</code> 3. Then generate an API key or see it!</p>"},{"location":"Submodules/GoThreatMatrix/#v40-below","title":"v4.0 below","text":"<p>Keys should be created from the admin interface of ThreatMatrix: you have to go in the Durin section (click on <code>Auth tokens</code>) and generate a key there.</p>"},{"location":"Submodules/GoThreatMatrix/examples/basicExample/example/","title":"Example","text":"<p>This example will show you how to do a basic scan!</p>"},{"location":"Submodules/GoThreatMatrix/examples/client/client/","title":"Client","text":""},{"location":"Submodules/GoThreatMatrix/examples/client/client/#client","title":"Client","text":"<p>A good client is a client that is easy to use, configurable and customizable to a user\u2019s liking. Hence, the client has 4 great features: 1. Configurable HTTP client 2. Customizable timeouts 3. Logger 4. Easy ways to create the <code>ThreatMatrixClient</code></p>"},{"location":"Submodules/GoThreatMatrix/examples/client/client/#configurable-http-client","title":"Configurable HTTP client","text":"<p>Now from the documentation, you can see you can pass your <code>http.Client</code>. This is to facilitate each user\u2019s requirement and taste! If you don\u2019t pass one (<code>nil</code>) a default <code>http.Client</code> will be made for you!</p>"},{"location":"Submodules/GoThreatMatrix/examples/client/client/#customizable-timeouts","title":"Customizable timeouts","text":"<p>From <code>ThreatMatrixClientOptions</code> you can add your own timeout to your requests as well.</p>"},{"location":"Submodules/GoThreatMatrix/examples/client/client/#logger","title":"Logger","text":"<p>To ease  developers' work go-threatmatrix provides a logger for easy debugging and tracking! For the logger we used logrus because of 2 reasons: 1. Easy to use 2. Extensible to your liking</p>"},{"location":"Submodules/GoThreatMatrix/examples/client/client/#easy-ways-to-create-the-threatmatrixclient","title":"Easy ways to create the <code>ThreatMatrixClient</code>","text":"<p>As you know working with Golang structs is sometimes cumbersome we thought we could provide a simple way to create the client in a way that helps speed up development. This gave birth to the idea of using a <code>JSON</code> file to create the ThreatMatrixClient. The method <code>NewThreatMatrixClientThroughJsonFile</code> does exactly that. Send the <code>ThreatMatrixClientOptions</code> JSON file path with your http.Client and LoggerParams in this method and you'll get the ThreatMatrixClient!</p>"},{"location":"Submodules/GoThreatMatrix/examples/optionalParams/optionalParams/","title":"Optional Parameters","text":"<p>For the sake of simplicity, we decided that for some endpoints we\u2019ll be passing <code>Option Parameters</code> this is to facilitate easy access, configuration and automation so that you don\u2019t need to pass in many parameters but just a simple struct that can be easily converted to and from JSON!</p> <p>For example, let us look at the <code>TagParams</code> we use it as an argument for a method <code>Create</code> for <code>TagService</code>. From a glance, the <code>TagParams</code> look simple. They hold 2 fields: <code>Label</code>, and <code>Color</code> which can be passed seperatly to the method but imagine if you have many fields! (if you don\u2019t believe see the <code>ObservableAnalysisParams</code>)</p> <p>For a practical implementation you can see the example</p>"},{"location":"Submodules/GoThreatMatrix/tests/CONTRIBUTING/","title":"CONTRIBUTING","text":""},{"location":"Submodules/GoThreatMatrix/tests/CONTRIBUTING/#how-unit-tests-were-written","title":"How unit tests were written","text":"<p>The unit tests were written as a combination of table driven tests and the approach used by go-github</p> <p>Firstly we use a <code>TestData</code> struct that has the following fields: 1. <code>Input</code> - this is an <code>interface</code> as it is to be used as the input required for an endpoint 2. <code>Data</code> - this is a <code>string</code> as it'll be the <code>JSON</code> string that the endpoint is expected to return 2. <code>StatusCode</code> - this is an <code>int</code> as it is meant to be used as the expected response returned by the endpoint 3. <code>Want</code> - the expected struct that the method will return</p> <p>Now the reason we made this was that these fields were needed for every endpoint hence combining them into a single struct provided us reusability and flexibility.</p> <p>Now the testing suite used go's <code>httptest</code> library where we use <code>httptest.Server</code> as this setups a test server so that we can easily mock it. We also use <code>http.ServerMux</code> to mock our endpoints response.</p>"},{"location":"Submodules/GoThreatMatrix/tests/CONTRIBUTING/#how-to-add-a-new-test-for-an-endpoint","title":"How to add a new test for an endpoint","text":"<p>Lets say ThreatMatrix added a new endpoint called supercool in <code>Tag</code>. Now you've implemented the endpoint as a method of <code>TagService</code> and now you want to add its unit tests.</p> <p>First go to <code>tagService_test.go</code> in the <code>tests</code> directory and add</p> <pre><code>func TestSuperCoolEndPoint(t *testing.T) {\n    testCases := make(map[string]TestData)\n    testCases[\"simple\"] = TestData{\n        Input:      nil,\n        Data:       `{ \"supercool\": \"you're a great developer :)\"}`,\n        StatusCode: http.StatusOK,\n        Want: \"you're a great developer :)\",\n    }\n    for name, testCase := range testCases {\n        // subtest\n        t.Run(name, func(t *testing.T) {\n            // setup will give you the client, mux/router, closeServer\n            client, apiHandler, closeServer := setup()\n            defer closeServer()\n            ctx := context.Background()\n            // now you can use apiHandler to mock how the server will handle this endpoints request\n            // you can use mux/router's Handle method or HandleFunc\n            apiHandler.Handle(\"/api/tag/supercool\", func(w http.ResponseWriter, r *http.Request) {\n                // this is a helper test to check if it is the expected request sent by the client\n                testMethod(t, r, \"GET\")\n                w.Write([]byte(testCase.Data))\n            })\n            expectedRespone, err := client.TagService.SuperCool(ctx)\n            if err != nil {\n                testError(t, testCase, err)\n            } else {\n                testWantData(t, testCase.Want, expectedRespone)\n            }\n        })\n    }\n}\n</code></pre> <p>Great! Now you've added your own unit tests.</p>"},{"location":"Submodules/ThreatMatrix/","title":"Threat Matrix","text":"<p>Do you want to get threat intelligence data about a malware, an IP address or a domain? Do you want to get this kind of data from multiple sources at the same time using a single API request?</p> <p>You are in the right place!</p> <p>ThreatMatrix is an Open Source solution for management of Threat Intelligence at scale. It integrates a number of analyzers available online and a lot of cutting-edge malware analysis tools.</p>"},{"location":"Submodules/ThreatMatrix/#features","title":"Features","text":"<p>This application is built to scale out and to speed up the retrieval of threat info.</p> <p>It provides:</p> <ul> <li>Enrichment of Threat Intel for files as well as observables (IP, Domain, URL, hash, etc).</li> <li>A Fully-fledged REST APIs written in Django and Python.</li> <li>An easy way to be integrated in your stack of security tools to automate common jobs usually performed, for instance, by SOC analysts manually. (Thanks to the official libraries pythreatmatrix and go-threatmatrix)</li> <li>A built-in GUI: provides features such as dashboard, visualizations of analysis data, easy to use forms for requesting new analysis, etc.</li> <li>A framework composed of modular components called Plugins:<ul> <li>analyzers that can be run to either retrieve data from external sources (like VirusTotal or AbuseIPDB) or to generate intel from internally available tools (like Yara or Oletools)</li> <li>connectors that can be run to export data to external platforms (like MISP or OpenCTI)</li> <li>pivots that are designed to trigger the execution of a chain of analysis and connect them to each other</li> <li>visualizers that are designed to create custom visualizations of analyzers results</li> <li>ingestors that allows to automatically ingest stream of observables or files to ThreatMatrix itself</li> <li>playbooks that are meant to make analysis easily repeatable</li> </ul> </li> </ul>"},{"location":"Submodules/ThreatMatrix/#documentation","title":"Documentation","text":"<p>We try hard to keep our documentation well written, easy to understand and always updated. All info about installation, usage, configuration and contribution can be found here</p>"},{"location":"Submodules/ThreatMatrix/#publications-and-media","title":"Publications and Media","text":"<p>To know more about the project and its growth over time, you may be interested in reading the official blog posts and/or videos about the project by clicking on this link</p>"},{"location":"Submodules/ThreatMatrix/#available-services-or-analyzers","title":"Available services or analyzers","text":"<p>You can see the full list of all available analyzers in the documentation.</p> Type Analyzers Available Inbuilt modules - Static Office Document, RTF, PDF, PE File Analysis and metadata extraction - Strings Deobfuscation and analysis (FLOSS, Stringsifter, ...) - PE Emulation with Qiling and Speakeasy - PE Signature verification - PE Capabilities Extraction (CAPA) - Javascript Emulation (Box-js) - Android Malware Analysis (Quark-Engine, ...) - SPF and DMARC Validator - Yara (a lot of public rules are available. You can also add your own rules) - more... External services - Abuse.ch MalwareBazaar/URLhaus/Threatfox/YARAify -  GreyNoise v2 -  Intezer - VirusTotal v3 -  Crowdsec - URLscan - Shodan - AlienVault OTX - Intelligence_X - MISP - many more.."},{"location":"Submodules/ThreatMatrix/#partnerships-and-sponsors","title":"Partnerships and sponsors","text":"<p>As open source project maintainers, we strongly rely on external support to get the resources and time to work on keeping the project alive, with a constant release of new features, bug fixes and general improvements.</p> <p>Because of this, we joined Open Collective to obtain non-profit equal level status which allows the organization to receive and manage donations transparently. Please support ThreatMatrix and all the community by choosing a plan (BRONZE, SILVER, etc).</p> <p> </p>"},{"location":"Submodules/ThreatMatrix/#gold","title":"\ud83e\udd47 GOLD","text":""},{"location":"Submodules/ThreatMatrix/#certego","title":"Certego","text":"<p>Certego is a MDR (Managed Detection and Response) and Threat Intelligence Provider based in Italy.</p> <p>ThreatMatrix was born out of Certego's Threat intelligence R&amp;D division and is constantly maintained and updated thanks to them.</p>"},{"location":"Submodules/ThreatMatrix/#the-honeynet-project","title":"The Honeynet Project","text":"<p>The Honeynet Project is a non-profit organization working on creating open source cyber security tools and sharing knowledge about cyber threats.</p> <p>Thanks to Honeynet, we are hosting a public demo of the application here. If you are interested, please contact a member of Honeynet to get access to the public service.</p>"},{"location":"Submodules/ThreatMatrix/#google-summer-of-code","title":"Google Summer of Code","text":"<p>Since its birth this project has been participating in the Google Summer of Code (GSoC)!</p> <p>If you are interested in participating in the next Google Summer of Code, check all the info available in the dedicated repository!</p>"},{"location":"Submodules/ThreatMatrix/#silver","title":"\ud83e\udd48 SILVER","text":""},{"location":"Submodules/ThreatMatrix/#threathunterai","title":"ThreatHunter.ai","text":"<p>ThreatHunter.ai\u00ae, is a 100% Service-Disabled Veteran-Owned Small Business started in 2007 under the name Milton Security Group. ThreatHunter.ai is the global leader in Dynamic Threat Hunting. Operating a true 24x7x365 Security Operation Center with AI/ML-enhanced human Threat Hunters, ThreatHunter.ai has changed the industry in how threats are found, and mitigated in real time. For over 15 years, our teams of Threat Hunters have stopped hundreds of thousands of threats and assisted organizations in defending against threat actors around the clock.</p>"},{"location":"Submodules/ThreatMatrix/#docker","title":"Docker","text":"<p>In 2021 ThreatMatrix joined the official Docker Open Source Program. This allows ThreatMatrix developers to easily manage Docker images and focus on writing the code. You may find the official ThreatMatrix Docker images here.</p>"},{"location":"Submodules/ThreatMatrix/#about-the-author-and-maintainers","title":"About the author and maintainers","text":"<p>Feel free to contact the main developers at any time on Twitter:</p> <ul> <li>KhulnaSoft DevSec: Author and principal maintainer</li> <li>Nx PKG: Backend Maintainer</li> <li>KhulnaSoft Lab: Frontend Maintainer</li> <li>KhulnaSoft BOT: Key Contributor</li> </ul>"},{"location":"Submodules/ThreatMatrix/docker/bin/","title":"Embedded binary list","text":""},{"location":"Submodules/ThreatMatrix/docker/bin/#osslsigncode","title":"Osslsigncode","text":"<p>We embedded the compiled version for Ubuntu for that can be retrieved from its original repo here.</p> <p>We decided to do not use the version shipped by default Ubuntu packages because it were too old (2.1)</p> <p>At the last time of writing we uploaded the version 2.6-dev</p>"},{"location":"Submodules/ThreatMatrix/frontend/","title":"ThreatMatrix - frontend","text":"<p>Built with @certego/certego-ui.</p>"},{"location":"Submodules/ThreatMatrix/frontend/#design-thesis","title":"Design thesis","text":"<ul> <li>Re-usable components/hooks/stores that other projects can also benefit from should be added to certego-ui package.</li> <li>ThreatMatrix specific:<ul> <li>components should be added to <code>src/components/common</code>.</li> <li>general hooks should be added to <code>src/hooks</code>.</li> <li>zustand stores hooks should be added to <code>src/stores</code>.</li> </ul> </li> </ul>"},{"location":"Submodules/ThreatMatrix/frontend/#directory-structure","title":"Directory Structure","text":"<pre><code>public/                                   public static assets\n|- icons/                                 icons/favicon\n|- index.html/                            root HTML file\nsrc/                                      source code\n|- components/                            pages and components\n|  |- auth/                               `authentication` (login, logout, OAuth pages)\n|  |- common/                             small re-usable components\n|  |- dashboard/                          dashboard page and charts\n|  |- home/                               landing/home page\n|  |- jobs/                               `api_app`\n|  |  |- result/                          JobResult.jsx\n|  |  |- table/                           JobsTable.jsx\n|  |- me/\n|  |  |- organization/                    `certego_saas.apps.organization`\n|  |  |- sessions/                        durin (sessions management)\n|  |- misc/\n|  |  |- notification/                    `certego_saas.apps.notifications`\n|  |- plugins/                            `api_app.analyzers_manager`, `api_app.connectors_manager`\n|  |- scan/                               new scan/job\n|  |- Routes.jsx                          lazy route-component mappings\n|- constants/                             constant values\n|  |- api.js                              API URLs\n|  |- environment.js                      environment variables\n|  |- index.js                            threatmatrix specific constants\n|- hooks/                                 react hooks\n|- layouts/                               header, main, footer containers\n|- stores/                                zustand stores hooks\n|- styles/                                scss files\n|- utils/                                 utility functions\n|- wrappers/                              Higher-Order components\n|- App.jsx                                App component\n|- index.jsx                              Root JS file (ReactDOM renderer)\n</code></pre>"},{"location":"Submodules/ThreatMatrix/frontend/#local-development-environment","title":"Local Development Environment","text":"<p>The frontend inside the docker containers does not hot-reload, so you need to use <code>CRA dev server</code> on your host machine to serve pages when doing development on the frontend, using docker nginx only as API source.</p> <ul> <li>Start ThreatMatrix containers (see docs). Original dockerized app is accessible on <code>http://localhost:80</code></li> </ul> <ul> <li>If you have not <code>node-js</code> installed, you have to do that. Follow the guide here. We tested this with NodeJS &gt;=16.6</li> </ul> <ul> <li>Install npm packages locally</li> </ul> <pre><code>cd ./frontend &amp;&amp; npm install\n</code></pre> <ul> <li>Start CRA dev server:</li> </ul> <pre><code>npm start\n</code></pre> <ul> <li>Now you can access the auto-reloading frontend on <code>http://localhost:3000</code>. It acts as proxy for API requests to original app web server.</li> </ul> <ul> <li>JS app main configs are available in <code>package.json</code>.</li> </ul> <ul> <li>(optional) Use local build of <code>certego-ui</code> package so it can also hot-reload. This is useful when you want to make changes in certego-ui and rapidly test them with ThreatMatrix. Refer here for setup instructions.</li> </ul>"},{"location":"Submodules/ThreatMatrix/frontend/#miscellaneous","title":"Miscellaneous","text":""},{"location":"Submodules/ThreatMatrix/frontend/#dependabot","title":"Dependabot","text":"<p>We have dependabot enabled for the React.js frontend application. The updates are scheduled for once a week.</p>"},{"location":"Submodules/ThreatMatrix/frontend/#external-docs","title":"External Docs","text":"<ul> <li>Create React App documentation.</li> <li>React documentation.</li> </ul>"},{"location":"Submodules/ThreatMatrix/integrations/malware_tools_analyzers/qiling/profiles/","title":"Please add your profile files here","text":""},{"location":"Submodules/ThreatMatrix/integrations/malware_tools_analyzers/qiling/rootfs/","title":"Please add your rootfs folder here","text":""},{"location":"Submodules/ThreatPot/","title":"ThreatPot","text":"<p>The project goal is to extract data of the attacks detected by a CyberPot or a cluster of them and to generate some feeds that can be used to prevent and detect attacks.</p> <p>Official announcement here.</p>"},{"location":"Submodules/ThreatPot/#documentation","title":"Documentation","text":"<p>Documentation about ThreatPot installation, usage, configuration and contribution can be found at this link</p>"},{"location":"Submodules/ThreatPot/#public-feeds","title":"Public feeds","text":"<p>There are public feeds provided by KhulnaSoft, Ltd in this site. Example</p> <p>Please do not perform too many requests to extract feeds or you will be banned.</p> <p>If you want to be updated regularly, please download the feeds only once every 10 minutes (this is the time between each internal update).</p> <p>To check all the available feeds, Please refer to our usage guide</p>"},{"location":"Submodules/ThreatPot/#enrichment-service","title":"Enrichment Service","text":"<p>ThreatPot provides an easy-to-query API to get the information available in GB regarding the queried observable (domain or IP address).</p> <p>To understand more, Please refer to our usage guide</p>"},{"location":"Submodules/ThreatPot/#run-threatpot-on-your-environment","title":"Run Threatpot on your environment","text":"<p>The tool has been created not only to provide the feeds from KhulnaSoft, Ltd's cluster of TPOTs.</p> <p>If you manage one or more T-POTs of your own, you can get the code of this application and run Threatpot on your environment. In this way, you are able to provide new feeds of your own.</p> <p>To install it locally, Please refer to our installation guide</p>"},{"location":"Submodules/ThreatPot/#sponsors","title":"Sponsors","text":""},{"location":"Submodules/ThreatPot/#certego","title":"Certego","text":"<p>Certego is a MDR (Managed Detection and Response) and Threat Intelligence Provider based in Italy.</p> <p>Started as a personal Christmas project from Matteo Lodi, since then ThreatPot is being improved mainly thanks to the efforts of the Certego Threat Intelligence Team.</p>"},{"location":"Submodules/ThreatPot/#khulnasoft-ltd","title":"KhulnaSoft, Ltd","text":"<p>KhulnaSoft, Ltd is a non-profit organization working on creating open source cyber security tools and sharing knowledge about cyber threats.</p>"},{"location":"Submodules/ThreatPot/FEEDS_LICENSE/","title":"FEEDS LICENSE","text":"<p>The data provided from the site https://threatpot.honeynet.org are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</p>"},{"location":"Submodules/ThreatPot/frontend/","title":"ThreatPot - frontend","text":"<p>Built with @certego/certego-ui.</p>"},{"location":"Submodules/ThreatPot/frontend/#design-thesis","title":"Design thesis","text":"<ul> <li>Re-usable components/hooks/stores that other projects can also benefit from should be added to certego-ui package.</li> <li>ThreatPot specific:<ul> <li>components should be added to <code>src/components</code>.</li> <li>general hooks should be added to <code>src/hooks</code>.</li> <li>zustand stores hooks should be added to <code>src/stores</code>.</li> </ul> </li> </ul>"},{"location":"Submodules/ThreatPot/frontend/#directory-structure","title":"Directory Structure","text":"<pre><code>public/                                   public static assets\n|- icons/                                 icons/favicon\n|- index.html/                            root HTML file\nsrc/                                      source code\n|- components/                            pages and components\n|  |- auth/                               `certego_saas.apps.auth` (login, logout pages)\n|  |- dashboard/                          dashboard page and charts\n|  |- home/                               landing/home page\n|  |- Routes.jsx                          lazy route-component mappings\n|- constants/                             constant values\n|  |- api.js                              API URLs\n|  |- environment.js                      environment variables\n|  |- index.js                            ThreatPot specific constants\n|- hooks/                                 react hooks\n|- layouts/                               header, main, footer containers\n|- stores/                                zustand stores hooks\n|- styles/                                scss files\n|- wrappers/                              Higher-Order components\n|- App.jsx                                App component\n|- index.jsx                              Root JS file (ReactDOM renderer)\n</code></pre>"},{"location":"Submodules/ThreatPot/frontend/#local-development-environment","title":"Local Development Environment","text":"<p>The frontend inside the docker containers does not hot-reload, so you need to use <code>CRA dev server</code> on your host machine to serve pages when doing development on the frontend, using docker nginx only as API source.</p> <ul> <li>Start ThreatPot containers (see docs). Original dockerized app is accessible on <code>http://localhost:80</code></li> </ul> <ul> <li>If you have not <code>node-js</code> installed, you have to do that. Follow the guide here. We tested this with NodeJS &gt;=16.6</li> </ul> <ul> <li>Install npm packages locally</li> </ul> <pre><code>cd ./frontend &amp;&amp; npm install\n</code></pre> <ul> <li>Start CRA dev server:</li> </ul> <pre><code>npm start\n</code></pre> <ul> <li>Now you can access the auto-reloading frontend on <code>http://localhost:3001</code>. It acts as proxy for API requests to original app web server.</li> </ul> <ul> <li>JS app main configs are available in <code>package.json</code> and <code>enviroments.js</code>.</li> </ul>"},{"location":"Submodules/ThreatPot/frontend/#external-docs","title":"External Docs","text":"<ul> <li>Create React App documentation.</li> <li>React documentation.</li> </ul>"},{"location":"Submodules/pythreatmatrix/","title":"PyThreatMatrix","text":"<p>Robust Python SDK and Command Line Client for interacting with ThreatMatrix's API.</p>"},{"location":"Submodules/pythreatmatrix/#features","title":"Features","text":"<ul> <li>Easy one-time configuration with self documented help and hints along the way.</li> <li>Request new analysis for observables and files.<ul> <li>Select which analyzers you want to run for every analysis you perform.</li> <li>Choose whether you want to HTTP poll for the analysis to finish or not.</li> </ul> </li> <li>List all jobs or view one job in a prettified tabular form.</li> <li>List all tags or view one tag in a prettified tabular form.</li> <li>Tabular view of the <code>analyzer_config.json</code> and <code>connector_config.json</code> from ThreatMatrix with RegEx matching capabilities.</li> </ul>"},{"location":"Submodules/pythreatmatrix/#demo","title":"Demo","text":""},{"location":"Submodules/pythreatmatrix/#installation","title":"Installation","text":"<pre><code>$ pip3 install pythreatmatrix\n</code></pre> <p>For development/testing, <code>pip3 install pythreatmatrix[dev]</code></p>"},{"location":"Submodules/pythreatmatrix/#quickstart","title":"Quickstart","text":""},{"location":"Submodules/pythreatmatrix/#as-command-line-client","title":"As Command Line Client","text":"<p>On successful installation, The <code>pythreatmatrix</code> entryscript should be directly invokable. For example,</p> <pre><code>$ pythreatmatrix\nUsage: pythreatmatrix [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -d, --debug  Set log level to DEBUG\n  --version    Show the version and exit.\n  -h, --help   Show this message and exit.\n\nCommands:\n  analyse                Send new analysis request\n  analyzer-healthcheck   Send healthcheck request for an analyzer...\n  config                 Set or view config variables\n  connector-healthcheck  Send healthcheck request for a connector\n  get-analyzer-config    Get current state of `analyzer_config.json` from...\n  get-connector-config   Get current state of `connector_config.json` from...\n  jobs                   Manage Jobs\n  tags                   Manage tags\n</code></pre>"},{"location":"Submodules/pythreatmatrix/#as-a-library-sdk","title":"As a library / SDK","text":"<pre><code>from pythreatmatrix import ThreatMatrix\nobj = ThreatMatrix(\"&lt;your_api_key&gt;\", \"&lt;your_threatmatrix_instance_url&gt;\", \"optional&lt;path_to_pem_file&gt;\", \"optional&lt;proxies&gt;\")\n</code></pre> <p>For more comprehensive documentation, please see https://pythreatmatrix.readthedocs.io/.</p>"},{"location":"Submodules/pythreatmatrix/#changelog","title":"Changelog","text":"<p>View CHANGELOG.md.</p>"},{"location":"Submodules/pythreatmatrix/#faq","title":"FAQ","text":""},{"location":"Submodules/pythreatmatrix/#generate-api-key","title":"Generate API key","text":"<p>You need a valid API key to interact with the ThreatMatrix server. Keys should be created from the admin interface of ThreatMatrix: you have to go in the Durin section (click on <code>Auth tokens</code>) and generate a key there.</p>"},{"location":"Submodules/pythreatmatrix/#incompatibility-after-version-30","title":"Incompatibility after version 3.0","text":"<p>We did a complete rewrite of the PyThreatMatrix client and CLI both for the version <code>3.0.0</code>. We very much recommend you to update to the latest version to enjoy all new features.</p>"},{"location":"Submodules/pythreatmatrix/#old-auth-method-jwt-token-authentication","title":"(old auth method) JWT Token Authentication","text":"<p>this auth was available in ThreatMatrix versions &lt;1.8.0 and pythreatmatrix versions &lt;2.0.0</p> <p>From the admin interface of ThreatMatrix, you have to go in the Outstanding tokens section and generate a token there.</p> <p>You can use it by pasting it into the file api_token.txt.</p>"},{"location":"Submodules/reconPoint/","title":"Recon Point","text":""},{"location":"Submodules/reconPoint/#what-is-reconpoint","title":"What is reconPoint?","text":"<p>reconPoint is your ultimate web application reconnaissance suite, designed to supercharge the recon process for security pros, pentesters, and bug bounty hunters. It is a go-to tool that simplifies and streamlines reconnaissance, featuring configurable engines, data correlation, continuous monitoring, database-backed reconnaissance data, and an intuitive user interface. reconPoint redefines how you gather critical information about target web applications.</p> <p>Traditional reconnaissance tools often fall short in configurability and efficiency. reconPoint addresses these shortcomings and emerges as an excellent alternative to existing commercial tools.</p> <p>Watch reconPoint 2.0-jasper release trailer here!</p>"},{"location":"Submodules/reconPoint/#documentation","title":"Documentation","text":"<p>Detailed documentation available at https://recon.khulnasoft.com</p>"},{"location":"Submodules/reconPoint/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Recon Point<ul> <li>What is reconPoint?</li> <li>Documentation</li> <li>Table of Contents</li> <li>About reconPoint</li> <li>Workflow</li> <li>Features</li> <li>Quick Installation<ul> <li>Quick Setup for Ubuntu/VPS</li> <li>Installation on Other Platforms</li> <li>Installation Video Tutorial</li> </ul> </li> <li>Updating</li> <li>Community-Curated Videos</li> <li>Screenshots<ul> <li>Scan Results</li> <li>General Usage</li> <li>Initiating Subscan</li> <li>Recon Data filtering</li> <li>Report Generation</li> <li>Toolbox</li> <li>Adding Custom tool in Tools Arsenal</li> </ul> </li> <li>Contributing</li> <li>Submitting issues</li> <li>First-time Open Source contributors</li> <li>reconPoint Support</li> <li>Support and Sponsoring</li> <li>Reporting Security Vulnerabilities</li> <li>License</li> </ul> </li> </ul>"},{"location":"Submodules/reconPoint/#about-reconpoint","title":"About reconPoint","text":"<p>reconPoint is a game-changing reconnaissance suite. It has turbocharged features for security professionals, penetration testers, and bug bounty hunters. It includes:</p> <ul> <li>Advanced scan engines</li> <li>Data correlation</li> <li>Continuous monitoring</li> <li>GPT-powered Vulnerability Reports</li> <li>Project management with role-based access control</li> </ul> <p>\ud83e\uddbe reconPoint performs end-to-end reconnaissance, from subdomain discovery to vulnerability scanning. It supports WHOIS identification, WAF detection, and more, making it a go-to tool for efficient reconnaissance.</p> <p>\ud83d\uddc3\ufe0f reconPoint integrates with a database, providing seamless data correlation and organization, allowing you to filter reconnaissance data effortlessly.</p> <p>\ud83d\udd27 Highly configurable, reconPoint uses a YAML-based configuration system, offering customizable scan engines for all types of recon.</p> <p>\ud83d\udc8e\u00a0\u00a0Subscans: Subscan is a game-changing feature in reconPoint, setting it apart as the only open-source tool of its kind to offer this capability. With Subscan, waiting for the entire pipeline to complete is a thing of the past. Now, users can swiftly respond to newfound discoveries during reconnaissance. Whether you've stumbled upon an intriguing subdomain and wish to conduct a focused port scan or want to delve deeper with a vulnerability assessment, reconPoint has you covered.</p> <p>\ud83d\udcc3\u00a0\u00a0 PDF Reports: In addition to its robust reconnaissance capabilities, reconPoint goes the extra mile by simplifying the report generation process, recognizing the crucial role that PDF reports play in the realm of end-to-end reconnaissance. Users can effortlessly generate and customize PDF reports to suit their exact needs. Whether it's a Full Scan Report, Vulnerability Report, or a concise reconnaissance report, reconPoint provides the flexibility to choose the report type that best communicates your findings. Moreover, the level of customization is unparalleled, allowing users to select report colors, fine-tune executive summaries, and even add personalized touches like company names and footers. With GPT integration, your reports aren't just a report, with remediation steps, and impacts, you get 360-degree view of the vulnerabilities you've uncovered.</p> <p>\ud83d\udd16\u00a0 \u00a0 Say Hello to Projects! reconPoint 2.0 introduces a powerful addition that enables you to efficiently organize your web application reconnaissance efforts. With this feature, you can create distinct project spaces, each tailored to a specific purpose, such as personal bug bounty hunting, client engagements, or any other specialized recon task. Each projects will have separate dashboard and all the scan results will be separated from each project, while scan engines and configuration will be shared across all the projects.</p> <p>\u2699\ufe0f\u00a0 \u00a0 Roles and Permissions! In reconPoint 2.0, we've taken your web application reconnaissance to a whole new level of control and security. Now, you can assign distinct roles to your team members\u2014Sys Admin, Penetration Tester, and Auditor\u2014each with precisely defined permissions to tailor their access and actions within the reconPoint ecosystem.</p> <ul> <li>\ud83d\udd10 Sys Admin: Sys Admin is a superuser that has permission to modify system and scan related configurations, scan engines, create new users, add new tools etc. Superuser can initiate scans and subscans effortlessly.</li> <li>\ud83d\udd0d Penetration Tester: Penetration Tester will be allowed to modify and initiate scans and subscans, add or update targets, etc. A penetration tester will not be allowed to modify system configurations.</li> <li>\ud83d\udcca Auditor: Auditor can only view and download the report. An auditor can not change any system or scan related configurations nor can initiate any scans or subscans.</li> </ul> <p>\ud83d\ude80\u00a0\u00a0 GPT Vulnerability Report Generation: Get ready for the future of penetration testing reports with reconPoint's groundbreaking feature: \"GPT-Powered Report Generation\"! With the power of OpenAI's GPT, reconPoint now provides you with detailed vulnerability descriptions, remediation strategies, and impact assessments that read like they were written by a human security expert! But that's not all! Our GPT-driven reports go the extra mile by scouring the web for related news articles, blogs, and references, so you have a 360-degree view of the vulnerabilities you've uncovered. With reconPoint 2.0 revolutionize your penetration testing game and impress your clients with reports that are not just informative but engaging and comprehensive with detailed analysis on impact assessment and remediation strategies.</p> <p>\ud83e\udd77\u00a0\u00a0 GPT-Powered Attack Surface Generation: With reconPoint 2.0, reconPoint seamlessly integrates with GPT to identify the attacks that you can likely perform on a subdomain. By making use of reconnaissance data such as page title, open ports, subdomain name etc. reconPoint can advise you the attacks you could perform on a target. reconPoint will also provide you the rationale on why the specific attack is likely to be successful.</p> <p>\ud83e\udded\u00a0\u00a0Continuous monitoring: Continuous monitoring is at the core of reconPoint's mission, and it's robust continuous monitoring feature ensures that their targets are under constant scrutiny. With the flexibility to schedule scans at regular intervals, penetration testers can effortlessly stay informed about their targets. What sets reconPoint apart is its seamless integration with popular notification channels such as Discord, Slack, and Telegram, delivering real-time alerts for newly discovered subdomains, vulnerabilities, or any changes in reconnaissance data.</p> <p></p>"},{"location":"Submodules/reconPoint/#workflow","title":"Workflow","text":""},{"location":"Submodules/reconPoint/#features","title":"Features","text":"<ul> <li>Reconnaissance:<ul> <li>Subdomain Discovery</li> <li>IP and Open Ports Identification</li> <li>Endpoints Discovery</li> <li>Directory/Files fuzzing</li> <li>Screenshot Gathering</li> <li>Vulnerability Scan<ul> <li>Nuclei</li> <li>Dalfox XSS Scanner</li> <li>CRLFuzzer</li> <li>Misconfigured S3 Scanner</li> </ul> </li> <li>WHOIS Identification</li> <li>WAF Detection</li> </ul> </li> <li>OSINT Capabilities<ul> <li>Meta info Gathering</li> <li>Employees Gathering</li> <li>Email Address gathering</li> <li>Google Dorking for sensitive info and urls</li> </ul> </li> <li>Projects, create distinct project spaces, each tailored to a specific purpose, such as personal bug bounty hunting, client engagements, or any other specialized recon task.</li> <li>Perform Advanced Query lookup using natural language alike and, or, not operations</li> <li>Highly configurable YAML-based Scan Engines</li> <li>Support for Parallel Scans</li> <li>Support for Subscans</li> <li>Recon Data visualization</li> <li>GPT Vulnerability Description, Impact and Remediation generation</li> <li>GPT Attack Surface Generator</li> <li>Multiple Roles and Permissions to cater a team's need</li> <li>Customizable Alerts/Notifications on Slack, Discord, and Telegram</li> <li>Automatically report Vulnerabilities to HackerOne</li> <li>Recon Notes and Todos</li> <li>Clocked Scans (Run reconnaissance exactly at X Hours and Y minutes) and Periodic Scans (Runs reconnaissance every X minutes/- hours/days/week)</li> <li>Proxy Support</li> <li>Screenshot Gallery with Filters</li> <li>Powerful recon data filtering with autosuggestions</li> <li>Recon Data changes, find new/removed subdomains/endpoints</li> <li>Tag targets into the Organization</li> <li>Smart Duplicate endpoint removal based on page title and content length to cleanup the reconnaissance data</li> <li>Identify Interesting Subdomains</li> <li>Custom GF patterns and custom Nuclei Templates</li> <li>Edit tool-related configuration files (Nuclei, Subfinder, Naabu, amass)</li> <li>Add external tools from GitHub/Go</li> <li>Interoperable with other tools, Import/Export Subdomains/Endpoints</li> <li>Import Targets via IP and/or CIDRs</li> <li>Report Generation</li> <li>Toolbox: Comes bundled with most commonly used tools during penetration testing such as whois lookup, CMS detector, CVE lookup, etc.</li> <li>Identification of related domains and related TLDs for targets</li> <li>Find actionable insights such as Most Common Vulnerability, Most Common CVE ID, Most Vulnerable Target/Subdomain, etc.</li> <li>You can now use local LLMs for Attack surface identification and vulnerability description (NEW: reconPoint 2.1.0)</li> </ul>"},{"location":"Submodules/reconPoint/#quick-installation","title":"Quick Installation","text":""},{"location":"Submodules/reconPoint/#quick-setup-for-ubuntuvps","title":"Quick Setup for Ubuntu/VPS","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/khulnasoft/reconpoint &amp;&amp; cd reconpoint\n</code></pre> </li> <li> <p>Configure the environment</p> <pre><code>nano .env\n</code></pre> <p>Ensure you change the <code>POSTGRES_PASSWORD</code> for security.</p> </li> <li> <p>(Optional) For non-interactive install, set admin credentials in <code>.env</code></p> <pre><code>DJANGO_SUPERUSER_USERNAME=yourUsername\nDJANGO_SUPERUSER_EMAIL=YourMail@example.com\nDJANGO_SUPERUSER_PASSWORD=yourStrongPassword\n</code></pre> <p>If you need to carry out a non-interactive installation, you can setup the login, email and password of the web interface admin directly from the .env file (instead of manually setting them from prompts during the installation process). This option can be interesting for automated installation (via ansible, vagrant, etc.).</p> <ul> <li><code>DJANGO_SUPERUSER_USERNAME</code>: web interface admin username (used to login to the web interface).</li> </ul> <ul> <li><code>DJANGO_SUPERUSER_EMAIL</code>: web interface admin email.</li> </ul> <ul> <li><code>DJANGO_SUPERUSER_PASSWORD</code>: web interface admin password (used to login to the web interface).</li> </ul> </li> <li> <p>Adjust Celery worker scaling in <code>.env</code></p> <pre><code>MAX_CONCURRENCY=80\nMIN_CONCURRENCY=10\n</code></pre> <p><code>MAX_CONCURRENCY</code>: This parameter specifies the maximum number of reconPoint's concurrent Celery worker processes that can be spawned. In this case, it's set to 80, meaning that the application can utilize up to 80 concurrent worker processes to execute tasks concurrently. This is useful for handling a high volume of scans or when you want to scale up processing power during periods of high demand. If you have more CPU cores, you will need to increase this for maximised performance.</p> <p><code>MIN_CONCURRENCY</code>: On the other hand, MIN_CONCURRENCY specifies the minimum number of concurrent worker processes that should be maintained, even during periods of lower demand. In this example, it's set to 10, which means that even when there are fewer tasks to process, at least 10 worker processes will be kept running. This helps ensure that the application can respond promptly to incoming tasks without the overhead of repeatedly starting and stopping worker processes.</p> <p>These settings allow for dynamic scaling of Celery workers, ensuring that the application efficiently manages its workload by adjusting the number of concurrent workers based on the workload's size and complexity.</p> <p>Here is the ideal value for <code>MIN_CONCURRENCY</code> and <code>MAX_CONCURRENCY</code> depending on the number of RAM your machine has:</p> <ul> <li>4GB: <code>MAX_CONCURRENCY=10</code></li> <li>8GB: <code>MAX_CONCURRENCY=30</code></li> <li>16GB: <code>MAX_CONCURRENCY=50</code></li> </ul> <p>This is just an ideal value which developers have tested and tried out and works! But feel free to play around with the values.  Maximum number of scans is determined by various factors, your network bandwidth, RAM, number of CPUs available. etc</p> </li> <li> <p>Run the installation script:</p> <pre><code>sudo ./install.sh\n</code></pre> <p>For non-interactive install: <code>sudo ./install.sh -n</code></p> <p>Note: If needed, run <code>chmod +x install.sh</code> to grant execution permissions.</p> </li> </ol> <p>reconPoint can now be accessed from https://127.0.0.1 or if you're on the VPS https://your_vps_ip_address</p> <p>Unless you are on development branch, please do not access reconPoint via any ports</p>"},{"location":"Submodules/reconPoint/#installation-on-other-platforms","title":"Installation on Other Platforms","text":"<p>For Mac, Windows, or other systems, refer to our detailed installation guide https://recon.khulnasoft.com/install/detailed/</p>"},{"location":"Submodules/reconPoint/#installation-video-tutorial","title":"Installation Video Tutorial","text":"<p>If you encounter any issues during installation or prefer a visual guide, one of our community members has created an excellent installation video for Kali Linux installation. You can find it here: https://www.youtube.com/watch?v=7OFfrU6VrWw</p> <p>Please note: This is community-curated content and is not owned by reconPoint. The installation process may change, so please refer to the official documentation for the most up-to-date instructions.</p>"},{"location":"Submodules/reconPoint/#updating","title":"Updating","text":"<ol> <li> <p>To update reconPoint, run:</p> <pre><code>cd reconpoint &amp;&amp;  sudo ./update.sh\n</code></pre> <p>If <code>update.sh</code> lacks execution permissions, use:</p> <pre><code>sudo chmod +x update.sh\n</code></pre> </li> </ol> <p></p>"},{"location":"Submodules/reconPoint/#community-curated-videos","title":"Community-Curated Videos","text":"<p>reconPoint has a vibrant community that often creates helpful content about installation, features, and usage. Below is a collection of community-curated videos that you might find useful. Please note that these videos are not official reconPoint content, and the information they contain may become outdated as reconPoint evolves.</p> <p>Always refer to the official documentation for the most up-to-date and accurate information. If you've created a video about reconPoint and would like it featured here, please send a pull request updating this table.</p> Video Title Language Uploader Date Link reconPoint Installation on Kali Linux English Secure the Cyber World 2024-02-29 Watch Resultados do ReconPoint - Automa\u00e7\u00e3o para Recon Portuguese Guia An\u00f4nima 2023-04-18 Watch reconPoint Introduction Moroccan Arabic Th3 Hacker News Bdarija 2021-07-27 Watch <p>We appreciate the community's contributions in creating these resources.</p> <p></p>"},{"location":"Submodules/reconPoint/#screenshots","title":"Screenshots","text":""},{"location":"Submodules/reconPoint/#scan-results","title":"Scan Results","text":""},{"location":"Submodules/reconPoint/#general-usage","title":"General Usage","text":""},{"location":"Submodules/reconPoint/#initiating-subscan","title":"Initiating Subscan","text":""},{"location":"Submodules/reconPoint/#recon-data-filtering","title":"Recon Data filtering","text":""},{"location":"Submodules/reconPoint/#report-generation","title":"Report Generation","text":""},{"location":"Submodules/reconPoint/#toolbox","title":"Toolbox","text":""},{"location":"Submodules/reconPoint/#adding-custom-tool-in-tools-arsenal","title":"Adding Custom tool in Tools Arsenal","text":""},{"location":"Submodules/reconPoint/#contributing","title":"Contributing","text":"<p>We welcome contributions of all sizes! The open-source community thrives on collaboration, and your input is invaluable. Whether you're fixing a typo, improving UI, or adding new features, every contribution matters.</p> <p>How you can contribute:</p> <ul> <li>Code improvements</li> <li>Documentation updates</li> <li>Bug reports and fixes</li> <li>New feature suggestions and implementations</li> <li>UI/UX enhancements</li> </ul> <p>To get started:</p> <ol> <li>Check our Contributing Guide</li> <li>Pick an open issue or propose a new one</li> <li>Fork the repository and create your branch</li> <li>Make your changes and submit a pull request</li> </ol> <p>Remember, no contribution is too small. Your efforts help make reconPoint better for everyone!</p> <p></p>"},{"location":"Submodules/reconPoint/#submitting-issues","title":"Submitting issues","text":"<p>When submitting issues, provide as much valuable information as possible to help developers resolve the problem quickly. Follow these steps to gather detailed debug information:</p> <ol> <li> <p>Enable Debug Mode:</p> <ul> <li>Edit <code>web/entrypoint.sh</code> in the project root</li> <li> <p>Add <code>export DEBUG=1</code> at the top of the file:</p> <pre><code>#!/bin/bash\n\nexport DEBUG=1\n\npython3 manage.py migrate\npython3 manage.py runserver 0.0.0.0:8000\n\nexec \"$@\"\n</code></pre> </li> </ul> <ul> <li>Restart the web container: <code>docker-compose restart web</code></li> </ul> </li> <li> <p>View Debug Output:</p> <ul> <li>Run <code>make logs</code> to see the full stack trace</li> <li>Check the browser's developer console for XHR requests with 500 errors</li> </ul> </li> <li> <p>Example Debug Output:</p> <p>```text  web_1          |   File \"/usr/local/lib/python3.10/dist-packages/celery/app/task.py\", line 411, in call  web_1          |     return self.run(args, *kwargs)  web_1          | TypeError: run_command() got an unexpected keyword argument 'echo'</p> </li> <li> <p>Submit Your Issue:</p> <ul> <li>Include the full stack trace in your GitHub issue</li> <li>Describe the steps to reproduce the problem</li> <li>Mention any relevant system information</li> </ul> </li> <li> <p>Disable Debug Mode:</p> <ul> <li>Set <code>DEBUG=0</code> in <code>web/entrypoint.sh</code></li> <li>Restart the web container</li> </ul> </li> </ol> <p>By providing this detailed information, you significantly help developers identify and fix issues more efficiently.</p> <p></p>"},{"location":"Submodules/reconPoint/#first-time-open-source-contributors","title":"First-time Open Source contributors","text":"<p>reconPoint is an open-source project that welcomes contributors of all experience levels, including beginners. If you've never contributed to open source before, we encourage you to start here!</p> <ul> <li>We're proud to support your first Pull Request (PR)</li> <li>Check our open issues for starter-friendly tasks</li> <li>Don't hesitate to ask questions in our community channels</li> </ul> <p>Your contribution, no matter how small, is valuable to us.</p> <p></p>"},{"location":"Submodules/reconPoint/#reconpoint-support","title":"reconPoint Support","text":"<p>Before seeking support:</p> <ul> <li>Please carefully read the README and documentation at recon.khulnasoft.com.</li> <li>Most common questions and issues are addressed there.</li> </ul> <p>If you still need assistance:</p> <ul> <li>Do not use GitHub issues for support requests.</li> <li>Join our community-maintained Discord channel.</li> </ul> <p>Please note:</p> <ul> <li>The Discord channel is maintained by the community.</li> <li>While we strive to help, there's no guarantee of support or response time.</li> <li>For confirmed bugs or feature requests, consider opening a GitHub issue.</li> </ul> <p></p>"},{"location":"Submodules/reconPoint/#support-and-sponsoring","title":"Support and Sponsoring","text":"<p>reconPoint is a passion project developed in my free time, alongside my day job. Your support helps keep this project alive and growing. Here's how you can contribute:</p> <ul> <li>Add a GitHub Star to the project.</li> <li>Share about reconPoint on social media or in blog posts</li> <li>Nominate me for GitHub Stars?</li> <li>Use my DigitalOcean referral link to get $100 credit (I receive $25)</li> </ul> <p>Your support, whether through donations or simply giving a star, tells me that reconPoint is valuable to you. It motivates me to continue improving and adding features to make reconPoint the go-to tool for reconnaissance.</p> <p>Thank you for your support!</p> <p></p>"},{"location":"Submodules/reconPoint/#reporting-security-vulnerabilities","title":"Reporting Security Vulnerabilities","text":"<p>We appreciate your efforts to responsibly disclose your findings and will make every effort to acknowledge your contributions.</p> <p>To report a security vulnerability, please follow these steps:</p> <ol> <li> <p>Do Not disclose the vulnerability publicly on GitHub issues or any other public forum.</p> </li> <li> <p>Go to the Security tab of the reconPoint repository.</p> </li> <li> <p>Click on \"Report a vulnerability\" to open GitHub's private vulnerability reporting form.</p> </li> <li> <p>Provide a detailed description of the vulnerability, including:</p> <ul> <li>Steps to reproduce</li> <li>Potential impact</li> <li>Any suggested fixes or mitigations (if you have them)</li> </ul> </li> <li> <p>I will review your report and respond as quickly as possible, usually within 48-72 hours.</p> </li> <li> <p>Please allow some time to investigate and address the vulnerability before disclosing it to others.</p> </li> </ol> <p>We are committed to working with security researchers to verify and address any potential vulnerabilities reported to us. After fixing the issue, we will publicly acknowledge your responsible disclosure, unless you prefer to remain anonymous.</p> <p>Thank you for helping to keep reconPoint and its users safe!</p> <p></p>"},{"location":"Submodules/reconPoint/#license","title":"License","text":"<p>Distributed under the GNU GPL v3 License. See LICENSE for more information.</p> <p></p> <p>Note: Parts of this README were written or refined using AI language models.</p>"},{"location":"Submodules/reconPoint/CHANGELOG/","title":"Changelog","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#v220","title":"v2.2.0","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed","title":"What's Changed","text":"<ul> <li>Update Dockerfile by @gitworkflows in https://github.com/khulnasoft/reconpoint/pull/37</li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.1.1...v2.2.0</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#213","title":"2.1.3","text":"<p>Release Date: Aug 18, 2024</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed_1","title":"What's Changed","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#security-update","title":"Security Update","text":"<ul> <li>(Security) CVE-2023-50094 Stored Cross-Site Scripting (XSS) via DNS Record Poisoning reported by @touhidshaikh Advisory https://github.com/khulnasoft/reconpoint/security/advisories/GHSA-96q4-fj2m-jqf7</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>remove redundant docker environment variables by @jxdv in https://github.com/khulnasoft/reconpoint/pull/1353</li> <li>fix: reconPoint installation issue due to orjson and langchain #1362 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1363</li> <li> </li> <li>chores: Add error handling for the curl command by @gitworkflows in https://github.com/khulnasoft/reconpoint/pull/1367</li> <li>Update Github Actions Workflows by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1369</li> <li>chores: Fix docker build on master by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1373</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#1364-fix-whois-lookup-and-improve-performance-by-executing-various-modules-of-whois-lookup-to-run-concurrently-by-khulnasoft-in-httpsgithubcomkhulnasoftreconpointpull1368","title":"1364 FIx whois lookup and improve performance by executing various modules of whois lookup to run concurrently by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1368","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#new-contributors","title":"New Contributors","text":"<ul> <li>@gitworkflows made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1367</li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.1.2...v2.1.3</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#212","title":"2.1.2","text":"<p>Release Date: July 30, 2024</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed_2","title":"What's Changed","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#security-update_1","title":"Security update","text":"<ul> <li>(Security) CVE-2023-50094 Fix Authenticated command injection in WAF detection tool reported by @n-thumann Advisory https://github.com/khulnasoft/reconpoint/security/advisories/GHSA-fx7f-f735-vgh4</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Fix issue while initiating periodic and clocked scan #1322 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1328</li> <li>Fix 500 error on \"Test Hackerone api Key\" by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1332</li> <li>UI Typos and bug Fixes #1333 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1334</li> <li>Fix error during tool update Fixes #1152 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1335</li> <li>Upgrade setuptools to 72.1.0 to resolve installation error by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1338</li> <li>(chores) Fix github pages build by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1339</li> <li>Fix subdomain import for subdomains with suffixes more than 4 chars Fixes #1128 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1340</li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.1.1...v2.1.2</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#211","title":"2.1.1","text":"<p>Release Date: July 20, 2024</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed-and-fixed","title":"What's Changed and Fixed","text":"<ul> <li>Update contribution guidelines reference by @emmanuel-ferdman in https://github.com/khulnasoft/reconpoint/pull/1286</li> <li>fix xss on page title fix #1185 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1296</li> <li>fix context key error #1263 #1209 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1294</li> <li>fix xss on vulnerability description payloads #1262 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1298</li> <li>(bug) fix screenshot csv parser #1299 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1300</li> <li>(Security) Fixes #1202 bug risk of leaking the scan result files by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1301</li> <li>Fix #1291 Refactor Makefiles for windows/linux to accomodate both v1 and v2 of docker compose by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1302</li> <li>Fix custom_header to accept multiple headers using custom_headers by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1303</li> <li>Handle hash in url, added navigation for Tabs, Fixes #1155 bug href link with html id does not link to the expected url by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1306</li> <li>Optimize uninstall scripts to perform operations only related to reconPoint Fixes # 1187 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1307</li> <li>Added validators to validate URL fixes #1176 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1308</li> <li>Fix LLM/langchain issue for fetching vulnerability report using local LLM model Fixed #1292 local model dont use fetch gpt vulnerability details by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1311</li> <li>Fixes for Clocked and Periodic Scans Fix #1287 Fixes #1015 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1313</li> <li>Fix Not able to add todo from All Subdomains Section Fixes #1310 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1314</li> <li>Fix #1315 Fix for todo URLs not compatible with slugs by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1316</li> <li>Fixes #1122 But in port service lookup that caused multiple entries of Port with same port number but different service name/description by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1317</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@emmanuel-ferdman made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1286</li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.1.0...v2.1.1</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#210","title":"2.1.0","text":"<p>Release Date: June 22, 2024</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed_3","title":"What's Changed","text":"<ul> <li>ARM support</li> <li>Add LLM Toolkit by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1259</li> <li>use django-env by @fopina in https://github.com/khulnasoft/reconpoint/pull/1230</li> <li>Add Lark to notifications. by @iuime in https://github.com/khulnasoft/reconpoint/pull/1137</li> <li>Added restart: always to redis container by @null-ref-0000 in https://github.com/khulnasoft/reconpoint/pull/1275</li> <li>Dockerfile cleanup: reduce image size 3x by @sa7mon in https://github.com/khulnasoft/reconpoint/pull/1212</li> <li>Support for ARM-based platforms and remove obsolete composer version by @metehan-arslan in https://github.com/khulnasoft/reconpoint/pull/1242</li> <li>Fix importing CIDR blocks by @pbehnke in https://github.com/khulnasoft/reconpoint/pull/1205</li> <li>Added SAN extension to the generated certs by @michschl in https://github.com/khulnasoft/reconpoint/pull/1282</li> <li>Release/2.1.0 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1147</li> <li>Dockerfile Build Multiple Platforms by @vncloudsco in https://github.com/khulnasoft/reconpoint/pull/1210</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#new-contributors_2","title":"New Contributors","text":"<ul> <li>@fopina made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1230</li> <li>@iuime made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1137</li> <li>@null-ref-0000 made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1275</li> <li>@sa7mon made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1212</li> <li>@metehan-arslan made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1242</li> <li>@pbehnke made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1205</li> <li>@michschl made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1282</li> <li>@vncloudsco made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1210</li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.0.6...v2.1.0</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#206","title":"2.0.6","text":"<p>Release Date: May 11, 2024</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed_4","title":"What's Changed","text":"<ul> <li>Fix installation error and celery workers having issues with httpcore</li> <li>remove duplicate gospider references by @Talanor in https://github.com/khulnasoft/reconpoint/pull/1245</li> <li>Fix \"subdomain\" s3 bucket by @Talanor in https://github.com/khulnasoft/reconpoint/pull/1244</li> <li>Fix Txt File Var Declaration by @specters312 in https://github.com/khulnasoft/reconpoint/pull/1239</li> <li>Bug Correction: When dumping and loading customscanengines by @TH3xACE in https://github.com/khulnasoft/reconpoint/pull/1224</li> <li>Fix/infoga removal by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1249</li> <li>Fix #1241 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1251</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#new-contributors_3","title":"New Contributors","text":"<ul> <li>@Talanor made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1245</li> <li>@specters312 made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1239</li> <li>@TH3xACE made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1224</li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.0.5...v2.0.6</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#205","title":"2.0.5","text":"<p>Release Date: April 20, 2024</p> <ul> <li>Fix #1234 reconPoint unable to load celery tasks due to mismatched celery and redis versions</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#204","title":"2.0.4","text":"<p>Release Date: April 18, 2024</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed_5","title":"What's Changed","text":"<ul> <li>chore: update version number to 2.0.3 by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/1180</li> <li>Fix various ffuf bugs by @yarysp in https://github.com/khulnasoft/reconpoint/pull/1199</li> <li>Set and update default YAML config with all latest vars by @yarysp in https://github.com/khulnasoft/reconpoint/pull/1200</li> <li>Add checks for placeholder in custom tool task by @yarysp in https://github.com/khulnasoft/reconpoint/pull/1201</li> <li>Whatportis - Replace purge by truncate to prevent port import error by @yarysp in https://github.com/khulnasoft/reconpoint/pull/1203</li> <li>ops(installation): fix nano not being installed when absent by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/1143</li> <li>Complete dev environment to debug/code easily by @yarysp in https://github.com/khulnasoft/reconpoint/pull/1196</li> <li>Revert \"Complete dev environment to debug/code easily\" by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1225</li> <li>Update README.md | Fixed 1 broken link to the regine.wiki by @jostasik in https://github.com/khulnasoft/reconpoint/pull/1226</li> <li>Fix uninitialised variable cmd in custom_subdomain_tools by @cpandya2909 in https://github.com/khulnasoft/reconpoint/pull/1207</li> <li>[FIX] security: OS Command Injection vulnerability (x2) #1219 by @0xtejas in https://github.com/khulnasoft/reconpoint/pull/1227</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#new-contributors_4","title":"New Contributors","text":"<ul> <li>@yarysp made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1199</li> <li>@jostasik made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1226</li> <li>@cpandya2909 made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1207</li> <li>@0xtejas made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1227</li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.0.3...v2.0.4</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#203","title":"2.0.3","text":"<p>Release Date: January 25, 2024</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed_6","title":"What's Changed","text":"<ul> <li>CI: update GitHub action versions by @jxdv in https://github.com/khulnasoft/reconpoint/pull/1136</li> <li>Fixed (subdomain_discovery | ERROR | local variable 'use_amass_config' referenced before assignment) by @Deathpoolxrs in https://github.com/khulnasoft/reconpoint/pull/1149</li> <li>chore: update LICENSE by @jxdv in https://github.com/khulnasoft/reconpoint/pull/1153</li> <li>Fix subdomains list empty in Target by @psyray in https://github.com/khulnasoft/reconpoint/pull/1166</li> <li>Fix top menu text overflow in low resolution by @psyray in https://github.com/khulnasoft/reconpoint/pull/1167</li> <li>Update auto comment workflow due to deprecation warnings by @ErdemOzgen in https://github.com/khulnasoft/reconpoint/pull/1126</li> <li>Change Redirect URL after login to prevent 500 error by @psyray in https://github.com/khulnasoft/reconpoint/pull/1124</li> <li>fix-1030: Add missing slug on target summary link by @psyray in https://github.com/khulnasoft/reconpoint/pull/1123</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#new-contributors_5","title":"New Contributors","text":"<ul> <li>@Deathpoolxrs made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1149</li> <li>@ErdemOzgen made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1126</li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.0.2...v2.0.3</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#202","title":"2.0.2","text":"<p>Release Date: December 8, 2023</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#whats-changed_7","title":"What's Changed","text":"<ul> <li>Added tooltip text to dashboard total vulnerabilities tooltip by @luizmlo in https://github.com/khulnasoft/reconpoint/pull/1029</li> <li>ops(<code>uninstall.sh</code>): add missing volumes and echo messages by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/977</li> <li>Fix no results in target subdomain list by @psyray in https://github.com/khulnasoft/reconpoint/pull/1036</li> <li>Fix Tool Settings Broken Link by @aqhmal in https://github.com/khulnasoft/reconpoint/pull/1021</li> <li>Fix subdomains list empty in Target by @psyray in https://github.com/khulnasoft/reconpoint/pull/1053</li> <li>Raise page limit to 500 for popup list by @psyray in https://github.com/khulnasoft/reconpoint/pull/1051</li> <li>Add directories count on Directories list by @psyray in https://github.com/khulnasoft/reconpoint/pull/1050</li> <li>ops(docker-compose): upgrade to 2.23.0 by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/1023</li> <li>Fix endpoints list and count by @psyray in https://github.com/khulnasoft/reconpoint/pull/1041</li> <li>Fix failing visualization when dorks are present by @psyray in https://github.com/khulnasoft/reconpoint/pull/1045</li> <li>Fix note not saving by @psyray in https://github.com/khulnasoft/reconpoint/pull/1047</li> <li>Count only not done todos in subdomains list by @psyray in https://github.com/khulnasoft/reconpoint/pull/1048</li> <li>Fix user agent definition keyword by @psyray in https://github.com/khulnasoft/reconpoint/pull/1054</li> <li>Upgrade project discovery tool at CT build by @psyray in https://github.com/khulnasoft/reconpoint/pull/1055</li> <li>Add a check to not load datatables twice by @psyray in https://github.com/khulnasoft/reconpoint/pull/1039</li> <li>Nmap port scan fails when Naabu return no port by @psyray in https://github.com/khulnasoft/reconpoint/pull/1067</li> <li>chore(issue-templates): incorrect label name by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/1066</li> <li>Endpoints list popup empty by @psyray in https://github.com/khulnasoft/reconpoint/pull/1070</li> <li>Add missing domain id value in subscan by @psyray in https://github.com/khulnasoft/reconpoint/pull/1069</li> <li>Fixes for #1033, #1026, #1027 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1071</li> <li>Temporary fix to prevent celery beat crash by @psyray in https://github.com/khulnasoft/reconpoint/pull/1072</li> <li>fix: ffuf ANSI code processing preventing task to finish by @ocervell in https://github.com/khulnasoft/reconpoint/pull/1058</li> <li>Update views.py by @Vijayragha1 in https://github.com/khulnasoft/reconpoint/pull/1074</li> <li>Fix crash on saving endpoint (FFUF related only) by @psyray in https://github.com/khulnasoft/reconpoint/pull/1063</li> <li>chore(issue-templates): fix incorrect description by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/1078</li> <li>IOError -&gt; OSError by @jxdv in https://github.com/khulnasoft/reconpoint/pull/1081</li> <li>Add directories count on Directories list by @psyray in https://github.com/khulnasoft/reconpoint/pull/1090</li> <li>chore(issue-template): don't allow blank issues by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/1089</li> <li>Fix bad nuclei config name by @psyray in https://github.com/khulnasoft/reconpoint/pull/1098</li> <li>disallow empty password by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1105</li> <li>fix attribute error on scan history #1103 by @khulnasoft in https://github.com/khulnasoft/reconpoint/pull/1104</li> <li>issue-633: added already-in-org filter to target dropdown in org form by @SeanOverton in https://github.com/khulnasoft/reconpoint/pull/1106</li> <li>Update Dockerfile to fix silicon incompatability by @SubGlitch1 in https://github.com/khulnasoft/reconpoint/pull/1107</li> <li>Add source for nmap scan by @psyray in https://github.com/khulnasoft/reconpoint/pull/1108</li> <li>Spelling mistake in hackerone.html by @Linuxinet in https://github.com/khulnasoft/reconpoint/pull/1112</li> <li>fix(version): incorrect number in art by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/1111</li> <li>Fix report generation when <code>Ignore Informational Vulnerabilities</code> checked by @psyray in https://github.com/khulnasoft/reconpoint/pull/1100</li> <li>fix(tool_arsenal): incorrect regex version numbers by @AnonymousWP in https://github.com/khulnasoft/reconpoint/pull/1086</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#new-contributors_6","title":"New Contributors","text":"<ul> <li>@luizmlo made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1029 </li> <li>@aqhmal made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1021 </li> <li>@C0wnuts made their first contribution in https://github.com/khulnasoft/reconpoint/pull/973 </li> <li>@ocervell made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1058 </li> <li>@Vijayragha1 made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1074 </li> <li>@jxdv made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1081 </li> <li>@SeanOverton made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1106 </li> <li>@SubGlitch1 made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1107 </li> <li>@Linuxinet made their first contribution in https://github.com/khulnasoft/reconpoint/pull/1112 </li> </ul> <p>Full Changelog: https://github.com/khulnasoft/reconpoint/compare/v2.0.1...v2.0.2</p> <p>Once again excellent work on reconPoint v2.0.2 by @AnonymousWP, @psyray, @ocervell and everybody else! </p>"},{"location":"Submodules/reconPoint/CHANGELOG/#201","title":"2.0.1","text":"<p>Release Date: October 24, 2023</p> <p>2.0.1 fixes a ton of issues in reconPoint 2.0.</p> <p>Fixes:</p> <ol> <li>Prevent duplicating Nuclei vulns for subdomain #1012 @psyray</li> <li>Fixes for empty subdomain returned during nuclei scan #1011 @psyray</li> <li>Add all the missing slug in scanEngine view &amp; other places #1005 @psyray</li> <li>Foxes for missing vulscan script #1004 @psyray</li> <li>Fixes for missing slug in report settings saving #1003</li> <li>Fixes for Nmap Parsing Error #1001 #1002 @psyray</li> <li>Fix nmap script ports iterable args #1000 @psyray</li> <li>Iterate over hostnames when multiple #1002 @psyray</li> <li>Gau install #998, change gauplus to gau @psyray</li> <li>Add missing slug parameter in schedule scan #996 @psyray</li> <li>Add missing slug parameter in schedule scan #996, fixes #940, #937, #897, #764 @psyray</li> <li>Add stack trace into make logs if DEBUG True #994 @psyray</li> <li>Fix dirfuzz base64 name display #993 #992 @psyray</li> <li>Fix target subdomains list not loading #991 @psyray</li> <li>Change WORDLIST constant value #987, fixes #986@psyray</li> <li>fix(notification_settings): submitting results in error 502 #981 fixes #970 @psyray</li> <li>Fixes with documentation and installation/update/uninstall scripts @anonymousWP</li> <li>Fix file directory popup not showing in detailed scan #912 @psyray</li> </ol> <p>@AnonymousWP and @psyray have been phenomenal in fixing these bugs. Thanks to both of you!  </p>"},{"location":"Submodules/reconPoint/CHANGELOG/#200","title":"2.0.0","text":"<p>Release Date: October 7, 2023</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#added","title":"Added","text":"<ul> <li>Projects: Projects allow you to efficiently organize their web application reconnaissance efforts. With this feature, you can create distinct project spaces, each tailored to a specific purpose, such as personal bug bounty hunting, client engagements, or any other specialized recon task.</li> <li>Roles and Permissions: assign distinct roles to your team members: Sys Admin, Penetration Tester, and Auditor\u2014each with precisely defined permissions to tailor their access and actions within the reconPoint ecosystem.</li> <li>GPT-powered Report Generation: With the power of OpenAI's GPT, reconPoint now provides you with detailed vulnerability descriptions, remediation strategies, and impact assessments.</li> <li>API Vault: This feature allows you to organize your API keys such as OpenAI or Netlas API keys.</li> <li>GPT-powered Attack Surface Generation</li> <li>URL gathering now is much more efficient, removing duplicate endpoints based on similar HTTP Responses, having the same content_lenth, or page_title. Custom duplicate fields can also be set from the scan engine configuration.</li> <li>URL Path filtering while initiating scan: For instance, if we want to scan only endpoints starting with https://example.com/start/, we can pass the /start as a path filter while starting the scan. @ocervell</li> <li>Expanding Target Concept: reconPoint 2.0 now accepts IPs, URLS, etc as targets. (#678, #658) Excellent work by @ocervell</li> <li>A ton of refactoring on reconPoint's core to improve scan efficiency. Massive kudos to @ocervell</li> <li>Created a custom celery workflow to be able to run several tasks in parallel that are not dependent on each other, such OSINT task and subdomain discovery will run in parallel, and directory and file fuzzing, vulnerability scan, screenshot gathering etc. will run in parallel after port scan or url fetching is completed. This will increase the efficiency of scans and instead of having one long flow of tasks, they can run independently on their own. @ocervell</li> <li>Refactored all tasks to run asynchronously @ocervell</li> <li>Added a stream_command that allows to read the output of a command live: this means the UI is updated with results while the command runs and does not have to wait until the task completes. Excellent work by @ocervell</li> <li>Pwndb is now replaced by h8mail. @ocervell</li> <li>Group Scan Results: reconPoint 2.0 allows to group of subdomains based on similar page titles and HTTP status, and also vulnerability grouping based on the same vulnerability title and severity.</li> <li>Added Support for Nmap: reconPoint 2.0 allows to run Nmap scripts and vuln scans on ports found by Naabu. @ocervell</li> <li>Added support for Shared Scan Variables in Scan Engine Configuration:<ul> <li><code>enable_http_crawl</code>: (true/false) You can disable it to be more stealthy or focus on something different than HTTP</li> <li><code>timeout</code>: set timeout for all tasks</li> <li><code>rate_limit</code>: set rate limit for all tasks</li> <li><code>retries</code>: set retries for all tasks</li> <li><code>custom_header</code>: set the custom header for all tasks</li> </ul> </li> <li>Added Dalfox for XSS Vulnerability Scan</li> <li>Added CRLFuzz for CRLF Vulnerability Scan</li> <li>Added S3Scanner for scanning misconfigured S3 buckets</li> <li>Improve OSINT Dork results, now detects admin panels, login pages and dashboards</li> <li>Added Custom Dorks</li> <li>Improved UI for vulnerability results, clicking on each vulnerability will open up a sidebar with vulnerability details.</li> <li>Added HTTP Request and Response in vulnerability Results</li> <li>Under Admin Settings, added an option to allow add/remove/deactivate additional users</li> <li>Added Option to Preview Scan Report instead of forcing to download</li> <li>Added Katana for crawling and spidering URLs</li> <li>Added Netlas for Whois and subdomain gathering</li> <li>Added TLSX for subdomain gathering</li> <li>Added CTFR for subdomain gathering</li> <li>Added historical IP in whois section</li> <li>Added Pagination on Large datatables such as subdomains, endpoints, vulnerabilities etc #949 @psyray</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#fixes","title":"Fixes","text":"<ul> <li>GF patterns do not run on 404 endpoints (#574 closed)</li> <li>Fixes for retrieving whois data (#693 closed)</li> <li>Related/Associated Domains in Whois section is now fixed</li> <li>Fixed missing lightbox css &amp; js on target screenshot page #947 #948 @psyray</li> <li>Issue in Port-scan: int object is not subscriptable Fixed #939, #938 @AnonymousWP</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#removed","title":"Removed","text":"<ul> <li>Removed pwndb and tor related to it.</li> <li>Removed tor for pwndb</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#136","title":"1.3.6","text":"<p>Release Date: March 2, 2023</p> <ul> <li>Fixed installation errors. Fixed #824, #823, #816, #809, #803, #801, #798, #797, #794, #791 .</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#135","title":"1.3.5","text":"<p>Release Date: December 29, 2022</p> <ul> <li>Fixed #769, #768, #766, #761, Thanks to, @bin-maker, @carsonchan12345, @paweloque, @opabravo</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#134","title":"1.3.4","text":"<p>Release Date: November 16, 2022</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#fixes_1","title":"Fixes","text":"<ul> <li>Fixed #748 , #743 , #738, #739</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#133","title":"1.3.3","text":"<p>Release Date: October 9, 2022</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#fixes_2","title":"Fixes","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#723-upgraded-go-to-1182","title":"723, Upgraded Go to 1.18.2","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#132","title":"1.3.2","text":"<p>Release Date: August 20, 2022</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#fixes_3","title":"Fixes","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#683-for-filtering-gf-tags","title":"683 For Filtering GF tags","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#669-where-directory-ui-had-to-be-collapsed","title":"669 Where Directory UI had to be collapsed","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#131","title":"1.3.1","text":"<p>Release Date: August 12, 2022</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#fixes_4","title":"Fixes","text":"<ul> <li>Fix for #643 Downloading issue for Subdomain and Endpoints</li> <li>Fix for #627 Too many Targets causes issues while loading datatable</li> <li>Fix version Numbering issue</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#130","title":"1.3.0","text":"<p>Release Date: July 11, 2022</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Geographic Distribution of Assets Map</li> <li>Added WAF Detector as an optional tool in Scan Engine</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#fixes_5","title":"Fixes","text":"<ul> <li>WHOIS Provider Changed</li> <li>Fixed Dark UI Issues</li> <li>Fix HTTPX Issue with custom Header</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#120","title":"1.2.0","text":"<p>Release Date: May 27, 2022</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#added_2","title":"Added","text":"<ul> <li>Naabu Exclude CDN Port Scanning</li> <li>Added WAF Detection</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#fixes_6","title":"Fixes","text":"<ul> <li>Fix #630 Character Name too Long Issue</li> <li>[Security] Fixed several instances of Command Injections, CVE-2022-28995, CVE-2022-1813</li> <li>Hakrawler Fixed - #623</li> <li>Fixed XSS on Hackerone report via Markdown</li> <li>Fixed XSS on Import Target using malicious filename</li> <li>Stop Scan Fixed #561</li> <li>Fix installation issue due to missing curl</li> <li>Updated docker-compose version</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#110","title":"\ud83c\udff7\ufe0f 1.1.0","text":"<p>Release Date: Apr 24, 2022</p> <ul> <li>Redeigned UI</li> <li> <p>Added Subscan Feature</p> <p>Subscan allows further scanning any subdomains. Assume from a normal recon process you identified a subdomain that you wish to do port scan. Earlier, you had to add that subdomain as a target. Now you can just select the subdomain and initiate subscan.</p> </li> </ul> <ul> <li>Ability to Download reconnaissance or vulnerability report</li> <li>Added option to customize report, customization includes the look and feel of report, executive summary etc.</li> </ul> <ul> <li>Add IP Address from IP</li> <li>WHOIS Addition on Detail Scan and fetch whois automatically on Adding Single Targets</li> <li>Universal Search Box</li> <li>Addition of Quick Add menus</li> <li> <p>Added ToolBox Feature</p> <p>ToolBox will feature most commonly used recon tools. One can use these tools to identify whois, CMSDetection etc without adding targets. Currently, Whois, CMSDetector and CVE ID lookup is supported. More tools to follow up.</p> </li> </ul> <ul> <li>Notify New Releases on reconPoint if available</li> <li>Tools Arsenal Section to feature preinstalled and custom tools</li> <li>Ability to Update preinstalled tools from Tools Arsenal Section</li> <li>Ability to download/add custom tools</li> <li>Added option for Custom Header on Scan Engine</li> <li>Added CVE_ID, CWE_ID, CVSS Score, CVSS Metrics on Vulnerability Section, this also includes lookup using cve_id, cwe_id, cvss_score etc</li> <li>Added curl command and references on Vulnerability Section</li> <li>Added Columns Filtering Option on Subdomain, Vulnerability and Endpoints Tables</li> <li>Added Error Handling for Failed Scans, reason for failure scan will be displayed</li> <li>Added Related Domains using WHOIS</li> <li>Added Related TLDs</li> <li>Added HTTP Status Breakdown Widget</li> <li>Added CMS Detector</li> <li>Updated Visualization</li> <li>Option to Download Selected Subdomains</li> <li>Added additional Nuclei Templates from https://github.com/geeknik/the-nuclei-templates</li> <li>Added SSRF check from Nagli Nuclei Template</li> <li>Added option to fetch CVE_ID details</li> <li>Added option to Delete Multiple Scans</li> <li>Added ffuf as Directory and Files fuzzer</li> <li>Added widgets such as Most vulnerable Targets, Most Common Vulnerabilities, Most Common CVE IDs, Most Common CWE IDs, Most Common Vulnerability Tags</li> </ul> <p>And more...</p>"},{"location":"Submodules/reconPoint/CHANGELOG/#101","title":"\ud83c\udff7\ufe0f 1.0.1","text":"<p>Release Date: Aug 29, 2021</p> <p>Changelog</p> <ul> <li>Fixed #482 Endpoints and Vulnerability Datatable were showing results of other targets due to the scan_id parameter</li> <li>Fixed #479 where the scan was failing due to recent httpx release, change was in the JSON output</li> <li>Fixed #476 where users were unable to click on Clocked Scan (Reported only on Firefox)</li> <li>Fixed #442 where an extra slash was added in Directory URLs</li> <li>Fixed #337 where users were unable to link custom wordlist</li> <li>Fixed #436 Checkbox in Notification Settings were not working due to same name attribute, now fixed</li> <li>Fixed #439 Hakrawler crashed if the deep mode was activated due to -plain flag</li> <li>Fixed #437 If Out of Scope subdomains were supplied, the scan was failing due to None value</li> <li>Fixed #424 Multiple Targets couldn't be scanned</li> </ul> <p>Improvements</p> <ul> <li>Enhanced install script, check for if docker is running service or not #468 Security</li> </ul> <ul> <li>Fixed Cross Site Scripting<ul> <li>#460</li> <li>#457</li> <li>#454</li> <li>#453</li> <li>#459</li> <li>#460</li> </ul> </li> <li>Fixed Cross Site Scripting reported on Huntr #478 https://www.huntr.dev/bounties/ac07ae2a-1335-4dca-8d55-64adf720bafb/</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#verion-10-major-release","title":"Verion 1.0 Major release","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#additions","title":"Additions","text":"<ul> <li>Dark Mode</li> <li>Recon Data visualization</li> <li>Improved correlation among recon data</li> <li>Ability to identify Interesting Subdomains</li> <li>Ability to Automatically report Vulnerabilities to Hackerone with customizable vulnerability report</li> <li>Added option to download URLs and Endpoints along with matched GF patterns</li> <li>Dorking support for stackoverflow, 3rdparty, social_media, project_management, code_sharing, config_files, jenkins, wordpress_files, cloud_buckets, php_error, exposed_documents, struts_rce, db_files, traefik, git_exposed</li> <li>Emails, metainfo, employees, leaked password discovery</li> <li>Optin to Add bulk targets</li> <li>Proxy Support</li> <li>Target Summary</li> <li>Recon Todo</li> <li>Unusual Port Identification</li> <li>GF patterns support #110, #88</li> <li>Screenshot Gallery with Filters</li> <li>Powerful recon data filtering with auto suggestions</li> <li>Added whatportis, this allows ports to be displayed as Service Name and Description</li> <li>Recon Data changes, finds new/removed subdomains/endpoints</li> <li>Tagging of targets into Organization</li> <li>Added option to delete all scan results or delete all screenshots inside Settings and reconPoint settings</li> <li>Support for custom GF patterns and Nuclei Templates</li> <li>Support for editing tool related configuration files (Nuclei, Subfinder, Naabu, amass)</li> <li>Option to Mark Subdomains as important</li> <li>Separate tab for Directory scan results</li> <li>Option to Import Subdomains</li> <li>Clean your scan results and screenshots</li> <li>Enhanced and Customizable Scan alert with support for sending recon data directly to Discord</li> <li>Improvement in Vulnerability Scanning, If endpoint scan is performed, those endpoints will be an input to Nuclei.</li> <li>Ignore file extensions in URLs</li> <li>Added response time in endpoints and subdomains</li> <li>Added badge to identify CDN and non CDN IPs</li> <li>Added gospider, gauplus and waybackurls for url discovery</li> <li>Added activity log in Scan activity</li> <li>For better UX shifted nav bar from vertical position to horizontal position on top. This allows better navigation on recon data.</li> <li>Separate table for Directory scan results #244</li> <li>Scan results UI now in tabs</li> <li>Added badge on Subdomain Result table to directly query Vulnerability and Endpoints</li> <li>Webserver and content_type badge has been addeed in Subdomain Result table</li> <li>Inside Targets list, Recent Scan button has been added to quickly go to the last scan results</li> <li>In target summary, timelin of scan has been added</li> <li>Randomized user agent in HTTPX</li> <li>reconPoint will no longer store any recon data apart from that in Database, this includes sorted_subdomains list.txt or any json file</li> <li>aquatone has been replaced with Eyewitness</li> <li>Out of Scope subdomains are no longer part of scan engine, they can be imported before initiating the scan</li> <li>Added script to uninstall reconPoint</li> <li>Added option to filter targets and scans using organization, scan status, etc</li> <li>Added random user agent in directory scan</li> <li>Added concurrency, rate limit, timeout, retries in Scan Engine YAML</li> <li>Added Rescan option</li> <li>Other tiny fixes.....</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#v053-feb25-2021","title":"V0.5.3 Feb25 2021","text":"<ul> <li>Build error for Naabu v2 Fixed</li> <li>Added rate support for Naabu</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#v052-feb-23-2021","title":"V0.5.2 Feb 23 2021","text":"<ul> <li>Fixed XSS https://github.com/khulnasoft/reconpoint/issues/347</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#v051-feb-19-2021","title":"V0.5.1 Feb 19 2021","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#features","title":"Features","text":"<ul> <li>Added Discord Support for Notification Web hooks</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#v05-29-nov-2020","title":"V0.5 29 Nov 2020","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#features_1","title":"Features","text":"<ul> <li>Nuclei Integration: v0.5 is primarily focused on vulnerability scanner using Nuclei. This was a long pending due and we've finally integrated it.</li> </ul> <ul> <li>Powerful search queries across endpoints, subdomains and vulnerability scan results: reconPoint reconnaissance data can now be queried using operators like &lt;,&gt;,&amp;,| and !, namely greater than, less than, and, or, and not. This is extremely useful in querying the recon data. More details can be found at Instructions to perform Queries on Recon data</li> </ul> <ul> <li>Out of scope options: Many of you have been asking for out of scope option. Thanks to Valerio Brussani for his pull request which made it possible for out of scope options. Please check the documentation on how to define out of scope options.</li> </ul> <ul> <li>Official Documentation(WIP): We often get asked on how to use reconPoint. For long, we had no official documentation. Finally, I've worked on it and we have the official documentation at recon.khulnasoft.com</li> </ul> <ul> <li>The documentation is divided into two parts, for Developers and for Penetration Testers. For developers, it's a work in progress. I will keep you all updated throughout the process.</li> </ul> <ul> <li>Redefined Dashboard: We've also made some changes in the Dashboard. The additions include vulnerability scan results, most vulnerable targets, most common vulnerabilities.</li> </ul> <ul> <li>Global Search: This feature has been one of the most requested features for reconPoint. Now you can search all the subdomains, endpoints, and vulnerabilities.</li> </ul> <ul> <li>OneForAll Support: reconPoint now supports OneForAll for subdomain discovery, it is currently in beta. I am working on how to integrate OneForAll APIKeys and Configuration files.</li> </ul> <ul> <li>Configuration Support for subfinder: You will now have ability to add configurations for subfinder as well.</li> </ul> <ul> <li>Timeout option for aquatone: We added timeout options in yaml configuration as a lot of screenshots were missing. You can now define timeout for http, scan and screenshots for timeout in milliseconds.</li> </ul> <ul> <li>Design Changes A lot of design changes has happened in reconPoint. Some of which are:</li> </ul> <ul> <li>Endpoints Results and Vulnerability Scan Results are now displayed as a separate page, this is to separate the results and decrease the page load time.   Checkbox next to Subdomains and Vulnerability report list to change the status, this allows you to mark all subdomains and vulnerabilities that you've already completed working on.</li> <li>Sometimes due to timeout, aquatone was skipping the screenshots and due to that, navigations between screenshots was little annoying. We have fixed it as well.   Ability to delete multiple targets and initiate multiple scans.</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#abandoned","title":"Abandoned","text":"<ul> <li>Subdomain Takeover: As we decided to use Nuclei for Vulnerability Scanner, and also, since Subjack wasn't giving enough results, I decided to remove Subjack. The subdomain Takeover will now be part of Nuclei Vulnerability Scanner.</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#v04-release-2020-10-08","title":"V0.4 Release 2020-10-08","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#features_2","title":"Features","text":"<ul> <li>Background tasks migrated to Celery and redis</li> <li>Periodic and clocked scan added</li> <li>Ability to Stop and delete the scan</li> <li>CNAME and IP address added on detail scan</li> <li>Content type added on Endpoints section</li> <li>Ability to initiate multiple scans at a time</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#v03-release-2020-07-21","title":"V0.3 Release 2020-07-21","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#features_3","title":"Features","text":"<ul> <li>YAML based Customization Engine</li> <li>Ability to add wordlists</li> <li>Login Feature</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#v02-release-2020-07-11","title":"V0.2 Release 2020-07-11","text":""},{"location":"Submodules/reconPoint/CHANGELOG/#features_4","title":"Features","text":"<ul> <li>Directory Search Enabled</li> <li>Fetch URLS using hakrawler</li> <li>Subdomain takeover using Subjack</li> <li>Add Bulk urls</li> <li>Delete Scan functionality</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#fix","title":"Fix","text":"<ul> <li>Windows Installation issue fixed</li> <li>Scrollbar Issue on small screens fixed</li> </ul>"},{"location":"Submodules/reconPoint/CHANGELOG/#v01-release-2020-07-08","title":"V0.1 Release 2020-07-08","text":"<ul> <li>reconPoint is released</li> </ul>"},{"location":"Submodules/reconPoint/CONTRIBUTORS/","title":"CONTRIBUTORS","text":""},{"location":"Submodules/reconPoint/CONTRIBUTORS/#contributors","title":"Contributors","text":"<p>Thanks to these individuals for making reconPoint awesome by fixing bugs, resolving issues and by creating PR!</p> <ul> <li>Aju100</li> <li>d1pakda5</li> <li>subha7595</li> <li>Suprita-25</li> <li>TheBinitGhimire</li> <li>Vinay Leo</li> <li>Erdem Ozgen</li> </ul> <p>If you have created a Pull request, feel free to add your name here, because we know you are awesome and deserve thanks from the community!</p> <p>Also, Thanks to these individuals for making reconPoint awesome by providing continuous suggestions and support!</p> <ul> <li>bhavsec</li> <li>Cor3min3r</li> <li>Ganesh Pandey</li> <li>Prial Islam</li> <li>SecArmy</li> <li>pulsar: FooBallZ</li> </ul>"},{"location":"ThreatMatrix/","title":"Index","text":"<p>This is a Documentation for ThreatMatrix.</p>"},{"location":"ThreatMatrix/advanced_configuration/","title":"Advanced Configuration","text":"<p>This page includes details about some advanced features that Intel Owl provides which can be optionally configured by the administrator.</p>"},{"location":"ThreatMatrix/advanced_configuration/#elasticsearch","title":"ElasticSearch","text":"<p>Right now only ElasticSearch v8 is supported.</p>"},{"location":"ThreatMatrix/advanced_configuration/#dsl","title":"DSL","text":"<p>ThreatMatrix makes use of django-elasticsearch-dsl to index Job results into elasticsearch. The <code>save</code> and <code>delete</code> operations are auto-synced so you always have the latest data in ES.</p> <p>In the <code>env_file_app_template</code>, you'd see various elasticsearch related environment variables. The user should spin their own Elastic Search instance and configure these variables.</p>"},{"location":"ThreatMatrix/advanced_configuration/#kibana","title":"Kibana","text":"<p>Intel Owl provides a Kibana's \"Saved Object\" configuration (with example dashboard and visualizations). It can be downloaded from here and can be imported into Kibana by going to the \"Saved Objects\" panel (http://localhost:5601/app/management/kibana/objects).</p>"},{"location":"ThreatMatrix/advanced_configuration/#example-configuration","title":"Example Configuration","text":"<ol> <li>Setup Elastic Search and Kibana and say it is running in a docker service with name <code>elasticsearch</code> on port <code>9200</code> which is exposed to the shared docker network.    (Alternatively, you can spin up a local Elastic Search instance, by appending <code>--elastic</code> to the <code>./start</code> command. Note that the local Elastic Search instance consumes large amount of memory, and hence having &gt;=16GB is recommended.))</li> <li>In the <code>env_file_app</code>, we set <code>ELASTICSEARCH_DSL_ENABLED</code> to <code>True</code> and <code>ELASTICSEARCH_DSL_HOST</code> to <code>elasticsearch:9200</code>.</li> <li>Now start the docker containers and execute</li> </ol> <pre><code>docker exec -ti threatmatrix_uwsgi python manage.py search_index --rebuild\n</code></pre> <p>This will build and populate all existing job objects into the <code>jobs</code> index.</p>"},{"location":"ThreatMatrix/advanced_configuration/#business-intelligence","title":"Business Intelligence","text":"<p>ThreatMatrix makes use of elasticsearch-py to store data that can be used for Business Intelligence purpose. Since plugin reports are deleted periodically, this feature allows to save indefinitely small amount of data to keep track of how analyzers perform and user usage. At the moment, the following information are sent to elastic:</p> <ul> <li>application name</li> <li>timestamp</li> <li>username</li> <li>configuration used</li> <li>process_time</li> <li>status</li> <li>end_time</li> <li>parameters</li> </ul> <p>Documents are saved in the <code>ELEASTICSEARCH_BI_INDEX-%YEAR-%MONTH</code>, allowing to manage the retention accordingly. To activate this feature, it is necessary to set <code>ELASTICSEARCH_BI_ENABLED</code> to <code>True</code> in the <code>env_file_app</code> and <code>ELASTICSEARCH_BI_HOST</code> to <code>elasticsearch:9200</code> or your elasticsearch server.</p> <p>An index template is created after the first bulk submission of reports. If you want to use kibana to visualize your data/make dashboard, you must create an index pattern: Go to Kibana -&gt; Discover -&gt; Stack Management -&gt; Index Patterns -&gt; search for your index and use as time field <code>timestamp</code></p>"},{"location":"ThreatMatrix/advanced_configuration/#authentication-options","title":"Authentication options","text":"<p>ThreatMatrix provides support for some of the most common authentication methods:</p> <ul> <li>Google Oauth2</li> <li>LDAP</li> <li>RADIUS</li> </ul>"},{"location":"ThreatMatrix/advanced_configuration/#google-oauth2","title":"Google OAuth2","text":"<p>The first step is to create a Google Cloud Platform project, and then create OAuth credentials for it.</p> <p>It is important to add the correct callback in the \"Authorized redirect URIs\" section to allow the application to redirect properly after the successful login. Add this:</p> <pre><code>http://&lt;localhost|yourowndomain&gt;/api/auth/google-callback\n</code></pre> <p>After that, specify the client ID and secret as <code>GOOGLE_CLIENT_ID</code> and <code>GOOGLE_CLIENT_SECRET</code> environment variables and restart ThreatMatrix to see the applied changes.</p> <p>Note</p> While configuring Google Auth2 you can choose either to enable access to the all users with a Google Account (\"External\" mode) or to enable access to only the users of your organization (\"Internal\" mode). Reference"},{"location":"ThreatMatrix/advanced_configuration/#ldap","title":"LDAP","text":"<p>ThreatMatrix leverages Django-auth-ldap to perform authentication via LDAP.</p> <p>How to configure and enable LDAP on Intel Owl?</p> <ol> <li>Change the values with your LDAP configuration inside <code>configuration/ldap_config.py</code>. This file is mounted as a docker volume, so you won't need to rebuild the image.</li> </ol> <p>Note</p> For more details on how to configure this file, check the official documentation of the django-auth-ldap library.  <ol> <li>Once you have done that, you have to set the environment variable <code>LDAP_ENABLED</code> as <code>True</code> in the environment configuration file <code>env_file_app</code>.    Finally, you can restart the application with <code>docker-compose up</code></li> </ol>"},{"location":"ThreatMatrix/advanced_configuration/#radius-authentication","title":"RADIUS Authentication","text":"<p>ThreatMatrix leverages Django-radius to perform authentication via RADIUS server.</p> <p>How to configure and enable RADIUS authentication on Intel Owl?</p> <ol> <li>Change the values with your RADIUS auth configuration inside <code>configuration/radius_config.py</code>. This file is mounted as a    docker volume, so you won't need to rebuild the image.</li> </ol> <p>Note</p> For more details on how to configure this file, check the official documentation of the django-radius library.  <ol> <li>Once you have done that, you have to set the environment variable <code>RADIUS_AUTH_ENABLED</code> as <code>True</code> in the environment    configuration file <code>env_file_app</code>. Finally, you can restart the application with <code>docker-compose up</code></li> </ol>"},{"location":"ThreatMatrix/advanced_configuration/#opencti","title":"OpenCTI","text":"<p>Like many other integrations that we have, we have an Analyzer and a Connector for the OpenCTI platform.</p> <p>This allows the users to leverage these 2 popular open source projects and frameworks together.</p> <p>So why we have a section here? This is because there are various compatibility problems with the official PyCTI library.</p> <p>We found out (see issues in ThreatMatrix and PyCTI) that, most of the times, it is required that the OpenCTI version of the server you are using and the pycti version installed in ThreatMatrix must match perfectly.</p> <p>Because of that, we decided to provide to the users the chance to customize the version of PyCTI installed in ThreatMatrix based on the OpenCTI version that they are using.</p> <p>To do that, you would need to leverage the option <code>--pycti-version</code> provided by the <code>./start</code> helper:</p> <ul> <li>check the default version that would be installed by checking the description of the option <code>--pycti-version</code> with <code>./start -h</code></li> <li>if the default version is different from your OpenCTI server version, you need to rebuild the ThreatMatrix Image with <code>./start test build --pycti-version &lt;your_version&gt;</code></li> <li>then restart the project <code>./start test up -- --build</code></li> <li>enjoy</li> </ul>"},{"location":"ThreatMatrix/advanced_configuration/#cloud-support","title":"Cloud Support","text":""},{"location":"ThreatMatrix/advanced_configuration/#aws-support","title":"AWS support","text":"<p>We have support for several AWS services.</p> <p>You can customize the AWS Region location of you services by changing the environment variable <code>AWS_REGION</code>. Default is <code>eu-central-1</code></p> <p>You have to add some credentials for AWS: if you have ThreatMatrix deployed on the AWS infrastructure, you can use IAM credentials: to allow that just set <code>AWS_IAM_ACCESS</code> to <code>True</code>. If that is not the case, you have to set both <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code></p>"},{"location":"ThreatMatrix/advanced_configuration/#s3","title":"S3","text":"<p>If you prefer to use S3 to store the analyzed samples, instead of the local storage, you can do it.</p> <p>First, you need to configure the environment variable <code>LOCAL_STORAGE</code> to <code>False</code> to enable it and set <code>AWS_STORAGE_BUCKET_NAME</code> to the AWS bucket you want to use.</p> <p>Then you need to configure permission access to the chosen S3 bucket.</p>"},{"location":"ThreatMatrix/advanced_configuration/#message-broker","title":"Message Broker","text":"<p>ThreatMatrix at the moment supports 3 different message brokers:</p> <ul> <li>Redis (default)</li> <li>RabbitMQ</li> <li>Aws SQS</li> </ul> <p>The default broker, if nothing is specified, is <code>Redis</code>.</p> <p>To use <code>RabbitMQ</code>, you must use the option <code>--rabbitmq</code> when launching ThreatMatrix with the <code>./start</code> script.</p> <p>To use <code>Aws SQS</code>, you must use the option <code>--sqs</code> when launching ThreatMatrix with the <code>.start</code> script. In that case, you should create new SQS queues in AWS called <code>threatmatrix-&lt;environment&gt;-&lt;queue_name&gt;</code> and give your instances on AWS the proper permissions to access it. Moreover, you must populate the <code>AWS_USER_NUMBER</code>. This is required to connect in the right way to the selected SQS queues. Only FIFO queues are supported.</p> <p>If you want to use a remote message broker (like an <code>ElasticCache</code> or <code>AmazonMQ</code> instance), you must populate the <code>BROKER_URL</code> environment variable.</p> <p>It is possible to use task priority inside ThreatMatrix: each User has default priority of 10, and robots users (like the Ingestors) have a priority of 7. You can customize these priorities inside Django Admin, in the <code>Authentication.User Profiles</code> section.</p>"},{"location":"ThreatMatrix/advanced_configuration/#websockets","title":"Websockets","text":"<p><code>Redis</code> is used for two different functions:</p> <ul> <li>message broker</li> <li>websockets</li> </ul> <p>For this reason, a <code>Redis</code> instance is mandatory. You can personalize ThreatMatrix in two different way:</p> <ul> <li>with a local <code>Redis</code> instance.</li> </ul> <p>This is the default behaviour.</p> <ul> <li>With a remote <code>Redis</code> instance.</li> </ul> <p>You must use the option <code>--use-external-redis</code> when launching ThreatMatrix with the <code>.start</code> script. Moreover, you need to populate the <code>WEBSOCKETS_URL</code> environment variable. If you are using <code>Redis</code> as a message broker too, remember to populate the <code>BROKER_URL</code> environment variable</p>"},{"location":"ThreatMatrix/advanced_configuration/#rds","title":"RDS","text":"<p>If you like, you could use AWS RDS instead of PostgreSQL for your database. In that case, you should change the database required options accordingly: <code>DB_HOST</code>, <code>DB_PORT</code>, <code>DB_USER</code>, <code>DB_PASSWORD</code> and setup your machine to access the service.</p> <p>If you have ThreatMatrix deployed on the AWS infrastructure, you can use IAM credentials to access the Postgres DB. To allow that just set <code>AWS_RDS_IAM_ROLE</code> to <code>True</code>. In this case <code>DB_PASSWORD</code> is not required anymore.</p> <p>Moreover, to avoid to run PostgreSQL locally, you would need to use the option <code>--use-external-database</code> when launching ThreatMatrix with the <code>./start</code> script.</p>"},{"location":"ThreatMatrix/advanced_configuration/#ses","title":"SES","text":"<p>If you like, you could use Amazon SES for sending automated emails (password resets / registration requests, etc).</p> <p>You need to configure the environment variable <code>AWS_SES</code> to <code>True</code> to enable it.</p>"},{"location":"ThreatMatrix/advanced_configuration/#secrets","title":"Secrets","text":"<p>You can use the \"Secrets Manager\" to store your credentials. In this way your secrets would be better protected.</p> <p>Instead of adding the variables to the environment file, you should just add them with the same name on the AWS Secrets Manager and Intel Owl will fetch them transparently.</p> <p>Obviously, you should have created and managed the permissions in AWS in advance and accordingly to your infrastructure requirements.</p> <p>Also, you need to set the environment variable <code>AWS_SECRETS</code> to <code>True</code> to enable this mode.</p>"},{"location":"ThreatMatrix/advanced_configuration/#nfs","title":"NFS","text":"<p>You can use a <code>Network File System</code> for the shared_files that are downloaded runtime by ThreatMatrix (for example Yara rules).</p> <p>To use this feature, you would need to add the address of the remote file system inside the <code>.env</code> file, and you would need to use the option <code>--nfs</code> when launching ThreatMatrix with the <code>./start</code> script.</p>"},{"location":"ThreatMatrix/advanced_configuration/#google-kubernetes-engine","title":"Google Kubernetes Engine","text":"<p>Right now there is no official support for Kubernetes deployments.</p> <p>But we have an active community. Please refer to the following blog post for an example on how to deploy ThreatMatrix on Google Kubernetes Engine:</p> <p>Deploying Intel-Owl on GKE by Mayank Malik.</p>"},{"location":"ThreatMatrix/advanced_configuration/#queues","title":"Queues","text":""},{"location":"ThreatMatrix/advanced_configuration/#multi-queue","title":"Multi Queue","text":"<p>ThreatMatrix provides an additional multi-queue.override.yml compose file allowing ThreatMatrix users to better scale with the performance of their own architecture.</p> <p>If you want to leverage it, you should add the option <code>--multi-queue</code> when starting the project. Example:</p> <pre><code>./start prod up --multi-queue\n</code></pre> <p>This functionality is not enabled by default because this deployment would start 2 more containers so the resource consumption is higher. We suggest to use this option only when leveraging ThreatMatrix massively.</p>"},{"location":"ThreatMatrix/advanced_configuration/#queue-customization","title":"Queue Customization","text":"<p>It is possible to define new celery workers: each requires the addition of a new container in the docker-compose file, as shown in the <code>multi-queue.override.yml</code>.</p> <p>Moreover ThreatMatrix requires that the name of the workers are provided in the <code>docker-compose</code> file. This is done through the environment variable <code>CELERY_QUEUES</code> inside the <code>uwsgi</code> container. Each queue must be separated using the character <code>,</code>, as shown in the example.</p> <p>One can customize what analyzer should use what queue by specifying so in the analyzer entry in the analyzer_config.json configuration file. If no queue(s) are provided, the <code>default</code> queue will be selected.</p>"},{"location":"ThreatMatrix/advanced_configuration/#queue-monitoring","title":"Queue monitoring","text":"<p>ThreatMatrix provides an additional flower.override.yml compose file allowing ThreatMatrix users to use Flower features to monitor and manage queues and tasks</p> <p>If you want to leverage it, you should add the option <code>--flower</code> when starting the project. Example:</p> <pre><code>./start prod up --flower\n</code></pre> <p>The flower interface is available at port 5555: to set the credentials for its access, update the environment variables</p> <pre><code>FLOWER_USER\nFLOWER_PWD\n</code></pre> <p>or change the <code>.htpasswd</code> file that is created in the <code>docker</code> directory in the <code>threatmatrix_flower</code> container.</p>"},{"location":"ThreatMatrix/advanced_configuration/#manual-usage","title":"Manual Usage","text":"<p>The <code>./start</code> script essentially acts as a wrapper over Docker Compose, performing additional checks. ThreatMatrix can still be started by using the standard <code>docker compose</code> command, but all the dependencies have to be manually installed by the user.</p>"},{"location":"ThreatMatrix/advanced_configuration/#options","title":"Options","text":"<p>The <code>--project-directory</code> and <code>-p</code> options are required to run the project. Default values set by <code>./start</code> script are \"docker\" and \"threat_matrix\", respectively.</p> <p>The startup is based on chaining various Docker Compose YAML files using <code>-f</code> option. All Docker Compose files are stored in <code>docker/</code> directory of the project. The default compose file, named <code>default.yml</code>, requires configuration for an external database and message broker. In their absence, the <code>postgres.override.yml</code> and <code>rabbitmq.override.yml</code> files should be chained to the default one.</p> <p>The command composed, considering what is said above (using <code>sudo</code>), is</p> <pre><code>sudo docker compose --project-directory docker -f docker/default.yml -f docker/postgres.override.yml -f docker/rabbitmq.override.yml -p threat_matrix up\n</code></pre> <p>The other most common compose file that can be used is for the testing environment. The equivalent of running <code>./start test up</code> is adding the <code>test.override.yml</code> file, resulting in:</p> <pre><code>sudo docker compose --project-directory docker -f docker/default.yml -f docker/postgres.override.yml -f docker/rabbitmq.override.yml -f docker/test.override.yml -p threat_matrix up\n</code></pre> <p>All other options available in the <code>./start</code> script (<code>./start -h</code> to view them) essentially chain other compose file to <code>docker compose</code> command with corresponding filenames.</p>"},{"location":"ThreatMatrix/advanced_configuration/#optional-analyzer","title":"Optional Analyzer","text":"<p>ThreatMatrix includes integrations with some analyzer that are not enabled by default. These analyzers, stored under the <code>integrations/</code> directory, are packed within Docker Compose files. The <code>compose.yml</code> file has to be chained to include the analyzer. The additional <code>compose-test.yml</code> file has to be chained for testing environment.</p>"},{"location":"ThreatMatrix/advanced_usage/","title":"Advanced Usage","text":"<p>This page includes details about some advanced features that Intel Owl provides which can be optionally enabled. Namely,</p>"},{"location":"ThreatMatrix/advanced_usage/#organizations-and-user-management","title":"Organizations and User management","text":"<p>Starting from ThreatMatrix v4, a new \"Organization\" section is available on the GUI. This section substitute the previous permission management via Django Admin and aims to provide an easier way to manage users and visibility.</p>"},{"location":"ThreatMatrix/advanced_usage/#multi-tenancy","title":"Multi Tenancy","text":"<p>Thanks to the \"Organization\" feature, ThreatMatrix can be used by multiple SOCs, companies, etc...very easily. Right now it works very simply: only users in the same organization can see analysis of one another. An user can belong to an organization only.</p>"},{"location":"ThreatMatrix/advanced_usage/#manage-organizations","title":"Manage organizations","text":"<p>You can create a new organization by going to the \"Organization\" section, available under the Dropdown menu you cand find under the username.</p> <p>Once you create an organization, you are the unique \"Owner\" of that organization. So you are the only one who can delete the organization and promote/demote/kick users. Another role, which is called \"Admin\", can be set to a user (via the Django Admin interface only for now). Owners and admins share the following powers: they can manage invitations and the organization's plugin configuration.</p>"},{"location":"ThreatMatrix/advanced_usage/#accept-invites","title":"Accept Invites","text":"<p>Once an invite has sent, the invited user has to login, go to the \"Organization\" section and accept the invite there. Afterwards the Administrator will be able to see the user in his \"Organization\" section.</p> <p></p>"},{"location":"ThreatMatrix/advanced_usage/#plugins-params-and-secrets","title":"Plugins Params and Secrets","text":"<p>From ThreatMatrix v4.1.0, Plugin Parameters and Secrets can be defined at the organization level, in the dedicated section. This allows to share configurations between users of the same org while allowing complete multi-tenancy of the application. Only Owners and Admins of the organization can set, change and delete them.</p>"},{"location":"ThreatMatrix/advanced_usage/#disable-plugins-at-org-level","title":"Disable Plugins at Org level","text":"<p>The org admin can disable a specific plugin for all the users in a specific org. To do that, Org Admins needs to go in the \"Plugins\" section and click the button \"Enabled for organization\" of the plugin that they want to disable.</p> <p></p>"},{"location":"ThreatMatrix/advanced_usage/#registration","title":"Registration","text":"<p>Since ThreatMatrix v4.2.0 we added a Registration Page that can be used to manage Registration requests when providing ThreatMatrix as a Service.</p> <p>After a user registration has been made, an email is sent to the user to verify their email address. If necessary, there are buttons on the login page to resend the verification email and to reset the password.</p> <p>Once the user has verified their email, they would be manually vetted before being allowed to use the ThreatMatrix platform. The registration requests would be handled in the Django Admin page by admins. If you have ThreatMatrix deployed on an AWS instance with an IAM role you can use the SES service.</p> <p>To have the \"Registration\" page to work correctly, you must configure some variables before starting ThreatMatrix. See Optional Environment Configuration</p> <p>In a development environment the emails that would be sent are written to the standard output.</p>"},{"location":"ThreatMatrix/advanced_usage/#optional-analyzers","title":"Optional Analyzers","text":"<p>Some analyzers which run in their own Docker containers are kept disabled by default. They are disabled by default to prevent accidentally starting too many containers and making your computer unresponsive.</p> Name Analyzers Description Malware Tools Analyzers <ul> <li><code>PEframe_Scan</code></li> <li><code>Capa_Info</code></li> <li><code>Floss</code></li> <li><code>Strings_Info</code></li> <li><code>ClamAV</code></li> <li><code>APKiD</code></li> <li><code>Thug_URL_Info</code>,       <code>Thug_HTML_Info</code></li> <li><code>BoxJS</code></li> <li><code>Qiling_Windows</code>,       <code>Qiling_Windows_Shellcode</code>,       <code>Qiling_Linux</code>,       <code>Qiling_Linux_Shellcode</code></li> </ul> <ul> <li>PEFrame performs static analysis on Portable Executable malware and malicious MS Office documents</li> <li>Capa detects capabilities in executable files</li> <li>FLOSS automatically deobfuscate strings from malware binaries</li> <li>String_Info_Classic extracts human-readable strings where as ML version of it ranks them</li> <li>ClamAV antivirus engine scans files for trojans, viruses, malwares using a multi-threaded daemon</li> <li>APKiD identifies many compilers, packers, obfuscators, and other weird stuff from an APK or DEX file.</li> <li>Thug performs hybrid dynamic/static analysis on a URL or HTML page.</li> <li>Box-JS is a tool for studying JavaScript malware</li> <li>Qiling is a tool for emulating the execution of a binary file or a shellcode.      It requires the configuration of its rootfs, and the optional configuration of profiles.      The rootfs can be copied from the  Qiling project: please remember that Windows dll  must be manually added for license reasons.      Qiling provides a  DllCollector to retrieve dlls from your licensed Windows.        Profiles  must be placed in the <code>profiles</code> subfolder      </li> </ul> TOR Analyzers <code>Onionscan</code> Scans TOR .onion domains for privacy leaks and information disclosures. CyberChef <code>CyberChef</code> Run a transformation on a CyberChef server using pre-defined or custom recipes(rules that describe how the input has to be transformed). Check further instructions here PCAP Analyzers <code>Suricata</code> You can upload a PCAP to have it analyzed by Suricata with the open Ruleset. The result will provide a list of the triggered signatures plus a more detailed report with all the raw data generated by Suricata. You can also add your own rules (See paragraph \"Analyzers with special configuration\"). The installation is optimized for scaling so the execution time is really fast. PhoneInfoga <code>PhoneInfoga_scan</code> PhoneInfoga is one of the most advanced tools to scan international phone numbers. It allows you to first gather basic information such as country, area, carrier and line type, then use various techniques to try to find the VoIP provider or identify the owner. It works with a collection of scanners that must be configured in order for the tool to be effective. PhoneInfoga doesn't automate everything, it's just there to help investigating on phone numbers. here <p>To enable all the optional analyzers you can add the option <code>--all_analyzers</code> when starting the project. Example:</p> <pre><code>./start prod up --all_analyzers\n</code></pre> <p>Otherwise you can enable just one of the cited integration by using the related option. Example:</p> <pre><code>./start prod up --tor_analyzers\n</code></pre>"},{"location":"ThreatMatrix/advanced_usage/#customize-analyzer-execution","title":"Customize analyzer execution","text":"<p>Some analyzers provide the chance to customize the performed analysis based on parameters that are different for each analyzer.</p>"},{"location":"ThreatMatrix/advanced_usage/#from-the-gui","title":"from the GUI","text":"<p>You can click on \"Runtime Configuration\"  button in the \"Scan\" page and add the runtime configuration in the form of a dictionary. Example:</p> <pre><code>\"VirusTotal_v3_File\": {\n    \"force_active_scan_if_old\": true\n}\n</code></pre>"},{"location":"ThreatMatrix/advanced_usage/#from-pythreatmatrix","title":"from Pythreatmatrix","text":"<p>While using <code>send_observable_analysis_request</code> or <code>send_file_analysis_request</code> endpoints, you can pass the parameter <code>runtime_configuration</code> with the optional values. Example:</p> <pre><code>runtime_configuration = {\n    \"Doc_Info\": {\n        \"additional_passwords_to_check\": [\"passwd\", \"2020\"]\n    }\n}\npythreatmatrix_client.send_file_analysis_request(..., runtime_configuration=runtime_configuration)\n</code></pre>"},{"location":"ThreatMatrix/advanced_usage/#phoneinfoga","title":"PhoneInfoga","text":"<p>PhoneInfoga provides several Scanners to extract as much information as possible from a given phone number. Those scanners may require authentication, so they're automatically skipped when no authentication credentials are found.</p> <p>By default the scanner used is <code>local</code>. Go through this guide to initiate other required API keys related to this analyzer.</p>"},{"location":"ThreatMatrix/advanced_usage/#cyberchef","title":"CyberChef","text":"<p>You can either use pre-defined recipes or create your own as explained here.</p> <p>To use a pre-defined recipe, set the <code>predefined_recipe_name</code> argument to the name of the recipe as defined here. Else, leave the <code>predefined_recipe_name</code> argument empty and set the <code>custom_recipe</code> argument to the contents of the recipe you want to use.</p> <p>Additionally, you can also (optionally) set the <code>output_type</code> argument.</p>"},{"location":"ThreatMatrix/advanced_usage/#pre-defined-recipes","title":"Pre-defined recipes","text":"<ul> <li>\"to decimal\": <code>[{\"op\": \"To Decimal\", \"args\": [\"Space\", False]}]</code></li> </ul>"},{"location":"ThreatMatrix/advanced_usage/#analyzers-with-special-configuration","title":"Analyzers with special configuration","text":"<p>Some analyzers could require a special configuration:</p> <ul> <li><code>GoogleWebRisk</code>: this analyzer needs a service account key with the Google Cloud credentials to work properly.   You should follow the official guide for creating the key.   Then you can populate the secret <code>service_account_json</code> for that analyzer with the JSON of the service account file.</li> </ul> <ul> <li><code>ClamAV</code>: this Docker-based analyzer uses <code>clamd</code> daemon as its scanner and is communicating with <code>clamdscan</code> utility to scan files. The daemon requires 2 different configuration files: <code>clamd.conf</code>(daemon's config) and <code>freshclam.conf</code> (virus database updater's config). These files are mounted as docker volumes in <code>/integrations/malware_tools_analyzers/clamav</code> and hence, can be edited by the user as per needs, without restarting the application. Moreover ClamAV is integrated with unofficial open source signatures extracted with Fangfrisch. The configuration file <code>fangfrisch.conf</code> is mounted in the same directory and can be customized on your wish. For instance, you should change it if you want to integrate open source signatures from SecuriteInfo</li> </ul> <ul> <li> <p><code>Suricata</code>: you can customize the behavior of Suricata:</p> <ul> <li><code>/integrations/pcap_analyzers/config/suricata/rules</code>: here there are Suricata rules. You can change the <code>custom.rules</code> files to add your own rules at any time. Once you made this change, you need to either restart ThreatMatrix or (this is faster) run a new analysis with the Suricata analyzer and set the parameter <code>reload_rules</code> to <code>true</code>.</li> <li><code>/integrations/pcap_analyzers/config/suricata/etc</code>: here there are Suricata configuration files. Change it based on your wish. Restart ThreatMatrix to see the changes applied.</li> </ul> </li> </ul> <ul> <li><code>Yara</code>:<ul> <li>You can customize both the <code>repositories</code> parameter and <code>private_repositories</code> secret to download and use different rules from the default that ThreatMatrix currently support.<ul> <li>The <code>repositories</code> values is what will be used to actually run the analysis: if you have added private repositories, remember to add the url in <code>repositories</code> too!</li> </ul> </li> <li>You can add local rules inside the directory at <code>/opt/deploy/files_required/yara/YOUR_USERNAME/custom_rules/</code>. Please remember that these rules are not synced in a cluster deploy: for this reason is advised to upload them on GitHub and use the <code>repositories</code> or <code>private_repositories</code> attributes.</li> </ul> </li> </ul>"},{"location":"ThreatMatrix/advanced_usage/#notifications","title":"Notifications","text":"<p>Since v4, ThreatMatrix integrated the notification system from the <code>certego_saas</code> package, allowing the admins to create notification that every user will be able to see.</p> <p>The user would find the Notifications button on the top right of the page:</p> <p></p> <p>There the user can read notifications provided by either the administrators or the ThreatMatrix Maintainers.</p> <p>As an Admin, if you want to add a notification to have it sent to all the users, you have to login to the Django Admin interface, go to the \"Notifications\" section and add it there. While adding a new notification, in the <code>body</code> section it is possible to even use HTML syntax, allowing to embed images, links, etc; in the <code>app_name field</code>, please remember to use <code>threatmatrix</code> as the app name.</p> <p>Everytime a new release is installed, once the backend goes up it will automatically create a new notification, having as content the latest changes described in the CHANGELOG.md, allowing the users to keep track of the changes inside threatmatrix itself.</p>"},{"location":"ThreatMatrix/api_docs/","title":"API Documentation","text":""},{"location":"ThreatMatrix/api_docs/#global-functions","title":"Global Functions","text":""},{"location":"ThreatMatrix/api_docs/#ask_analysis_availability","title":"<code>ask_analysis_availability</code>","text":"<p>API endpoint to check for existing analysis based on an MD5 hash.</p> <p>This endpoint helps avoid redundant analysis by checking if there is already an analysis in progress or completed with status \"running\" or \"reported_without_fails\" for the provided MD5 hash. The analyzers that need to be executed should be specified to ensure expected results.</p> <p>Deprecated: This endpoint will be deprecated after 01-07-2023.</p> <p>Parameters: - request (POST): Contains the MD5 hash and analyzer details.</p> <p>Returns: - 200: JSON response with the analysis status, job ID, and analyzers to be executed.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    This is useful to avoid repeating the same analysis multiple times.\n    By default this API checks if there are existing analysis related to the md5 in\n    status \"running\" or \"reported_without_fails\"\n    Also, you need to specify the analyzers needed because, otherwise, it is\n    highly probable that you won't get all the results that you expect\"\"\",\n    request=JobAvailabilitySerializer,\n    responses={\n        200: inline_serializer(\n            name=\"AskAnalysisAvailabilitySuccessResponse\",\n            fields={\n                \"status\": rfs.StringRelatedField(),\n                \"job_id\": rfs.StringRelatedField(),\n                \"analyzers_to_execute\": OpenApiTypes.OBJECT,\n            },\n        ),\n    },\n)\n@deprecated_endpoint(deprecation_date=\"01-07-2023\")\n@api_view([\"POST\"])\ndef ask_analysis_availability(request):\n    \"\"\"\n    API endpoint to check for existing analysis based on an MD5 hash.\n\n    This endpoint helps avoid redundant analysis by checking if there is already an analysis\n    in progress or completed with status \"running\" or \"reported_without_fails\" for the provided MD5 hash.\n    The analyzers that need to be executed should be specified to ensure expected results.\n\n    Deprecated: This endpoint will be deprecated after 01-07-2023.\n\n    Parameters:\n    - request (POST): Contains the MD5 hash and analyzer details.\n\n    Returns:\n    - 200: JSON response with the analysis status, job ID, and analyzers to be executed.\n    \"\"\"\n    serializer = JobAvailabilitySerializer(\n        data=request.data, context={\"request\": request}\n    )\n    serializer.is_valid(raise_exception=True)\n    try:\n        job = serializer.save()\n    except Job.DoesNotExist:\n        result = None\n    else:\n        result = job\n    return Response(\n        JobResponseSerializer(result).data,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#ask_multi_analysis_availability","title":"<code>ask_multi_analysis_availability</code>","text":"<p>API endpoint to check for existing analysis for multiple MD5 hashes.</p> <p>Similar to <code>ask_analysis_availability</code>, this endpoint checks for existing analysis for multiple MD5 hashes. It prevents redundant analysis by verifying if there are any jobs in progress or completed with status \"running\" or \"reported_without_fails\". The analyzers required should be specified to ensure accurate results.</p> <p>Parameters: - request (POST): Contains multiple MD5 hashes and analyzer details.</p> <p>Returns: - 200: JSON response with the analysis status, job IDs, and analyzers to be executed for each MD5 hash.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    This is useful to avoid repeating the same analysis multiple times.\n    By default this API checks if there are existing analysis related to the md5 in\n    status \"running\" or \"reported_without_fails\"\n    Also, you need to specify the analyzers needed because, otherwise, it is\n    highly probable that you won't get all the results that you expect.\n    NOTE: This API is similar to ask_analysis_availability, but it allows multiple\n    md5s to be checked at the same time.\"\"\",\n    responses={200: JobAvailabilitySerializer(many=True)},\n)\n@api_view([\"POST\"])\ndef ask_multi_analysis_availability(request):\n    \"\"\"\n    API endpoint to check for existing analysis for multiple MD5 hashes.\n\n    Similar to `ask_analysis_availability`, this endpoint checks for existing analysis for multiple MD5 hashes.\n    It prevents redundant analysis by verifying if there are any jobs in progress or completed with status\n    \"running\" or \"reported_without_fails\". The analyzers required should be specified to ensure accurate results.\n\n    Parameters:\n    - request (POST): Contains multiple MD5 hashes and analyzer details.\n\n    Returns:\n    - 200: JSON response with the analysis status, job IDs, and analyzers to be executed for each MD5 hash.\n    \"\"\"\n    logger.info(f\"received ask_multi_analysis_availability from user {request.user}\")\n    serializer = JobAvailabilitySerializer(\n        data=request.data, context={\"request\": request}, many=True\n    )\n    serializer.is_valid(raise_exception=True)\n    try:\n        jobs = serializer.save()\n    except Job.DoesNotExist:\n        result = []\n    else:\n        result = jobs\n    jrs = JobResponseSerializer(result, many=True).data\n    logger.info(f\"finished ask_multi_analysis_availability from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#analyze_file","title":"<code>analyze_file</code>","text":"<p>API endpoint to start an analysis job for a single file.</p> <p>This endpoint initiates an analysis job for a single file and sends it to the specified analyzers. The file-related information and analyzers should be provided in the request data.</p> <p>Parameters: - request (POST): Contains file data and analyzer details.</p> <p>Returns: - 200: JSON response with the job details after initiating the analysis.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"This endpoint allows to start a Job related for a single File.\"\n    \" Retained for retro-compatibility\",\n    request=FileJobSerializer,\n    responses={200: JobResponseSerializer(many=True)},\n)\n@api_view([\"POST\"])\ndef analyze_file(request):\n    \"\"\"\n    API endpoint to start an analysis job for a single file.\n\n    This endpoint initiates an analysis job for a single file and sends it to the\n    specified analyzers. The file-related information and analyzers should be provided\n    in the request data.\n\n    Parameters:\n    - request (POST): Contains file data and analyzer details.\n\n    Returns:\n    - 200: JSON response with the job details after initiating the analysis.\n    \"\"\"\n    logger.info(f\"received analyze_file from user {request.user}\")\n    fas = FileJobSerializer(data=request.data, context={\"request\": request})\n    fas.is_valid(raise_exception=True)\n    job = fas.save(send_task=True)\n    jrs = JobResponseSerializer(job).data\n    logger.info(f\"finished analyze_file from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#analyze_multiple_files","title":"<code>analyze_multiple_files</code>","text":"<p>API endpoint to start analysis jobs for multiple files.</p> <p>This endpoint initiates analysis jobs for multiple files and sends them to the specified analyzers. The file-related information and analyzers should be provided in the request data.</p> <p>Parameters: - request (POST): Contains multiple file data and analyzer details.</p> <p>Returns: - 200: JSON response with the job details for each initiated analysis.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"This endpoint allows to start Jobs related to multiple Files\",\n    # It should be better to link the doc to the related MultipleFileAnalysisSerializer.\n    # It is not straightforward because you can't just add a class\n    # which extends a ListSerializer.\n    # Follow this doc to try to find a fix:\n    # https://drf-spectacular.readthedocs.io/en/latest/customization.html#declare-serializer-magic-with\n    # -openapiserializerextension\n    request=inline_serializer(\n        name=\"MultipleFilesSerializer\",\n        fields={\n            \"files\": rfs.ListField(child=rfs.FileField()),\n            \"file_names\": rfs.ListField(child=rfs.CharField()),\n            \"file_mimetypes\": rfs.ListField(child=rfs.CharField()),\n        },\n    ),\n    responses={200: JobResponseSerializer},\n)\n@api_view([\"POST\"])\ndef analyze_multiple_files(request):\n    \"\"\"\n    API endpoint to start analysis jobs for multiple files.\n\n    This endpoint initiates analysis jobs for multiple files and sends them to the specified analyzers.\n    The file-related information and analyzers should be provided in the request data.\n\n    Parameters:\n    - request (POST): Contains multiple file data and analyzer details.\n\n    Returns:\n    - 200: JSON response with the job details for each initiated analysis.\n    \"\"\"\n    logger.info(f\"received analyze_multiple_files from user {request.user}\")\n    fas = FileJobSerializer(data=request.data, context={\"request\": request}, many=True)\n    fas.is_valid(raise_exception=True)\n    parent_job = fas.validated_data[0].get(\"parent_job\", None)\n    jobs = fas.save(send_task=True, parent=parent_job)\n    jrs = JobResponseSerializer(jobs, many=True).data\n    logger.info(f\"finished analyze_multiple_files from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#analyze_observable","title":"<code>analyze_observable</code>","text":"<p>API endpoint to start an analysis job for a single observable.</p> <p>This endpoint initiates an analysis job for a single observable (e.g., domain, IP, URL, etc.) and sends it to the specified analyzers. The observable-related information and analyzers should be provided in the request data.</p> <p>Parameters: - request (POST): Contains observable data and analyzer details.</p> <p>Returns: - 200: JSON response with the job details after initiating the analysis.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"This endpoint allows to start a Job related to an observable. \"\n    \"Retained for retro-compatibility\",\n    request=ObservableAnalysisSerializer,\n    responses={200: JobResponseSerializer},\n)\n@api_view([\"POST\"])\ndef analyze_observable(request):\n    \"\"\"\n    API endpoint to start an analysis job for a single observable.\n\n    This endpoint initiates an analysis job for a single observable (e.g., domain, IP, URL, etc.)\n    and sends it to the specified analyzers. The observable-related information and analyzers should be\n    provided in the request data.\n\n    Parameters:\n    - request (POST): Contains observable data and analyzer details.\n\n    Returns:\n    - 200: JSON response with the job details after initiating the analysis.\n    \"\"\"\n    logger.info(f\"received analyze_observable from user {request.user}\")\n    oas = ObservableAnalysisSerializer(data=request.data, context={\"request\": request})\n    oas.is_valid(raise_exception=True)\n    job = oas.save(send_task=True)\n    jrs = JobResponseSerializer(job).data\n    logger.info(f\"finished analyze_observable from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#analyze_multiple_observables","title":"<code>analyze_multiple_observables</code>","text":"<p>API endpoint to start analysis jobs for multiple observables.</p> <p>This endpoint initiates analysis jobs for multiple observables (e.g., domain, IP, URL, etc.) and sends them to the specified analyzers. The observables and analyzer details should be provided in the request data.</p> <p>Parameters: - request (POST): Contains multiple observable data and analyzer details.</p> <p>Returns: - 200: JSON response with the job details for each initiated analysis.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"This endpoint allows to start Jobs related to multiple observables.\n                 Observable parameter must be composed like this:\n                 [(&lt;observable_classification&gt;, &lt;observable_name&gt;), ...]\"\"\",\n    request=inline_serializer(\n        name=\"MultipleObservableSerializer\",\n        fields={\n            \"observables\": rfs.ListField(\n                child=rfs.ListField(max_length=2, min_length=2)\n            )\n        },\n    ),\n    responses={200: JobResponseSerializer},\n)\n@api_view([\"POST\"])\ndef analyze_multiple_observables(request):\n    \"\"\"\n    API endpoint to start analysis jobs for multiple observables.\n\n    This endpoint initiates analysis jobs for multiple observables (e.g., domain, IP, URL, etc.)\n    and sends them to the specified analyzers. The observables and analyzer details should\n    be provided in the request data.\n\n    Parameters:\n    - request (POST): Contains multiple observable data and analyzer details.\n\n    Returns:\n    - 200: JSON response with the job details for each initiated analysis.\n    \"\"\"\n    logger.info(f\"received analyze_multiple_observables from user {request.user}\")\n    oas = ObservableAnalysisSerializer(\n        data=request.data, many=True, context={\"request\": request}\n    )\n    oas.is_valid(raise_exception=True)\n    parent_job = oas.validated_data[0].get(\"parent_job\", None)\n    jobs = oas.save(send_task=True, parent=parent_job)\n    jrs = JobResponseSerializer(jobs, many=True).data\n    logger.info(f\"finished analyze_multiple_observables from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#classes","title":"Classes","text":""},{"location":"ThreatMatrix/api_docs/#commentviewset","title":"<code>CommentViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>CommentViewSet provides the following actions:</p> <ul> <li>list: Retrieve a list of comments associated with jobs visible to the authenticated user.</li> <li>retrieve: Retrieve a specific comment by ID, accessible to the comment's owner or anyone in the same organization.</li> <li>destroy: Delete a comment by ID, allowed only for the comment's owner.</li> <li>update: Update a comment by ID, allowed only for the comment's owner.</li> <li>partial_update: Partially update a comment by ID, allowed only for the comment's owner.</li> </ul> <p>Permissions: - IsAuthenticated: Requires authentication for all actions. - IsObjectUserPermission: Allows only the comment owner to update or delete the comment. - IsObjectUserOrSameOrgPermission: Allows the comment owner or anyone in the same organization to retrieve the comment.</p> <p>Queryset: - Filters comments to include only those associated with jobs visible to the authenticated user.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    REST endpoint to fetch list of job comments or\n    retrieve/delete a job comment with job comment ID.\n    Requires authentication.\n    \"\"\"\n)\nclass CommentViewSet(ModelViewSet):\n    \"\"\"\n    CommentViewSet provides the following actions:\n\n    - **list**: Retrieve a list of comments associated with jobs visible to the authenticated user.\n    - **retrieve**: Retrieve a specific comment by ID, accessible to the comment's owner or anyone in the same organization.\n    - **destroy**: Delete a comment by ID, allowed only for the comment's owner.\n    - **update**: Update a comment by ID, allowed only for the comment's owner.\n    - **partial_update**: Partially update a comment by ID, allowed only for the comment's owner.\n\n    Permissions:\n    - **IsAuthenticated**: Requires authentication for all actions.\n    - **IsObjectUserPermission**: Allows only the comment owner to update or delete the comment.\n    - **IsObjectUserOrSameOrgPermission**: Allows the comment owner or anyone in the same organization to retrieve the comment.\n\n    Queryset:\n    - Filters comments to include only those associated with jobs visible to the authenticated user.\n    \"\"\"\n\n    queryset = Comment.objects.all()\n    serializer_class = CommentSerializer\n    permission_classes = [IsAuthenticated]\n\n    def get_permissions(self):\n        \"\"\"\n        Customizes permissions based on the action being performed.\n\n        - For `destroy`, `update`, and `partial_update` actions, adds `IsObjectUserPermission` to ensure that only\n          the comment owner can perform these actions.\n        - For the `retrieve` action, adds `IsObjectUserOrSameOrgPermission` to allow the comment owner or anyone in the same\n          organization to retrieve the comment.\n\n        Returns:\n        - List of applicable permissions.\n        \"\"\"\n        permissions = super().get_permissions()\n\n        # only the owner of the comment can update or delete the comment\n        if self.action in [\"destroy\", \"update\", \"partial_update\"]:\n            permissions.append(IsObjectUserPermission())\n        # the owner and anyone in the org can read the comment\n        if self.action in [\"retrieve\"]:\n            permissions.append(IsObjectUserOrSameOrgPermission())\n\n        return permissions\n\n    def get_queryset(self):\n        \"\"\"\n        Filters the queryset to include only comments related to jobs visible to the authenticated user.\n\n        - Fetches job IDs that are visible to the user.\n        - Filters the comment queryset to include only comments associated with these jobs.\n\n        Returns:\n        - Filtered queryset of comments.\n        \"\"\"\n        queryset = super().get_queryset()\n        jobs = Job.objects.visible_for_user(self.request.user).values_list(\n            \"pk\", flat=True\n        )\n        return queryset.filter(job__id__in=jobs)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.CommentViewSet.get_permissions","title":"<code>get_permissions()</code>","text":"<p>Customizes permissions based on the action being performed.</p> <ul> <li>For <code>destroy</code>, <code>update</code>, and <code>partial_update</code> actions, adds <code>IsObjectUserPermission</code> to ensure that only   the comment owner can perform these actions.</li> <li>For the <code>retrieve</code> action, adds <code>IsObjectUserOrSameOrgPermission</code> to allow the comment owner or anyone in the same   organization to retrieve the comment.</li> </ul> <p>Returns: - List of applicable permissions.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_permissions(self):\n    \"\"\"\n    Customizes permissions based on the action being performed.\n\n    - For `destroy`, `update`, and `partial_update` actions, adds `IsObjectUserPermission` to ensure that only\n      the comment owner can perform these actions.\n    - For the `retrieve` action, adds `IsObjectUserOrSameOrgPermission` to allow the comment owner or anyone in the same\n      organization to retrieve the comment.\n\n    Returns:\n    - List of applicable permissions.\n    \"\"\"\n    permissions = super().get_permissions()\n\n    # only the owner of the comment can update or delete the comment\n    if self.action in [\"destroy\", \"update\", \"partial_update\"]:\n        permissions.append(IsObjectUserPermission())\n    # the owner and anyone in the org can read the comment\n    if self.action in [\"retrieve\"]:\n        permissions.append(IsObjectUserOrSameOrgPermission())\n\n    return permissions\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.CommentViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Filters the queryset to include only comments related to jobs visible to the authenticated user.</p> <ul> <li>Fetches job IDs that are visible to the user.</li> <li>Filters the comment queryset to include only comments associated with these jobs.</li> </ul> <p>Returns: - Filtered queryset of comments.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Filters the queryset to include only comments related to jobs visible to the authenticated user.\n\n    - Fetches job IDs that are visible to the user.\n    - Filters the comment queryset to include only comments associated with these jobs.\n\n    Returns:\n    - Filtered queryset of comments.\n    \"\"\"\n    queryset = super().get_queryset()\n    jobs = Job.objects.visible_for_user(self.request.user).values_list(\n        \"pk\", flat=True\n    )\n    return queryset.filter(job__id__in=jobs)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#jobviewset","title":"<code>JobViewSet</code>","text":"<p>               Bases: <code>ReadAndDeleteOnlyViewSet</code>, <code>SerializerActionMixin</code></p> <p>JobViewSet provides the following actions:</p> <ul> <li>list: Retrieve a list of jobs visible to the authenticated user, ordered by request time.</li> <li>retrieve: Retrieve a specific job by ID.</li> <li>destroy: Delete a job by ID, allowed only for the job owner or anyone in the same organization.</li> <li>recent_scans: Retrieve recent jobs based on an MD5 hash, limited by a maximum temporal distance.</li> <li>recent_scans_user: Retrieve recent jobs for the authenticated user, filtered by sample status.</li> <li>retry: Retry a job if its status is in a final state.</li> <li>kill: Kill a running job by closing celery tasks and marking it as killed.</li> <li>download_sample: Download a file/sample associated with a job.</li> <li>pivot: Perform a pivot operation from a job's reports.</li> <li>aggregate_status: Aggregate jobs by their status over a specified time range.</li> <li>aggregate_type: Aggregate jobs by type (file or observable) over a specified time range.</li> <li>aggregate_observable_classification: Aggregate jobs by observable classification over a specified time range.</li> <li>aggregate_file_mimetype: Aggregate jobs by file MIME type over a specified time range.</li> <li>aggregate_observable_name: Aggregate jobs by observable name over a specified time range.</li> <li>aggregate_md5: Aggregate jobs by MD5 hash over a specified time range.</li> </ul> <p>Permissions: - IsAuthenticated: Requires authentication for all actions. - IsObjectUserOrSameOrgPermission: Allows job deletion or killing only by the job owner or anyone in the same organization.</p> <p>Queryset: - Prefetches related tags and orders jobs by request time, filtered to include only jobs visible to the authenticated user.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    REST endpoint to fetch list of jobs or retrieve/delete a job with job ID.\n    Requires authentication.\n    \"\"\"\n)\nclass JobViewSet(ReadAndDeleteOnlyViewSet, SerializerActionMixin):\n    \"\"\"\n    JobViewSet provides the following actions:\n\n    - **list**: Retrieve a list of jobs visible to the authenticated user, ordered by request time.\n    - **retrieve**: Retrieve a specific job by ID.\n    - **destroy**: Delete a job by ID, allowed only for the job owner or anyone in the same organization.\n    - **recent_scans**: Retrieve recent jobs based on an MD5 hash, limited by a maximum temporal distance.\n    - **recent_scans_user**: Retrieve recent jobs for the authenticated user, filtered by sample status.\n    - **retry**: Retry a job if its status is in a final state.\n    - **kill**: Kill a running job by closing celery tasks and marking it as killed.\n    - **download_sample**: Download a file/sample associated with a job.\n    - **pivot**: Perform a pivot operation from a job's reports.\n    - **aggregate_status**: Aggregate jobs by their status over a specified time range.\n    - **aggregate_type**: Aggregate jobs by type (file or observable) over a specified time range.\n    - **aggregate_observable_classification**: Aggregate jobs by observable classification over a specified time range.\n    - **aggregate_file_mimetype**: Aggregate jobs by file MIME type over a specified time range.\n    - **aggregate_observable_name**: Aggregate jobs by observable name over a specified time range.\n    - **aggregate_md5**: Aggregate jobs by MD5 hash over a specified time range.\n\n    Permissions:\n    - **IsAuthenticated**: Requires authentication for all actions.\n    - **IsObjectUserOrSameOrgPermission**: Allows job deletion or killing only by the job owner or anyone in the same organization.\n\n    Queryset:\n    - Prefetches related tags and orders jobs by request time, filtered to include only jobs visible to the authenticated user.\n    \"\"\"\n\n    queryset = (\n        Job.objects.prefetch_related(\"tags\").order_by(\"-received_request_time\").all()\n    )\n    serializer_class = RestJobSerializer\n    serializer_action_classes = {\n        \"retrieve\": RestJobSerializer,\n        \"list\": JobListSerializer,\n    }\n    filterset_class = JobFilter\n    ordering_fields = [\n        \"received_request_time\",\n        \"finished_analysis_time\",\n        \"process_time\",\n    ]\n\n    def get_permissions(self):\n        \"\"\"\n        Customizes permissions based on the action being performed.\n\n        - For `destroy` and `kill` actions, adds `IsObjectUserOrSameOrgPermission` to ensure that only\n          the job owner or anyone in the same organization can perform these actions.\n\n        Returns:\n        - List of applicable permissions.\n        \"\"\"\n        permissions = super().get_permissions()\n        if self.action in [\"destroy\", \"kill\"]:\n            permissions.append(IsObjectUserOrSameOrgPermission())\n        return permissions\n\n    def get_queryset(self):\n        \"\"\"\n        Filters the queryset to include only jobs visible to the authenticated user, ordered by request time.\n\n        Logs the request parameters and returns the filtered queryset.\n\n        Returns:\n        - Filtered queryset of jobs.\n        \"\"\"\n        user = self.request.user\n        logger.info(\n            f\"user: {user} request the jobs with params: {self.request.query_params}\"\n        )\n        return Job.objects.visible_for_user(user).order_by(\"-received_request_time\")\n\n    @action(detail=False, methods=[\"post\"])\n    def recent_scans(self, request):\n        \"\"\"\n        Retrieve recent jobs based on an MD5 hash, filtered by a maximum temporal distance.\n\n        Expects the following parameters in the request data:\n        - `md5`: The MD5 hash to filter jobs by.\n        - `max_temporal_distance`: The maximum number of days to look back for recent jobs (default is 14 days).\n\n        Returns:\n        - List of recent jobs matching the MD5 hash.\n        \"\"\"\n        if \"md5\" not in request.data:\n            raise ValidationError({\"detail\": \"md5 is required\"})\n        max_temporal_distance = request.data.get(\"max_temporal_distance\", 14)\n        jobs = (\n            Job.objects.filter(md5=request.data[\"md5\"])\n            .visible_for_user(self.request.user)\n            .filter(\n                finished_analysis_time__gte=now()\n                - datetime.timedelta(days=max_temporal_distance)\n            )\n            .annotate_importance(request.user)\n            .order_by(\"-importance\", \"-finished_analysis_time\")\n        )\n        return Response(\n            JobRecentScanSerializer(jobs, many=True).data, status=status.HTTP_200_OK\n        )\n\n    @action(detail=False, methods=[\"post\"])\n    def recent_scans_user(self, request):\n        \"\"\"\n        Retrieve recent jobs for the authenticated user, filtered by sample status.\n\n        Expects the following parameters in the request data:\n        - `is_sample`: Whether to filter jobs by sample status (required).\n        - `limit`: The maximum number of recent jobs to return (default is 5).\n\n        Returns:\n        - List of recent jobs for the user.\n        \"\"\"\n        limit = request.data.get(\"limit\", 5)\n        if \"is_sample\" not in request.data:\n            raise ValidationError({\"detail\": \"is_sample is required\"})\n        jobs = (\n            Job.objects.filter(user__pk=request.user.pk)\n            .filter(is_sample=request.data[\"is_sample\"])\n            .annotate_importance(request.user)\n            .order_by(\"-importance\", \"-finished_analysis_time\")[:limit]\n        )\n        return Response(\n            JobRecentScanSerializer(jobs, many=True).data, status=status.HTTP_200_OK\n        )\n\n    @action(detail=True, methods=[\"patch\"])\n    def retry(self, request, pk=None):\n        \"\"\"\n        Retry a job if its status is in a final state.\n\n        If the job is currently running, raises a validation error.\n\n        Returns:\n        - No content (204) if the job is successfully retried.\n        \"\"\"\n        job = self.get_object()\n        if job.status not in Job.Status.final_statuses():\n            raise ValidationError({\"detail\": \"Job is running\"})\n        job.retry()\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n    @add_docs(\n        description=\"Kill running job by closing celery tasks and marking as killed\",\n        request=None,\n        responses={\n            204: None,\n        },\n    )\n    @action(detail=True, methods=[\"patch\"])\n    def kill(self, request, pk=None):\n        \"\"\"\n        Kill a running job by closing celery tasks and marking the job as killed.\n\n        If the job is not running, raises a validation error.\n\n        Returns:\n        - No content (204) if the job is successfully killed.\n        \"\"\"\n        # get job object or raise 404\n        job = self.get_object()\n\n        # check if job running\n        if job.status in Job.Status.final_statuses():\n            raise ValidationError({\"detail\": \"Job is not running\"})\n        # close celery tasks and mark reports as killed\n        job.kill_if_ongoing()\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n    @add_docs(\n        description=\"Download file/sample associated with a job\",\n        request=None,\n        responses={200: OpenApiTypes.BINARY, 400: None},\n    )\n    @action(detail=True, methods=[\"get\"])\n    def download_sample(self, request, pk=None):\n        \"\"\"\n        Download a sample associated with a job.\n\n        If the job does not have a sample, raises a validation error.\n\n        Returns:\n        - The file associated with the job as an attachment.\n\n        :param url: pk (job_id)\n        :returns: bytes\n        \"\"\"\n        # get job object\n        job = self.get_object()\n\n        # make sure it is a sample\n        if not job.is_sample:\n            raise ValidationError(\n                {\"detail\": \"Requested job does not have a sample associated with it.\"}\n            )\n        return FileResponse(\n            job.file,\n            filename=job.file_name,\n            content_type=job.file_mimetype,\n            as_attachment=True,\n        )\n\n    @add_docs(description=\"Pivot a job\")\n    @action(\n        detail=True, methods=[\"post\"]\n    )  # , url_path=\"pivot-(?P&lt;pivot_config_pk&gt;\\d+)\")\n    def pivot(self, request, pk=None, pivot_config_pk=None):\n        \"\"\"\n        Perform a pivot operation from a job's reports based on a specified pivot configuration.\n\n        Expects the following parameters:\n        - `pivot_config_pk`: The primary key of the pivot configuration to use.\n\n        Returns:\n        - List of job IDs created as a result of the pivot.\n        \"\"\"\n        starting_job = self.get_object()\n        try:\n            pivot_config: PivotConfig = PivotConfig.objects.get(pk=pivot_config_pk)\n        except PivotConfig.DoesNotExist:\n            raise ValidationError({\"detail\": \"Requested pivot config does not exist.\"})\n        else:\n            try:\n                pivots = pivot_config.pivot_job(starting_job.reports)\n            except KeyError:\n                msg = (\n                    f\"Unable to retrieve value at {self.field}\"\n                    f\" from job {starting_job.pk}\"\n                )\n                logger.error(msg)\n                raise ValidationError({\"detail\": msg})\n            except Exception as e:\n                logger.exception(e)\n                raise ValidationError(\n                    {\"detail\": f\"Unable to start pivot from job {starting_job.pk}\"}\n                )\n            else:\n                return Response(\n                    [pivot.ending_job.pk for pivot in pivots],\n                    status=status.HTTP_201_CREATED,\n                )\n\n    @action(\n        url_path=\"aggregate/status\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_status(self, request):\n        \"\"\"\n        Aggregate jobs by their status.\n\n        Returns:\n        - Aggregated count of jobs for each status.\n        \"\"\"\n        annotations = {\n            key.lower(): Count(\"status\", filter=Q(status=key))\n            for key in Job.Status.values\n        }\n        return self.__aggregation_response_static(\n            annotations, users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/type\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_type(self, request):\n        \"\"\"\n        Aggregate jobs by type (file or observable).\n\n        Returns:\n        - Aggregated count of jobs for each type.\n        \"\"\"\n        annotations = {\n            \"file\": Count(\"is_sample\", filter=Q(is_sample=True)),\n            \"observable\": Count(\"is_sample\", filter=Q(is_sample=False)),\n        }\n        return self.__aggregation_response_static(\n            annotations, users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/observable_classification\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_observable_classification(self, request):\n        \"\"\"\n        Aggregate jobs by observable classification.\n\n        Returns:\n        - Aggregated count of jobs for each observable classification.\n        \"\"\"\n        annotations = {\n            oc.lower(): Count(\n                \"observable_classification\", filter=Q(observable_classification=oc)\n            )\n            for oc in ObservableTypes.values\n        }\n        return self.__aggregation_response_static(\n            annotations, users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/file_mimetype\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_file_mimetype(self, request):\n        \"\"\"\n        Aggregate jobs by file MIME type.\n\n        Returns:\n        - Aggregated count of jobs for each MIME type.\n        \"\"\"\n        return self.__aggregation_response_dynamic(\n            \"file_mimetype\", users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/observable_name\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_observable_name(self, request):\n        \"\"\"\n        Aggregate jobs by observable name.\n\n        Returns:\n        - Aggregated count of jobs for each observable name.\n        \"\"\"\n        return self.__aggregation_response_dynamic(\n            \"observable_name\", False, users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/md5\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_md5(self, request):\n        \"\"\"\n        Aggregate jobs by MD5 hash.\n\n        Returns:\n        - Aggregated count of jobs for each MD5 hash.\n        \"\"\"\n        # this is for file\n        return self.__aggregation_response_dynamic(\n            \"md5\", False, users=self.get_org_members(request)\n        )\n\n    @staticmethod\n    def get_org_members(request):\n        \"\"\"\n        Retrieve members of the organization associated with the authenticated user.\n\n        If the 'org' query parameter is set to 'true', this method returns all\n        users who are members of the authenticated user's organization.\n\n        Args:\n            request: The HTTP request object containing user information and query parameters.\n\n        Returns:\n            list or None: A list of users who are members of the user's organization\n            if the 'org' query parameter is 'true', otherwise None.\n        \"\"\"\n        user = request.user\n        org_param = request.GET.get(\"org\", \"\").lower() == \"true\"\n        users_of_organization = None\n        if org_param:\n            organization = user.membership.organization\n            users_of_organization = [\n                membership.user for membership in organization.members.all()\n            ]\n        return users_of_organization\n\n    def __aggregation_response_static(self, annotations: dict, users=None) -&gt; Response:\n        \"\"\"\n        Generate a static aggregation of Job objects filtered by a time range.\n\n        This method applies the provided annotations to aggregate Job objects\n        within the specified time range. Optionally, it filters the results by\n        the given list of users.\n\n        Args:\n            annotations (dict): Annotations to apply for the aggregation.\n            users (list, optional): A list of users to filter the Job objects by.\n\n        Returns:\n            Response: A Django REST framework Response object containing the aggregated data.\n        \"\"\"\n        delta, basis = self.__parse_range(self.request)\n        filter_kwargs = {\"received_request_time__gte\": delta}\n        if users:\n            filter_kwargs[\"user__in\"] = users\n        qs = (\n            Job.objects.filter(**filter_kwargs)\n            .annotate(date=Trunc(\"received_request_time\", basis))\n            .values(\"date\")\n            .annotate(**annotations)\n        )\n        return Response(qs)\n\n    def __aggregation_response_dynamic(\n        self,\n        field_name: str,\n        group_by_date: bool = True,\n        limit: int = 5,\n        users=None,\n    ) -&gt; Response:\n        \"\"\"\n        Dynamically aggregate Job objects based on a specified field and time range.\n\n        This method identifies the most frequent values of a given field within\n        a specified time range and aggregates the Job objects accordingly.\n        Optionally, it can group the results by date and limit the number of\n        most frequent values.\n\n        Args:\n            field_name (str): The name of the field to aggregate by.\n            group_by_date (bool, optional): Whether to group the results by date. Defaults to True.\n            limit (int, optional): The maximum number of most frequent values to retrieve. Defaults to 5.\n            users (list, optional): A list of users to filter the Job objects by.\n\n        Returns:\n            Response: A Django REST framework Response object containing the most frequent values\n            and the aggregated data.\n        \"\"\"\n        delta, basis = self.__parse_range(self.request)\n        filter_kwargs = {\"received_request_time__gte\": delta}\n        if users:\n            filter_kwargs[\"user__in\"] = users\n        if field_name == \"md5\":\n            filter_kwargs[\"is_sample\"] = True\n\n        most_frequent_values = (\n            Job.objects.filter(**filter_kwargs)\n            .exclude(**{f\"{field_name}__isnull\": True})\n            .exclude(**{f\"{field_name}__exact\": \"\"})\n            # excluding those because they could lead to SQL query errors\n            .exclude(\n                observable_classification__in=[\n                    ObservableClassification.URL,\n                    ObservableClassification.GENERIC,\n                ]\n            )\n            .annotate(count=Count(field_name))\n            .distinct()\n            .order_by(\"-count\")[:limit]\n            .values_list(field_name, flat=True)\n        )\n\n        logger.info(\n            f\"request: {field_name} found most_frequent_values: {most_frequent_values}\"\n        )\n\n        if len(most_frequent_values):\n            annotations = {\n                val: Count(field_name, filter=Q(**{field_name: val}))\n                for val in most_frequent_values\n            }\n            logger.debug(f\"request: {field_name} annotations: {annotations}\")\n            if group_by_date:\n                aggregation = (\n                    Job.objects.filter(**filter_kwargs)\n                    .annotate(date=Trunc(\"received_request_time\", basis))\n                    .values(\"date\")\n                    .annotate(**annotations)\n                )\n            else:\n                aggregation = Job.objects.filter(**filter_kwargs).aggregate(\n                    **annotations\n                )\n        else:\n            aggregation = {}\n\n        return Response(\n            {\n                \"values\": most_frequent_values,\n                \"aggregation\": aggregation,\n            }\n        )\n\n    @staticmethod\n    def __parse_range(request):\n        \"\"\"\n        Parse the time range from the request query parameters.\n\n        This method attempts to extract the 'range' query parameter from the\n        request. If the parameter is not provided, it defaults to '7d' (7 days).\n\n        Args:\n            request: The HTTP request object containing query parameters.\n\n        Returns:\n            tuple: A tuple containing the parsed time delta and the basis for date truncation.\n        \"\"\"\n        try:\n            range_str = request.GET[\"range\"]\n        except KeyError:\n            # default\n            range_str = \"7d\"\n\n        return parse_humanized_range(range_str)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.__aggregation_response_dynamic","title":"<code>__aggregation_response_dynamic(field_name, group_by_date=True, limit=5, users=None)</code>","text":"<p>Dynamically aggregate Job objects based on a specified field and time range.</p> <p>This method identifies the most frequent values of a given field within a specified time range and aggregates the Job objects accordingly. Optionally, it can group the results by date and limit the number of most frequent values.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>The name of the field to aggregate by.</p> required <code>group_by_date</code> <code>bool</code> <p>Whether to group the results by date. Defaults to True.</p> <code>True</code> <code>limit</code> <code>int</code> <p>The maximum number of most frequent values to retrieve. Defaults to 5.</p> <code>5</code> <code>users</code> <code>list</code> <p>A list of users to filter the Job objects by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>A Django REST framework Response object containing the most frequent values</p> <code>Response</code> <p>and the aggregated data.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def __aggregation_response_dynamic(\n    self,\n    field_name: str,\n    group_by_date: bool = True,\n    limit: int = 5,\n    users=None,\n) -&gt; Response:\n    \"\"\"\n    Dynamically aggregate Job objects based on a specified field and time range.\n\n    This method identifies the most frequent values of a given field within\n    a specified time range and aggregates the Job objects accordingly.\n    Optionally, it can group the results by date and limit the number of\n    most frequent values.\n\n    Args:\n        field_name (str): The name of the field to aggregate by.\n        group_by_date (bool, optional): Whether to group the results by date. Defaults to True.\n        limit (int, optional): The maximum number of most frequent values to retrieve. Defaults to 5.\n        users (list, optional): A list of users to filter the Job objects by.\n\n    Returns:\n        Response: A Django REST framework Response object containing the most frequent values\n        and the aggregated data.\n    \"\"\"\n    delta, basis = self.__parse_range(self.request)\n    filter_kwargs = {\"received_request_time__gte\": delta}\n    if users:\n        filter_kwargs[\"user__in\"] = users\n    if field_name == \"md5\":\n        filter_kwargs[\"is_sample\"] = True\n\n    most_frequent_values = (\n        Job.objects.filter(**filter_kwargs)\n        .exclude(**{f\"{field_name}__isnull\": True})\n        .exclude(**{f\"{field_name}__exact\": \"\"})\n        # excluding those because they could lead to SQL query errors\n        .exclude(\n            observable_classification__in=[\n                ObservableClassification.URL,\n                ObservableClassification.GENERIC,\n            ]\n        )\n        .annotate(count=Count(field_name))\n        .distinct()\n        .order_by(\"-count\")[:limit]\n        .values_list(field_name, flat=True)\n    )\n\n    logger.info(\n        f\"request: {field_name} found most_frequent_values: {most_frequent_values}\"\n    )\n\n    if len(most_frequent_values):\n        annotations = {\n            val: Count(field_name, filter=Q(**{field_name: val}))\n            for val in most_frequent_values\n        }\n        logger.debug(f\"request: {field_name} annotations: {annotations}\")\n        if group_by_date:\n            aggregation = (\n                Job.objects.filter(**filter_kwargs)\n                .annotate(date=Trunc(\"received_request_time\", basis))\n                .values(\"date\")\n                .annotate(**annotations)\n            )\n        else:\n            aggregation = Job.objects.filter(**filter_kwargs).aggregate(\n                **annotations\n            )\n    else:\n        aggregation = {}\n\n    return Response(\n        {\n            \"values\": most_frequent_values,\n            \"aggregation\": aggregation,\n        }\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.__aggregation_response_static","title":"<code>__aggregation_response_static(annotations, users=None)</code>","text":"<p>Generate a static aggregation of Job objects filtered by a time range.</p> <p>This method applies the provided annotations to aggregate Job objects within the specified time range. Optionally, it filters the results by the given list of users.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>Annotations to apply for the aggregation.</p> required <code>users</code> <code>list</code> <p>A list of users to filter the Job objects by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>A Django REST framework Response object containing the aggregated data.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def __aggregation_response_static(self, annotations: dict, users=None) -&gt; Response:\n    \"\"\"\n    Generate a static aggregation of Job objects filtered by a time range.\n\n    This method applies the provided annotations to aggregate Job objects\n    within the specified time range. Optionally, it filters the results by\n    the given list of users.\n\n    Args:\n        annotations (dict): Annotations to apply for the aggregation.\n        users (list, optional): A list of users to filter the Job objects by.\n\n    Returns:\n        Response: A Django REST framework Response object containing the aggregated data.\n    \"\"\"\n    delta, basis = self.__parse_range(self.request)\n    filter_kwargs = {\"received_request_time__gte\": delta}\n    if users:\n        filter_kwargs[\"user__in\"] = users\n    qs = (\n        Job.objects.filter(**filter_kwargs)\n        .annotate(date=Trunc(\"received_request_time\", basis))\n        .values(\"date\")\n        .annotate(**annotations)\n    )\n    return Response(qs)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.__parse_range","title":"<code>__parse_range(request)</code>  <code>staticmethod</code>","text":"<p>Parse the time range from the request query parameters.</p> <p>This method attempts to extract the 'range' query parameter from the request. If the parameter is not provided, it defaults to '7d' (7 days).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The HTTP request object containing query parameters.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the parsed time delta and the basis for date truncation.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@staticmethod\ndef __parse_range(request):\n    \"\"\"\n    Parse the time range from the request query parameters.\n\n    This method attempts to extract the 'range' query parameter from the\n    request. If the parameter is not provided, it defaults to '7d' (7 days).\n\n    Args:\n        request: The HTTP request object containing query parameters.\n\n    Returns:\n        tuple: A tuple containing the parsed time delta and the basis for date truncation.\n    \"\"\"\n    try:\n        range_str = request.GET[\"range\"]\n    except KeyError:\n        # default\n        range_str = \"7d\"\n\n    return parse_humanized_range(range_str)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.aggregate_file_mimetype","title":"<code>aggregate_file_mimetype(request)</code>","text":"<p>Aggregate jobs by file MIME type.</p> <p>Returns: - Aggregated count of jobs for each MIME type.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/file_mimetype\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_file_mimetype(self, request):\n    \"\"\"\n    Aggregate jobs by file MIME type.\n\n    Returns:\n    - Aggregated count of jobs for each MIME type.\n    \"\"\"\n    return self.__aggregation_response_dynamic(\n        \"file_mimetype\", users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.aggregate_md5","title":"<code>aggregate_md5(request)</code>","text":"<p>Aggregate jobs by MD5 hash.</p> <p>Returns: - Aggregated count of jobs for each MD5 hash.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/md5\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_md5(self, request):\n    \"\"\"\n    Aggregate jobs by MD5 hash.\n\n    Returns:\n    - Aggregated count of jobs for each MD5 hash.\n    \"\"\"\n    # this is for file\n    return self.__aggregation_response_dynamic(\n        \"md5\", False, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.aggregate_observable_classification","title":"<code>aggregate_observable_classification(request)</code>","text":"<p>Aggregate jobs by observable classification.</p> <p>Returns: - Aggregated count of jobs for each observable classification.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/observable_classification\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_observable_classification(self, request):\n    \"\"\"\n    Aggregate jobs by observable classification.\n\n    Returns:\n    - Aggregated count of jobs for each observable classification.\n    \"\"\"\n    annotations = {\n        oc.lower(): Count(\n            \"observable_classification\", filter=Q(observable_classification=oc)\n        )\n        for oc in ObservableTypes.values\n    }\n    return self.__aggregation_response_static(\n        annotations, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.aggregate_observable_name","title":"<code>aggregate_observable_name(request)</code>","text":"<p>Aggregate jobs by observable name.</p> <p>Returns: - Aggregated count of jobs for each observable name.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/observable_name\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_observable_name(self, request):\n    \"\"\"\n    Aggregate jobs by observable name.\n\n    Returns:\n    - Aggregated count of jobs for each observable name.\n    \"\"\"\n    return self.__aggregation_response_dynamic(\n        \"observable_name\", False, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.aggregate_status","title":"<code>aggregate_status(request)</code>","text":"<p>Aggregate jobs by their status.</p> <p>Returns: - Aggregated count of jobs for each status.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/status\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_status(self, request):\n    \"\"\"\n    Aggregate jobs by their status.\n\n    Returns:\n    - Aggregated count of jobs for each status.\n    \"\"\"\n    annotations = {\n        key.lower(): Count(\"status\", filter=Q(status=key))\n        for key in Job.Status.values\n    }\n    return self.__aggregation_response_static(\n        annotations, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.aggregate_type","title":"<code>aggregate_type(request)</code>","text":"<p>Aggregate jobs by type (file or observable).</p> <p>Returns: - Aggregated count of jobs for each type.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/type\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_type(self, request):\n    \"\"\"\n    Aggregate jobs by type (file or observable).\n\n    Returns:\n    - Aggregated count of jobs for each type.\n    \"\"\"\n    annotations = {\n        \"file\": Count(\"is_sample\", filter=Q(is_sample=True)),\n        \"observable\": Count(\"is_sample\", filter=Q(is_sample=False)),\n    }\n    return self.__aggregation_response_static(\n        annotations, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.download_sample","title":"<code>download_sample(request, pk=None)</code>","text":"<p>Download a sample associated with a job.</p> <p>If the job does not have a sample, raises a validation error.</p> <p>Returns: - The file associated with the job as an attachment.</p> <p>:param url: pk (job_id) :returns: bytes</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Download file/sample associated with a job\",\n    request=None,\n    responses={200: OpenApiTypes.BINARY, 400: None},\n)\n@action(detail=True, methods=[\"get\"])\ndef download_sample(self, request, pk=None):\n    \"\"\"\n    Download a sample associated with a job.\n\n    If the job does not have a sample, raises a validation error.\n\n    Returns:\n    - The file associated with the job as an attachment.\n\n    :param url: pk (job_id)\n    :returns: bytes\n    \"\"\"\n    # get job object\n    job = self.get_object()\n\n    # make sure it is a sample\n    if not job.is_sample:\n        raise ValidationError(\n            {\"detail\": \"Requested job does not have a sample associated with it.\"}\n        )\n    return FileResponse(\n        job.file,\n        filename=job.file_name,\n        content_type=job.file_mimetype,\n        as_attachment=True,\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.get_org_members","title":"<code>get_org_members(request)</code>  <code>staticmethod</code>","text":"<p>Retrieve members of the organization associated with the authenticated user.</p> <p>If the 'org' query parameter is set to 'true', this method returns all users who are members of the authenticated user's organization.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The HTTP request object containing user information and query parameters.</p> required <p>Returns:</p> Type Description <p>list or None: A list of users who are members of the user's organization</p> <p>if the 'org' query parameter is 'true', otherwise None.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@staticmethod\ndef get_org_members(request):\n    \"\"\"\n    Retrieve members of the organization associated with the authenticated user.\n\n    If the 'org' query parameter is set to 'true', this method returns all\n    users who are members of the authenticated user's organization.\n\n    Args:\n        request: The HTTP request object containing user information and query parameters.\n\n    Returns:\n        list or None: A list of users who are members of the user's organization\n        if the 'org' query parameter is 'true', otherwise None.\n    \"\"\"\n    user = request.user\n    org_param = request.GET.get(\"org\", \"\").lower() == \"true\"\n    users_of_organization = None\n    if org_param:\n        organization = user.membership.organization\n        users_of_organization = [\n            membership.user for membership in organization.members.all()\n        ]\n    return users_of_organization\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.get_permissions","title":"<code>get_permissions()</code>","text":"<p>Customizes permissions based on the action being performed.</p> <ul> <li>For <code>destroy</code> and <code>kill</code> actions, adds <code>IsObjectUserOrSameOrgPermission</code> to ensure that only   the job owner or anyone in the same organization can perform these actions.</li> </ul> <p>Returns: - List of applicable permissions.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_permissions(self):\n    \"\"\"\n    Customizes permissions based on the action being performed.\n\n    - For `destroy` and `kill` actions, adds `IsObjectUserOrSameOrgPermission` to ensure that only\n      the job owner or anyone in the same organization can perform these actions.\n\n    Returns:\n    - List of applicable permissions.\n    \"\"\"\n    permissions = super().get_permissions()\n    if self.action in [\"destroy\", \"kill\"]:\n        permissions.append(IsObjectUserOrSameOrgPermission())\n    return permissions\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Filters the queryset to include only jobs visible to the authenticated user, ordered by request time.</p> <p>Logs the request parameters and returns the filtered queryset.</p> <p>Returns: - Filtered queryset of jobs.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Filters the queryset to include only jobs visible to the authenticated user, ordered by request time.\n\n    Logs the request parameters and returns the filtered queryset.\n\n    Returns:\n    - Filtered queryset of jobs.\n    \"\"\"\n    user = self.request.user\n    logger.info(\n        f\"user: {user} request the jobs with params: {self.request.query_params}\"\n    )\n    return Job.objects.visible_for_user(user).order_by(\"-received_request_time\")\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.kill","title":"<code>kill(request, pk=None)</code>","text":"<p>Kill a running job by closing celery tasks and marking the job as killed.</p> <p>If the job is not running, raises a validation error.</p> <p>Returns: - No content (204) if the job is successfully killed.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Kill running job by closing celery tasks and marking as killed\",\n    request=None,\n    responses={\n        204: None,\n    },\n)\n@action(detail=True, methods=[\"patch\"])\ndef kill(self, request, pk=None):\n    \"\"\"\n    Kill a running job by closing celery tasks and marking the job as killed.\n\n    If the job is not running, raises a validation error.\n\n    Returns:\n    - No content (204) if the job is successfully killed.\n    \"\"\"\n    # get job object or raise 404\n    job = self.get_object()\n\n    # check if job running\n    if job.status in Job.Status.final_statuses():\n        raise ValidationError({\"detail\": \"Job is not running\"})\n    # close celery tasks and mark reports as killed\n    job.kill_if_ongoing()\n    return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.pivot","title":"<code>pivot(request, pk=None, pivot_config_pk=None)</code>","text":"<p>Perform a pivot operation from a job's reports based on a specified pivot configuration.</p> <p>Expects the following parameters: - <code>pivot_config_pk</code>: The primary key of the pivot configuration to use.</p> <p>Returns: - List of job IDs created as a result of the pivot.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(description=\"Pivot a job\")\n@action(\n    detail=True, methods=[\"post\"]\n)  # , url_path=\"pivot-(?P&lt;pivot_config_pk&gt;\\d+)\")\ndef pivot(self, request, pk=None, pivot_config_pk=None):\n    \"\"\"\n    Perform a pivot operation from a job's reports based on a specified pivot configuration.\n\n    Expects the following parameters:\n    - `pivot_config_pk`: The primary key of the pivot configuration to use.\n\n    Returns:\n    - List of job IDs created as a result of the pivot.\n    \"\"\"\n    starting_job = self.get_object()\n    try:\n        pivot_config: PivotConfig = PivotConfig.objects.get(pk=pivot_config_pk)\n    except PivotConfig.DoesNotExist:\n        raise ValidationError({\"detail\": \"Requested pivot config does not exist.\"})\n    else:\n        try:\n            pivots = pivot_config.pivot_job(starting_job.reports)\n        except KeyError:\n            msg = (\n                f\"Unable to retrieve value at {self.field}\"\n                f\" from job {starting_job.pk}\"\n            )\n            logger.error(msg)\n            raise ValidationError({\"detail\": msg})\n        except Exception as e:\n            logger.exception(e)\n            raise ValidationError(\n                {\"detail\": f\"Unable to start pivot from job {starting_job.pk}\"}\n            )\n        else:\n            return Response(\n                [pivot.ending_job.pk for pivot in pivots],\n                status=status.HTTP_201_CREATED,\n            )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.recent_scans","title":"<code>recent_scans(request)</code>","text":"<p>Retrieve recent jobs based on an MD5 hash, filtered by a maximum temporal distance.</p> <p>Expects the following parameters in the request data: - <code>md5</code>: The MD5 hash to filter jobs by. - <code>max_temporal_distance</code>: The maximum number of days to look back for recent jobs (default is 14 days).</p> <p>Returns: - List of recent jobs matching the MD5 hash.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(detail=False, methods=[\"post\"])\ndef recent_scans(self, request):\n    \"\"\"\n    Retrieve recent jobs based on an MD5 hash, filtered by a maximum temporal distance.\n\n    Expects the following parameters in the request data:\n    - `md5`: The MD5 hash to filter jobs by.\n    - `max_temporal_distance`: The maximum number of days to look back for recent jobs (default is 14 days).\n\n    Returns:\n    - List of recent jobs matching the MD5 hash.\n    \"\"\"\n    if \"md5\" not in request.data:\n        raise ValidationError({\"detail\": \"md5 is required\"})\n    max_temporal_distance = request.data.get(\"max_temporal_distance\", 14)\n    jobs = (\n        Job.objects.filter(md5=request.data[\"md5\"])\n        .visible_for_user(self.request.user)\n        .filter(\n            finished_analysis_time__gte=now()\n            - datetime.timedelta(days=max_temporal_distance)\n        )\n        .annotate_importance(request.user)\n        .order_by(\"-importance\", \"-finished_analysis_time\")\n    )\n    return Response(\n        JobRecentScanSerializer(jobs, many=True).data, status=status.HTTP_200_OK\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.recent_scans_user","title":"<code>recent_scans_user(request)</code>","text":"<p>Retrieve recent jobs for the authenticated user, filtered by sample status.</p> <p>Expects the following parameters in the request data: - <code>is_sample</code>: Whether to filter jobs by sample status (required). - <code>limit</code>: The maximum number of recent jobs to return (default is 5).</p> <p>Returns: - List of recent jobs for the user.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(detail=False, methods=[\"post\"])\ndef recent_scans_user(self, request):\n    \"\"\"\n    Retrieve recent jobs for the authenticated user, filtered by sample status.\n\n    Expects the following parameters in the request data:\n    - `is_sample`: Whether to filter jobs by sample status (required).\n    - `limit`: The maximum number of recent jobs to return (default is 5).\n\n    Returns:\n    - List of recent jobs for the user.\n    \"\"\"\n    limit = request.data.get(\"limit\", 5)\n    if \"is_sample\" not in request.data:\n        raise ValidationError({\"detail\": \"is_sample is required\"})\n    jobs = (\n        Job.objects.filter(user__pk=request.user.pk)\n        .filter(is_sample=request.data[\"is_sample\"])\n        .annotate_importance(request.user)\n        .order_by(\"-importance\", \"-finished_analysis_time\")[:limit]\n    )\n    return Response(\n        JobRecentScanSerializer(jobs, many=True).data, status=status.HTTP_200_OK\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.JobViewSet.retry","title":"<code>retry(request, pk=None)</code>","text":"<p>Retry a job if its status is in a final state.</p> <p>If the job is currently running, raises a validation error.</p> <p>Returns: - No content (204) if the job is successfully retried.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(detail=True, methods=[\"patch\"])\ndef retry(self, request, pk=None):\n    \"\"\"\n    Retry a job if its status is in a final state.\n\n    If the job is currently running, raises a validation error.\n\n    Returns:\n    - No content (204) if the job is successfully retried.\n    \"\"\"\n    job = self.get_object()\n    if job.status not in Job.Status.final_statuses():\n        raise ValidationError({\"detail\": \"Job is running\"})\n    job.retry()\n    return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#tagviewset","title":"<code>TagViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>A viewset that provides CRUD (Create, Read, Update, Delete) operations for the <code>Tag</code> model.</p> <p>This viewset leverages Django REST framework's <code>ModelViewSet</code> to handle requests for the <code>Tag</code> model. It includes the default implementations for <code>list</code>, <code>retrieve</code>, <code>create</code>, <code>update</code>, <code>partial_update</code>, and <code>destroy</code> actions.</p> <p>Attributes:</p> Name Type Description <code>queryset</code> <code>QuerySet</code> <p>The queryset that retrieves all Tag objects from the database.</p> <code>serializer_class</code> <code>Serializer</code> <p>The serializer class used to convert Tag model instances to JSON and vice versa.</p> <code>pagination_class</code> <p>Pagination is disabled for this viewset.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    REST endpoint to perform CRUD operations on ``Tag`` model.\n    Requires authentication.\n    \"\"\"\n)\nclass TagViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    A viewset that provides CRUD (Create, Read, Update, Delete) operations\n    for the ``Tag`` model.\n\n    This viewset leverages Django REST framework's `ModelViewSet` to handle\n    requests for the `Tag` model. It includes the default implementations\n    for `list`, `retrieve`, `create`, `update`, `partial_update`, and `destroy` actions.\n\n    Attributes:\n        queryset (QuerySet): The queryset that retrieves all Tag objects from the database.\n        serializer_class (Serializer): The serializer class used to convert Tag model instances to JSON and vice versa.\n        pagination_class: Pagination is disabled for this viewset.\n    \"\"\"\n\n    queryset = Tag.objects.all()\n    serializer_class = TagSerializer\n    pagination_class = None\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#modelwithownershipviewset","title":"<code>ModelWithOwnershipViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>A viewset that enforces ownership-based access control for models.</p> <p>This class extends the functionality of <code>ModelViewSet</code> to restrict access to objects based on ownership. It modifies the queryset for the <code>list</code> action to only include objects visible to the requesting user, and adds custom permission checks for <code>destroy</code> and <code>update</code> actions.</p> <p>Methods:</p> Name Description <code>get_queryset</code> <p>Returns the queryset of the model, filtered for visibility             to the requesting user during the <code>list</code> action.</p> <code>get_permissions</code> <p>Returns the permissions required for the current action,                with additional checks for ownership during <code>destroy</code>                and <code>update</code> actions. Raises <code>PermissionDenied</code> for <code>PUT</code> requests.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>class ModelWithOwnershipViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    A viewset that enforces ownership-based access control for models.\n\n    This class extends the functionality of `ModelViewSet` to restrict access to\n    objects based on ownership. It modifies the queryset for the `list` action\n    to only include objects visible to the requesting user, and adds custom\n    permission checks for `destroy` and `update` actions.\n\n    Methods:\n        get_queryset(): Returns the queryset of the model, filtered for visibility\n                        to the requesting user during the `list` action.\n        get_permissions(): Returns the permissions required for the current action,\n                           with additional checks for ownership during `destroy`\n                           and `update` actions. Raises `PermissionDenied` for `PUT` requests.\n    \"\"\"\n\n    def get_queryset(self):\n        \"\"\"\n        Retrieves the queryset for the viewset, modifying it for the `list` action\n        to only include objects visible to the requesting user.\n\n        Returns:\n            QuerySet: The queryset of the model, possibly filtered for visibility.\n        \"\"\"\n        qs = super().get_queryset()\n        if self.action == \"list\":\n            return qs.visible_for_user(self.request.user)\n        return qs\n\n    def get_permissions(self):\n        \"\"\"\n        Retrieves the permissions required for the current action.\n\n        For the `destroy` and `update` actions, additional checks are performed to\n        ensure that only object owners or admins can perform these actions. Raises\n        a `PermissionDenied` exception for `PUT` requests.\n\n        Returns:\n            list: A list of permission instances.\n        \"\"\"\n        permissions = super().get_permissions()\n        if self.action in [\"destroy\", \"update\"]:\n            if self.request.method == \"PUT\":\n                raise PermissionDenied()\n            # code quality checker marks this as error, but it works correctly\n            permissions.append(\n                (  # skipcq: PYL-E1102\n                    IsObjectAdminPermission | IsObjectOwnerPermission\n                )()\n            )\n\n        return permissions\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.ModelWithOwnershipViewSet.get_permissions","title":"<code>get_permissions()</code>","text":"<p>Retrieves the permissions required for the current action.</p> <p>For the <code>destroy</code> and <code>update</code> actions, additional checks are performed to ensure that only object owners or admins can perform these actions. Raises a <code>PermissionDenied</code> exception for <code>PUT</code> requests.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of permission instances.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_permissions(self):\n    \"\"\"\n    Retrieves the permissions required for the current action.\n\n    For the `destroy` and `update` actions, additional checks are performed to\n    ensure that only object owners or admins can perform these actions. Raises\n    a `PermissionDenied` exception for `PUT` requests.\n\n    Returns:\n        list: A list of permission instances.\n    \"\"\"\n    permissions = super().get_permissions()\n    if self.action in [\"destroy\", \"update\"]:\n        if self.request.method == \"PUT\":\n            raise PermissionDenied()\n        # code quality checker marks this as error, but it works correctly\n        permissions.append(\n            (  # skipcq: PYL-E1102\n                IsObjectAdminPermission | IsObjectOwnerPermission\n            )()\n        )\n\n    return permissions\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.ModelWithOwnershipViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Retrieves the queryset for the viewset, modifying it for the <code>list</code> action to only include objects visible to the requesting user.</p> <p>Returns:</p> Name Type Description <code>QuerySet</code> <p>The queryset of the model, possibly filtered for visibility.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Retrieves the queryset for the viewset, modifying it for the `list` action\n    to only include objects visible to the requesting user.\n\n    Returns:\n        QuerySet: The queryset of the model, possibly filtered for visibility.\n    \"\"\"\n    qs = super().get_queryset()\n    if self.action == \"list\":\n        return qs.visible_for_user(self.request.user)\n    return qs\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#pluginconfigviewset","title":"<code>PluginConfigViewSet</code>","text":"<p>               Bases: <code>ModelWithOwnershipViewSet</code></p> <p>A viewset for managing <code>PluginConfig</code> objects with ownership-based access control.</p> <p>This viewset extends <code>ModelWithOwnershipViewSet</code> to handle <code>PluginConfig</code> objects, allowing users to list, retrieve, and delete configurations while ensuring that only authorized configurations are accessible. It customizes the queryset to exclude default values and orders the configurations by ID.</p> <p>Attributes:</p> Name Type Description <code>serializer_class</code> <code>class</code> <p>The serializer class used for <code>PluginConfig</code> objects.</p> <code>pagination_class</code> <code>class</code> <p>Specifies that pagination is not applied.</p> <code>queryset</code> <code>QuerySet</code> <p>The queryset for <code>PluginConfig</code> objects, initially set to all objects.</p> <p>Methods:</p> Name Description <code>get_queryset</code> <p>Returns the queryset for <code>PluginConfig</code> objects, excluding default values             (where the owner is <code>NULL</code>) and ordering the remaining objects by ID.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    REST endpoint to fetch list of PluginConfig or retrieve/delete a CustomConfig.\n    Requires authentication. Allows access to only authorized CustomConfigs.\n    \"\"\"\n)\nclass PluginConfigViewSet(ModelWithOwnershipViewSet):\n    \"\"\"\n    A viewset for managing `PluginConfig` objects with ownership-based access control.\n\n    This viewset extends `ModelWithOwnershipViewSet` to handle `PluginConfig` objects,\n    allowing users to list, retrieve, and delete configurations while ensuring that only\n    authorized configurations are accessible. It customizes the queryset to exclude default\n    values and orders the configurations by ID.\n\n    Attributes:\n        serializer_class (class): The serializer class used for `PluginConfig` objects.\n        pagination_class (class): Specifies that pagination is not applied.\n        queryset (QuerySet): The queryset for `PluginConfig` objects, initially set to all objects.\n\n    Methods:\n        get_queryset(): Returns the queryset for `PluginConfig` objects, excluding default values\n                        (where the owner is `NULL`) and ordering the remaining objects by ID.\n    \"\"\"\n\n    serializer_class = PluginConfigSerializer\n    pagination_class = None\n    queryset = PluginConfig.objects.all()\n\n    def get_queryset(self):\n        \"\"\"\n        Retrieves the queryset for `PluginConfig` objects, excluding those with default values\n        (where the owner is `NULL`) and ordering the remaining objects by ID.\n\n        Returns:\n            QuerySet: The filtered and ordered queryset of `PluginConfig` objects.\n        \"\"\"\n        # the .exclude is to remove the default values\n        return super().get_queryset().exclude(owner__isnull=True).order_by(\"id\")\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PluginConfigViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Retrieves the queryset for <code>PluginConfig</code> objects, excluding those with default values (where the owner is <code>NULL</code>) and ordering the remaining objects by ID.</p> <p>Returns:</p> Name Type Description <code>QuerySet</code> <p>The filtered and ordered queryset of <code>PluginConfig</code> objects.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Retrieves the queryset for `PluginConfig` objects, excluding those with default values\n    (where the owner is `NULL`) and ordering the remaining objects by ID.\n\n    Returns:\n        QuerySet: The filtered and ordered queryset of `PluginConfig` objects.\n    \"\"\"\n    # the .exclude is to remove the default values\n    return super().get_queryset().exclude(owner__isnull=True).order_by(\"id\")\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#pythonreportactionviewset","title":"<code>PythonReportActionViewSet</code>","text":"<p>               Bases: <code>GenericViewSet</code></p> <p>A base view set for handling actions related to plugin reports.</p> <p>This view set provides methods for killing and retrying plugin reports, and requires users to have appropriate permissions based on the <code>IsObjectUserOrSameOrgPermission</code>.</p> <p>Attributes:</p> Name Type Description <code>permission_classes</code> <code>list</code> <p>List of permission classes to apply.</p> <p>Methods: get_queryset: Returns the queryset of reports based on the model class. get_object: Retrieves a specific report object by job_id and report_id. perform_kill: Kills a running plugin by terminating its Celery task and marking it as killed. perform_retry: Retries a failed or killed plugin run. kill: Handles the endpoint to kill a specific report. retry: Handles the endpoint to retry a specific report.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>class PythonReportActionViewSet(viewsets.GenericViewSet, metaclass=ABCMeta):\n    \"\"\"\n    A base view set for handling actions related to plugin reports.\n\n    This view set provides methods for killing and retrying plugin reports,\n    and requires users to have appropriate permissions based on the\n    `IsObjectUserOrSameOrgPermission`.\n\n    Attributes:\n        permission_classes (list): List of permission classes to apply.\n\n    Methods:\n    get_queryset: Returns the queryset of reports based on the model class.\n    get_object: Retrieves a specific report object by job_id and report_id.\n    perform_kill: Kills a running plugin by terminating its Celery task and marking it as killed.\n    perform_retry: Retries a failed or killed plugin run.\n    kill: Handles the endpoint to kill a specific report.\n    retry: Handles the endpoint to retry a specific report.\n\n    \"\"\"\n\n    permission_classes = [\n        IsObjectUserOrSameOrgPermission,\n    ]\n\n    @classmethod\n    @property\n    @abstractmethod\n    def report_model(cls):\n        \"\"\"\n        Abstract property that should return the model class for the report.\n\n        Subclasses must implement this property to specify the model\n        class for the reports being handled by this view set.\n\n        Returns:\n            Type[AbstractReport]: The model class for the report.\n\n        Raises:\n            NotImplementedError: If not overridden by a subclass.\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_queryset(self):\n        \"\"\"\n        Returns the queryset of reports based on the model class.\n\n        Filters the queryset to return all instances of the report model.\n\n        Returns:\n            QuerySet: A queryset of all report instances.\n        \"\"\"\n        return self.report_model.objects.all()\n\n    def get_object(self, job_id: int, report_id: int) -&gt; AbstractReport:\n        \"\"\"\n        Retrieves a specific report object by job_id and report_id.\n\n        Overrides the drf's default `get_object` method to fetch a report object\n        based on job_id and report_id, and checks the permissions for the object.\n\n        Args:\n            job_id (int): The ID of the job associated with the report.\n            report_id (int): The ID of the report.\n\n        Returns:\n            AbstractReport: The report object.\n\n        Raises:\n            NotFound: If the report does not exist.\n        \"\"\"\n        try:\n            obj = self.report_model.objects.get(\n                job_id=job_id,\n                pk=report_id,\n            )\n        except self.report_model.DoesNotExist:\n            raise NotFound()\n        else:\n            self.check_object_permissions(self.request, obj)\n            return obj\n\n    @staticmethod\n    def perform_kill(report: AbstractReport):\n        \"\"\"\n        Kills a running plugin by terminating its Celery task and marking it as killed.\n\n        This method is a callback for performing additional actions after a\n        kill operation, including updating the report status and cleaning up\n        the associated job.\n\n        Args:\n            report (AbstractReport): The report to be killed.\n        \"\"\"\n        # kill celery task\n        celery_app.control.revoke(report.task_id, terminate=True)\n        # update report\n        report.status = AbstractReport.Status.KILLED\n        report.save(update_fields=[\"status\"])\n        # clean up job\n\n        job = Job.objects.get(pk=report.job.pk)\n        job.set_final_status()\n        JobConsumer.serialize_and_send_job(job)\n\n    @staticmethod\n    def perform_retry(report: AbstractReport):\n        \"\"\"\n        Retries a failed or killed plugin run.\n\n        This method clears the errors and re-runs the plugin with the same arguments.\n        It fetches the appropriate task signature and schedules the job again.\n\n        Args:\n            report (AbstractReport): The report to be retried.\n\n        Raises:\n            RuntimeError: If unable to find a valid task signature for the report.\n        \"\"\"\n        report.errors.clear()\n        report.save(update_fields=[\"errors\"])\n        try:\n            signature = next(\n                report.config.__class__.objects.filter(pk=report.config.pk)\n                .annotate_runnable(report.job.user)\n                .get_signatures(\n                    report.job,\n                )\n            )\n        except StopIteration:\n            raise RuntimeError(f\"Unable to find signature for report {report.pk}\")\n        runner = signature | tasks.job_set_final_status.signature(\n            args=[report.job.id],\n            kwargs={},\n            queue=report.config.queue,\n            immutable=True,\n            MessageGroupId=str(uuid.uuid4()),\n            priority=report.job.priority,\n        )\n        runner()\n\n    @add_docs(\n        description=\"Kill running plugin by closing celery task and marking as killed\",\n        request=None,\n        responses={\n            204: None,\n        },\n    )\n    @action(detail=False, methods=[\"patch\"])\n    def kill(self, request, job_id, report_id):\n        \"\"\"\n        Kills a specific report by terminating its Celery task and marking it as killed.\n\n        This endpoint handles the patch request to kill a report if its status is\n        running or pending.\n\n        Args:\n            request (HttpRequest): The request object containing the HTTP PATCH request.\n            job_id (int): The ID of the job associated with the report.\n            report_id (int): The ID of the report.\n\n        Returns:\n            Response: HTTP 204 No Content if successful.\n\n        Raises:\n            ValidationError: If the report is not in a valid state for killing.\n        \"\"\"\n        logger.info(\n            f\"kill request from user {request.user}\"\n            f\" for job_id {job_id}, pk {report_id}\"\n        )\n        # get report object or raise 404\n        report = self.get_object(job_id, report_id)\n        if report.status not in [\n            AbstractReport.Status.RUNNING,\n            AbstractReport.Status.PENDING,\n        ]:\n            raise ValidationError({\"detail\": \"Plugin is not running or pending\"})\n\n        self.perform_kill(report)\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n    @add_docs(\n        description=\"Retry a plugin run if it failed/was killed previously\",\n        request=None,\n        responses={\n            204: None,\n        },\n    )\n    @action(detail=False, methods=[\"patch\"])\n    def retry(self, request, job_id, report_id):\n        \"\"\"\n        Retries a failed or killed plugin run.\n\n        This method clears the errors and re-runs the plugin with the same arguments.\n        It fetches the appropriate task signature and schedules the job again.\n\n        Args:\n            report (AbstractReport): The report to be retried.\n\n        Raises:\n            RuntimeError: If unable to find a valid task signature for the report.\n        \"\"\"\n        logger.info(\n            f\"retry request from user {request.user}\"\n            f\" for job_id {job_id}, report_id {report_id}\"\n        )\n        # get report object or raise 404\n        report = self.get_object(job_id, report_id)\n        if report.status not in [\n            AbstractReport.Status.FAILED,\n            AbstractReport.Status.KILLED,\n        ]:\n            raise ValidationError(\n                {\"detail\": \"Plugin status should be failed or killed\"}\n            )\n\n        # retry with the same arguments\n        try:\n            self.perform_retry(report)\n        except StopIteration:\n            logger.exception(f\"Unable to find signature for report {report.pk}\")\n            return Response(status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n        return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonReportActionViewSet.report_model","title":"<code>report_model</code>  <code>abstractmethod</code> <code>classmethod</code> <code>property</code>","text":"<p>Abstract property that should return the model class for the report.</p> <p>Subclasses must implement this property to specify the model class for the reports being handled by this view set.</p> <p>Returns:</p> Type Description <p>Type[AbstractReport]: The model class for the report.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not overridden by a subclass.</p>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonReportActionViewSet.get_object","title":"<code>get_object(job_id, report_id)</code>","text":"<p>Retrieves a specific report object by job_id and report_id.</p> <p>Overrides the drf's default <code>get_object</code> method to fetch a report object based on job_id and report_id, and checks the permissions for the object.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>The ID of the job associated with the report.</p> required <code>report_id</code> <code>int</code> <p>The ID of the report.</p> required <p>Returns:</p> Name Type Description <code>AbstractReport</code> <code>AbstractReport</code> <p>The report object.</p> <p>Raises:</p> Type Description <code>NotFound</code> <p>If the report does not exist.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_object(self, job_id: int, report_id: int) -&gt; AbstractReport:\n    \"\"\"\n    Retrieves a specific report object by job_id and report_id.\n\n    Overrides the drf's default `get_object` method to fetch a report object\n    based on job_id and report_id, and checks the permissions for the object.\n\n    Args:\n        job_id (int): The ID of the job associated with the report.\n        report_id (int): The ID of the report.\n\n    Returns:\n        AbstractReport: The report object.\n\n    Raises:\n        NotFound: If the report does not exist.\n    \"\"\"\n    try:\n        obj = self.report_model.objects.get(\n            job_id=job_id,\n            pk=report_id,\n        )\n    except self.report_model.DoesNotExist:\n        raise NotFound()\n    else:\n        self.check_object_permissions(self.request, obj)\n        return obj\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonReportActionViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Returns the queryset of reports based on the model class.</p> <p>Filters the queryset to return all instances of the report model.</p> <p>Returns:</p> Name Type Description <code>QuerySet</code> <p>A queryset of all report instances.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Returns the queryset of reports based on the model class.\n\n    Filters the queryset to return all instances of the report model.\n\n    Returns:\n        QuerySet: A queryset of all report instances.\n    \"\"\"\n    return self.report_model.objects.all()\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonReportActionViewSet.kill","title":"<code>kill(request, job_id, report_id)</code>","text":"<p>Kills a specific report by terminating its Celery task and marking it as killed.</p> <p>This endpoint handles the patch request to kill a report if its status is running or pending.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>HttpRequest</code> <p>The request object containing the HTTP PATCH request.</p> required <code>job_id</code> <code>int</code> <p>The ID of the job associated with the report.</p> required <code>report_id</code> <code>int</code> <p>The ID of the report.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP 204 No Content if successful.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the report is not in a valid state for killing.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Kill running plugin by closing celery task and marking as killed\",\n    request=None,\n    responses={\n        204: None,\n    },\n)\n@action(detail=False, methods=[\"patch\"])\ndef kill(self, request, job_id, report_id):\n    \"\"\"\n    Kills a specific report by terminating its Celery task and marking it as killed.\n\n    This endpoint handles the patch request to kill a report if its status is\n    running or pending.\n\n    Args:\n        request (HttpRequest): The request object containing the HTTP PATCH request.\n        job_id (int): The ID of the job associated with the report.\n        report_id (int): The ID of the report.\n\n    Returns:\n        Response: HTTP 204 No Content if successful.\n\n    Raises:\n        ValidationError: If the report is not in a valid state for killing.\n    \"\"\"\n    logger.info(\n        f\"kill request from user {request.user}\"\n        f\" for job_id {job_id}, pk {report_id}\"\n    )\n    # get report object or raise 404\n    report = self.get_object(job_id, report_id)\n    if report.status not in [\n        AbstractReport.Status.RUNNING,\n        AbstractReport.Status.PENDING,\n    ]:\n        raise ValidationError({\"detail\": \"Plugin is not running or pending\"})\n\n    self.perform_kill(report)\n    return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonReportActionViewSet.perform_kill","title":"<code>perform_kill(report)</code>  <code>staticmethod</code>","text":"<p>Kills a running plugin by terminating its Celery task and marking it as killed.</p> <p>This method is a callback for performing additional actions after a kill operation, including updating the report status and cleaning up the associated job.</p> <p>Parameters:</p> Name Type Description Default <code>report</code> <code>AbstractReport</code> <p>The report to be killed.</p> required Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@staticmethod\ndef perform_kill(report: AbstractReport):\n    \"\"\"\n    Kills a running plugin by terminating its Celery task and marking it as killed.\n\n    This method is a callback for performing additional actions after a\n    kill operation, including updating the report status and cleaning up\n    the associated job.\n\n    Args:\n        report (AbstractReport): The report to be killed.\n    \"\"\"\n    # kill celery task\n    celery_app.control.revoke(report.task_id, terminate=True)\n    # update report\n    report.status = AbstractReport.Status.KILLED\n    report.save(update_fields=[\"status\"])\n    # clean up job\n\n    job = Job.objects.get(pk=report.job.pk)\n    job.set_final_status()\n    JobConsumer.serialize_and_send_job(job)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonReportActionViewSet.perform_retry","title":"<code>perform_retry(report)</code>  <code>staticmethod</code>","text":"<p>Retries a failed or killed plugin run.</p> <p>This method clears the errors and re-runs the plugin with the same arguments. It fetches the appropriate task signature and schedules the job again.</p> <p>Parameters:</p> Name Type Description Default <code>report</code> <code>AbstractReport</code> <p>The report to be retried.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to find a valid task signature for the report.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@staticmethod\ndef perform_retry(report: AbstractReport):\n    \"\"\"\n    Retries a failed or killed plugin run.\n\n    This method clears the errors and re-runs the plugin with the same arguments.\n    It fetches the appropriate task signature and schedules the job again.\n\n    Args:\n        report (AbstractReport): The report to be retried.\n\n    Raises:\n        RuntimeError: If unable to find a valid task signature for the report.\n    \"\"\"\n    report.errors.clear()\n    report.save(update_fields=[\"errors\"])\n    try:\n        signature = next(\n            report.config.__class__.objects.filter(pk=report.config.pk)\n            .annotate_runnable(report.job.user)\n            .get_signatures(\n                report.job,\n            )\n        )\n    except StopIteration:\n        raise RuntimeError(f\"Unable to find signature for report {report.pk}\")\n    runner = signature | tasks.job_set_final_status.signature(\n        args=[report.job.id],\n        kwargs={},\n        queue=report.config.queue,\n        immutable=True,\n        MessageGroupId=str(uuid.uuid4()),\n        priority=report.job.priority,\n    )\n    runner()\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonReportActionViewSet.retry","title":"<code>retry(request, job_id, report_id)</code>","text":"<p>Retries a failed or killed plugin run.</p> <p>This method clears the errors and re-runs the plugin with the same arguments. It fetches the appropriate task signature and schedules the job again.</p> <p>Parameters:</p> Name Type Description Default <code>report</code> <code>AbstractReport</code> <p>The report to be retried.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to find a valid task signature for the report.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Retry a plugin run if it failed/was killed previously\",\n    request=None,\n    responses={\n        204: None,\n    },\n)\n@action(detail=False, methods=[\"patch\"])\ndef retry(self, request, job_id, report_id):\n    \"\"\"\n    Retries a failed or killed plugin run.\n\n    This method clears the errors and re-runs the plugin with the same arguments.\n    It fetches the appropriate task signature and schedules the job again.\n\n    Args:\n        report (AbstractReport): The report to be retried.\n\n    Raises:\n        RuntimeError: If unable to find a valid task signature for the report.\n    \"\"\"\n    logger.info(\n        f\"retry request from user {request.user}\"\n        f\" for job_id {job_id}, report_id {report_id}\"\n    )\n    # get report object or raise 404\n    report = self.get_object(job_id, report_id)\n    if report.status not in [\n        AbstractReport.Status.FAILED,\n        AbstractReport.Status.KILLED,\n    ]:\n        raise ValidationError(\n            {\"detail\": \"Plugin status should be failed or killed\"}\n        )\n\n    # retry with the same arguments\n    try:\n        self.perform_retry(report)\n    except StopIteration:\n        logger.exception(f\"Unable to find signature for report {report.pk}\")\n        return Response(status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n    return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#abstractconfigviewset","title":"<code>AbstractConfigViewSet</code>","text":"<p>               Bases: <code>PaginationMixin</code>, <code>ReadOnlyModelViewSet</code></p> <p>A base view set for handling plugin configuration actions.</p> <p>This view set provides methods for enabling and disabling plugins within an organization. It requires users to be authenticated and to have appropriate permissions.</p> <p>Attributes:</p> Name Type Description <code>permission_classes</code> <code>list</code> <p>List of permission classes to apply.</p> <code>ordering</code> <code>list</code> <p>Default ordering for the queryset.</p> <code>lookup_field</code> <code>str</code> <p>Field to look up in the URL.</p> <p>Methods:</p> Name Description <code>disable_in_org</code> <p>Disables the plugin for the organization of the authenticated user.</p> <code>enable_in_org</code> <p>Enables the plugin for the organization of the authenticated user.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>class AbstractConfigViewSet(\n    PaginationMixin, viewsets.ReadOnlyModelViewSet, metaclass=ABCMeta\n):\n    \"\"\"\n    A base view set for handling plugin configuration actions.\n\n    This view set provides methods for enabling and disabling plugins\n    within an organization. It requires users to be authenticated and\n    to have appropriate permissions.\n\n    Attributes:\n        permission_classes (list): List of permission classes to apply.\n        ordering (list): Default ordering for the queryset.\n        lookup_field (str): Field to look up in the URL.\n\n    Methods:\n        disable_in_org(request, name=None):\n            Disables the plugin for the organization of the authenticated user.\n        enable_in_org(request, name=None):\n            Enables the plugin for the organization of the authenticated user.\n    \"\"\"\n\n    permission_classes = [IsAuthenticated]\n    ordering = [\"name\"]\n    lookup_field = \"name\"\n\n    @add_docs(\n        description=\"Disable/Enable plugin for your organization\",\n        request=None,\n        responses={201: {}, 202: {}},\n    )\n    @action(\n        methods=[\"post\"],\n        detail=True,\n        url_path=\"organization\",\n    )\n    def disable_in_org(self, request, name=None):\n        \"\"\"\n        Disables the plugin for the organization of the authenticated user.\n\n        Only organization admins can disable the plugin. If the plugin is\n        already disabled, a validation error is raised.\n\n        Args:\n            request (Request): The HTTP request object.\n            name (str, optional): The name of the plugin. Defaults to None.\n\n        Returns:\n            Response: HTTP response indicating the success or failure of the operation.\n        \"\"\"\n        logger.info(f\"get disable_in_org from user {request.user}, name {name}\")\n        obj: AbstractConfig = self.get_object()\n        if request.user.has_membership():\n            if not request.user.membership.is_admin:\n                raise PermissionDenied()\n        else:\n            raise PermissionDenied()\n        organization = request.user.membership.organization\n        org_configuration = obj.get_or_create_org_configuration(organization)\n        if org_configuration.disabled:\n            raise ValidationError({\"detail\": f\"Plugin {obj.name} already disabled\"})\n        org_configuration.disable_manually(request.user)\n        return Response(status=status.HTTP_201_CREATED)\n\n    @disable_in_org.mapping.delete\n    def enable_in_org(self, request, name=None):\n        \"\"\"\n        Enables the plugin for the organization of the authenticated user.\n\n        Only organization admins can enable the plugin. If the plugin is\n        already enabled, a validation error is raised.\n\n        Args:\n            request (Request): The HTTP request object.\n            name (str, optional): The name of the plugin. Defaults to None.\n\n        Returns:\n            Response: HTTP response indicating the success or failure of the operation.\n        \"\"\"\n        logger.info(f\"get enable_in_org from user {request.user}, name {name}\")\n        obj: AbstractConfig = self.get_object()\n        if request.user.has_membership():\n            if not request.user.membership.is_admin:\n                raise PermissionDenied()\n        else:\n            raise PermissionDenied()\n        organization = request.user.membership.organization\n        org_configuration = obj.get_or_create_org_configuration(organization)\n        if not org_configuration.disabled:\n            raise ValidationError({\"detail\": f\"Plugin {obj.name} already enabled\"})\n        org_configuration.enable_manually(request.user)\n        return Response(status=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.AbstractConfigViewSet.disable_in_org","title":"<code>disable_in_org(request, name=None)</code>","text":"<p>Disables the plugin for the organization of the authenticated user.</p> <p>Only organization admins can disable the plugin. If the plugin is already disabled, a validation error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object.</p> required <code>name</code> <code>str</code> <p>The name of the plugin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP response indicating the success or failure of the operation.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Disable/Enable plugin for your organization\",\n    request=None,\n    responses={201: {}, 202: {}},\n)\n@action(\n    methods=[\"post\"],\n    detail=True,\n    url_path=\"organization\",\n)\ndef disable_in_org(self, request, name=None):\n    \"\"\"\n    Disables the plugin for the organization of the authenticated user.\n\n    Only organization admins can disable the plugin. If the plugin is\n    already disabled, a validation error is raised.\n\n    Args:\n        request (Request): The HTTP request object.\n        name (str, optional): The name of the plugin. Defaults to None.\n\n    Returns:\n        Response: HTTP response indicating the success or failure of the operation.\n    \"\"\"\n    logger.info(f\"get disable_in_org from user {request.user}, name {name}\")\n    obj: AbstractConfig = self.get_object()\n    if request.user.has_membership():\n        if not request.user.membership.is_admin:\n            raise PermissionDenied()\n    else:\n        raise PermissionDenied()\n    organization = request.user.membership.organization\n    org_configuration = obj.get_or_create_org_configuration(organization)\n    if org_configuration.disabled:\n        raise ValidationError({\"detail\": f\"Plugin {obj.name} already disabled\"})\n    org_configuration.disable_manually(request.user)\n    return Response(status=status.HTTP_201_CREATED)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.AbstractConfigViewSet.enable_in_org","title":"<code>enable_in_org(request, name=None)</code>","text":"<p>Enables the plugin for the organization of the authenticated user.</p> <p>Only organization admins can enable the plugin. If the plugin is already enabled, a validation error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object.</p> required <code>name</code> <code>str</code> <p>The name of the plugin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP response indicating the success or failure of the operation.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@disable_in_org.mapping.delete\ndef enable_in_org(self, request, name=None):\n    \"\"\"\n    Enables the plugin for the organization of the authenticated user.\n\n    Only organization admins can enable the plugin. If the plugin is\n    already enabled, a validation error is raised.\n\n    Args:\n        request (Request): The HTTP request object.\n        name (str, optional): The name of the plugin. Defaults to None.\n\n    Returns:\n        Response: HTTP response indicating the success or failure of the operation.\n    \"\"\"\n    logger.info(f\"get enable_in_org from user {request.user}, name {name}\")\n    obj: AbstractConfig = self.get_object()\n    if request.user.has_membership():\n        if not request.user.membership.is_admin:\n            raise PermissionDenied()\n    else:\n        raise PermissionDenied()\n    organization = request.user.membership.organization\n    org_configuration = obj.get_or_create_org_configuration(organization)\n    if not org_configuration.disabled:\n        raise ValidationError({\"detail\": f\"Plugin {obj.name} already enabled\"})\n    org_configuration.enable_manually(request.user)\n    return Response(status=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#pythonconfigviewset","title":"<code>PythonConfigViewSet</code>","text":"<p>               Bases: <code>AbstractConfigViewSet</code></p> <p>A view set for handling actions related to Python plugin configurations.</p> <p>This view set provides methods to perform health checks and pull updates for Python-based plugins. It inherits from <code>AbstractConfigViewSet</code> and requires users to be authenticated.</p> <p>Attributes:</p> Name Type Description <code>serializer_class</code> <code>class</code> <p>Serializer class for the view set.</p> <p>Methods:</p> Name Description <code>health_check</code> <p>Checks if the server instance associated with the plugin is up.</p> <code>pull</code> <p>Pulls updates for the plugin.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>class PythonConfigViewSet(AbstractConfigViewSet):\n    \"\"\"\n    A view set for handling actions related to Python plugin configurations.\n\n    This view set provides methods to perform health checks and pull updates\n    for Python-based plugins. It inherits from `AbstractConfigViewSet` and\n    requires users to be authenticated.\n\n    Attributes:\n        serializer_class (class): Serializer class for the view set.\n\n    Methods:\n        health_check(request, name=None):\n            Checks if the server instance associated with the plugin is up.\n        pull(request, name=None):\n            Pulls updates for the plugin.\n    \"\"\"\n\n    serializer_class = PythonConfigSerializer\n\n    def get_queryset(self):\n        \"\"\"\n        Returns a queryset of all PythonConfig instances with related\n        python_module parameters pre-fetched.\n\n        Returns:\n            QuerySet: A queryset of PythonConfig instances.\n        \"\"\"\n        return self.serializer_class.Meta.model.objects.all().prefetch_related(\n            \"python_module__parameters\"\n        )\n\n    @add_docs(\n        description=\"Health Check: \"\n        \"if server instance associated with plugin is up or not\",\n        request=None,\n        responses={\n            200: inline_serializer(\n                name=\"PluginHealthCheckSuccessResponse\",\n                fields={\n                    \"status\": rfs.BooleanField(allow_null=True),\n                },\n            ),\n        },\n    )\n    @action(\n        methods=[\"get\"],\n        detail=True,\n        url_path=\"health_check\",\n    )\n    def health_check(self, request, name=None):\n        \"\"\"\n        Checks the health of the server instance associated with the plugin.\n\n        This method attempts to check if the plugin's server instance is\n        up and running. It uses the `health_check` method of the plugin's\n        Python class.\n\n        Args:\n            request (Request): The HTTP request object.\n            name (str, optional): The name of the plugin. Defaults to None.\n\n        Returns:\n            Response: HTTP response with the health status of the plugin.\n\n        Raises:\n            ValidationError: If no health check is implemented or if an\n                             unexpected exception occurs.\n        \"\"\"\n        logger.info(f\"get healthcheck from user {request.user}, name {name}\")\n        config: PythonConfig = self.get_object()\n        python_obj = config.python_module.python_class(config)\n        try:\n            health_status = python_obj.health_check(request.user)\n        except NotImplementedError as e:\n            logger.info(f\"NotImplementedError {e}, user {request.user}, name {name}\")\n            raise ValidationError({\"detail\": \"No healthcheck implemented\"})\n        except Exception as e:\n            logger.exception(e)\n            raise ValidationError(\n                {\"detail\": \"Unexpected exception raised. Check the code.\"}\n            )\n        else:\n            return Response(data={\"status\": health_status}, status=status.HTTP_200_OK)\n\n    @action(\n        methods=[\"post\"],\n        detail=True,\n        url_path=\"pull\",\n    )\n    def pull(self, request, name=None):\n        \"\"\"\n        Pulls updates for the plugin.\n\n        This method attempts to pull updates for the plugin by calling\n        the `update` method of the plugin's Python class. It also handles\n        any exceptions that occur during this process.\n\n        Args:\n            request (Request): The HTTP request object.\n            name (str, optional): The name of the plugin. Defaults to None.\n\n        Returns:\n            Response: HTTP response with the update status of the plugin.\n\n        Raises:\n            ValidationError: If the update is not implemented or if an\n                             unexpected exception occurs.\n        \"\"\"\n        logger.info(f\"post pull from user {request.user}, name {name}\")\n        obj: PythonConfig = self.get_object()\n        python_obj = obj.python_module.python_class(obj)\n        try:\n            update_status = python_obj.update()\n        except NotImplementedError as e:\n            raise ValidationError({\"detail\": str(e)})\n        except Exception as e:\n            logger.exception(e)\n            raise ValidationError(\n                {\"detail\": \"Unexpected exception raised. Check the code.\"}\n            )\n        else:\n            if update_status is None:\n                raise ValidationError(\n                    {\"detail\": \"This Plugin has no Update implemented\"}\n                )\n            return Response(data={\"status\": update_status}, status=status.HTTP_200_OK)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonConfigViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Returns a queryset of all PythonConfig instances with related python_module parameters pre-fetched.</p> <p>Returns:</p> Name Type Description <code>QuerySet</code> <p>A queryset of PythonConfig instances.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Returns a queryset of all PythonConfig instances with related\n    python_module parameters pre-fetched.\n\n    Returns:\n        QuerySet: A queryset of PythonConfig instances.\n    \"\"\"\n    return self.serializer_class.Meta.model.objects.all().prefetch_related(\n        \"python_module__parameters\"\n    )\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonConfigViewSet.health_check","title":"<code>health_check(request, name=None)</code>","text":"<p>Checks the health of the server instance associated with the plugin.</p> <p>This method attempts to check if the plugin's server instance is up and running. It uses the <code>health_check</code> method of the plugin's Python class.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object.</p> required <code>name</code> <code>str</code> <p>The name of the plugin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP response with the health status of the plugin.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If no health check is implemented or if an              unexpected exception occurs.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Health Check: \"\n    \"if server instance associated with plugin is up or not\",\n    request=None,\n    responses={\n        200: inline_serializer(\n            name=\"PluginHealthCheckSuccessResponse\",\n            fields={\n                \"status\": rfs.BooleanField(allow_null=True),\n            },\n        ),\n    },\n)\n@action(\n    methods=[\"get\"],\n    detail=True,\n    url_path=\"health_check\",\n)\ndef health_check(self, request, name=None):\n    \"\"\"\n    Checks the health of the server instance associated with the plugin.\n\n    This method attempts to check if the plugin's server instance is\n    up and running. It uses the `health_check` method of the plugin's\n    Python class.\n\n    Args:\n        request (Request): The HTTP request object.\n        name (str, optional): The name of the plugin. Defaults to None.\n\n    Returns:\n        Response: HTTP response with the health status of the plugin.\n\n    Raises:\n        ValidationError: If no health check is implemented or if an\n                         unexpected exception occurs.\n    \"\"\"\n    logger.info(f\"get healthcheck from user {request.user}, name {name}\")\n    config: PythonConfig = self.get_object()\n    python_obj = config.python_module.python_class(config)\n    try:\n        health_status = python_obj.health_check(request.user)\n    except NotImplementedError as e:\n        logger.info(f\"NotImplementedError {e}, user {request.user}, name {name}\")\n        raise ValidationError({\"detail\": \"No healthcheck implemented\"})\n    except Exception as e:\n        logger.exception(e)\n        raise ValidationError(\n            {\"detail\": \"Unexpected exception raised. Check the code.\"}\n        )\n    else:\n        return Response(data={\"status\": health_status}, status=status.HTTP_200_OK)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#docs.Submodules.ThreatMatrix.api_app.views.PythonConfigViewSet.pull","title":"<code>pull(request, name=None)</code>","text":"<p>Pulls updates for the plugin.</p> <p>This method attempts to pull updates for the plugin by calling the <code>update</code> method of the plugin's Python class. It also handles any exceptions that occur during this process.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object.</p> required <code>name</code> <code>str</code> <p>The name of the plugin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP response with the update status of the plugin.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the update is not implemented or if an              unexpected exception occurs.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@action(\n    methods=[\"post\"],\n    detail=True,\n    url_path=\"pull\",\n)\ndef pull(self, request, name=None):\n    \"\"\"\n    Pulls updates for the plugin.\n\n    This method attempts to pull updates for the plugin by calling\n    the `update` method of the plugin's Python class. It also handles\n    any exceptions that occur during this process.\n\n    Args:\n        request (Request): The HTTP request object.\n        name (str, optional): The name of the plugin. Defaults to None.\n\n    Returns:\n        Response: HTTP response with the update status of the plugin.\n\n    Raises:\n        ValidationError: If the update is not implemented or if an\n                         unexpected exception occurs.\n    \"\"\"\n    logger.info(f\"post pull from user {request.user}, name {name}\")\n    obj: PythonConfig = self.get_object()\n    python_obj = obj.python_module.python_class(obj)\n    try:\n        update_status = python_obj.update()\n    except NotImplementedError as e:\n        raise ValidationError({\"detail\": str(e)})\n    except Exception as e:\n        logger.exception(e)\n        raise ValidationError(\n            {\"detail\": \"Unexpected exception raised. Check the code.\"}\n        )\n    else:\n        if update_status is None:\n            raise ValidationError(\n                {\"detail\": \"This Plugin has no Update implemented\"}\n            )\n        return Response(data={\"status\": update_status}, status=status.HTTP_200_OK)\n</code></pre>"},{"location":"ThreatMatrix/api_docs/#functions","title":"Functions","text":""},{"location":"ThreatMatrix/api_docs/#plugin_state_viewer","title":"<code>plugin_state_viewer</code>","text":"<p>View to retrieve the state of plugin configurations for the requesting user\u2019s organization.</p> <p>This endpoint is accessible only to users with an active membership in an organization. It returns a JSON response with the state of each plugin configuration, specifically indicating whether each plugin is disabled.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>HttpRequest</code> <p>The request object containing the HTTP GET request.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response with the state of each plugin configuration,       indicating whether it is disabled or not.</p> <p>Raises:</p> Type Description <code>PermissionDenied</code> <p>If the requesting user does not belong to any organization.</p> Source code in <code>docs/Submodules/ThreatMatrix/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"This endpoint allows organization owners\n    and members to view plugin state.\"\"\",\n    responses={\n        200: inline_serializer(\n            name=\"PluginStateViewerResponseSerializer\",\n            fields={\n                \"data\": rfs.JSONField(),\n            },\n        ),\n    },\n)\n@api_view([\"GET\"])\ndef plugin_state_viewer(request):\n    \"\"\"\n    View to retrieve the state of plugin configurations for the requesting user\u2019s organization.\n\n    This endpoint is accessible only to users with an active membership in an organization.\n    It returns a JSON response with the state of each plugin configuration, specifically\n    indicating whether each plugin is disabled.\n\n    Args:\n        request (HttpRequest): The request object containing the HTTP GET request.\n\n    Returns:\n        Response: A JSON response with the state of each plugin configuration,\n                  indicating whether it is disabled or not.\n\n    Raises:\n        PermissionDenied: If the requesting user does not belong to any organization.\n    \"\"\"\n    if not request.user.has_membership():\n        raise PermissionDenied()\n\n    result = {\"data\": {}}\n    for opc in OrganizationPluginConfiguration.objects.filter(disabled=True):\n        result[\"data\"][opc.config.name] = {\n            \"disabled\": True,\n        }\n    return Response(result)\n</code></pre>"},{"location":"ThreatMatrix/contribute/","title":"Contribute","text":"<p>There are a lot of different ways you could choose to contribute to the ThreatMatrix Project:</p> <ul> <li>main repository: ThreatMatrix</li> </ul> <ul> <li>official Python client: pythreatmatrix.</li> </ul> <ul> <li>official GO client: go-threatmatrix.</li> </ul> <ul> <li>official ThreatMatrix Site: khulnasoft.github.io.</li> </ul> <ul> <li>honeypots project: Greedybear</li> </ul>"},{"location":"ThreatMatrix/contribute/#rules","title":"Rules","text":"<p>Intel Owl welcomes contributors from anywhere and from any kind of education or skill level. We strive to create a community of developers that is welcoming, friendly and right.</p> <p>For this reason it is important to follow some easy rules based on a simple but important concept: Respect.</p> <ul> <li>Before asking any questions regarding how the project works, please read through all the documentation and install the project on your own local machine to try it and understand how it basically works. This is a form of respect to the maintainers.</li> </ul> <ul> <li>DO NOT contact the maintainers with direct messages unless it is an urgent request. We don't have much time and cannot just answer to all the questions that we receive like \"Guide me please! Help me understand how the project work\". There is plenty of documentation and a lot of people in the community that can help you and would benefit from your questions. Share your problems and your knowledge. Please ask your questions in open channels (Github and Slack). This is a form of respect to the maintainers and to the community.</li> </ul> <ul> <li>Before starting to work on an issue, you need to get the approval of one of the maintainers. Therefore please ask to be assigned to an issue. If you do not that but you still raise a PR for that issue, your PR can be rejected. This is a form of respect for both the maintainers and the other contributors who could have already started to work on the same problem.</li> </ul> <ul> <li>When you ask to be assigned to an issue, it means that you are ready to work on it. When you get assigned, take the lock and then you disappear, you are not respecting the maintainers and the other contributors who could be able to work on that. So, after having been assigned, you have a week of time to deliver your first draft PR. After that time has passed without any notice, you will be unassigned.</li> </ul> <ul> <li>Once you started working on an issue and you have some work to share and discuss with us, please raise a draft PR early with incomplete changes. This way you can continue working on the same and we can track your progress and actively review and help. This is a form of respect to you and to the maintainers.</li> </ul> <ul> <li>When creating a PR, please read through the sections that you will find in the PR template and compile it appropriately. If you do not, your PR can be rejected. This is a form of respect to the maintainers.</li> </ul>"},{"location":"ThreatMatrix/contribute/#code-style","title":"Code Style","text":"<p>Keeping to a consistent code style throughout the project makes it easier to contribute and collaborate. We make use of <code>psf/black</code> and isort for code formatting and <code>flake8</code> for style guides.</p>"},{"location":"ThreatMatrix/contribute/#how-to-start-setup-project-and-development-instance","title":"How to start (Setup project and development instance)","text":"<p>This guide assumes that you have already performed the steps required to install the project. If not, please do it (Installation Guide).</p> <p>Create a personal fork of the project on Github. Then, please create a new branch based on the develop branch that contains the most recent changes. This is mandatory.</p> <p><code>git checkout -b myfeature develop</code></p> <p>Then we strongly suggest to configure pre-commit to force linters on every commits you perform</p> <pre><code># From the project directory\npython3 -m venv venv\nsource venv/bin/activate\n# from the project base directory\npip install pre-commit\npre-commit install\n\n# create .env file for controlling repo_downloader.sh\n# (to speed up image builds during development: it avoid downloading some repos)\ncp docker/.env.start.test.template docker/.env.start.test\n\n# set STAGE env variable to \"local\"\nsed -i \"s/STAGE=\\\"production\\\"/STAGE=\\\"local\\\"/g\" docker/env_file_app\n</code></pre>"},{"location":"ThreatMatrix/contribute/#backend","title":"Backend","text":"<p>Now, you can execute ThreatMatrix in development mode by selecting the mode <code>test</code> while launching the startup script:</p> <pre><code>./start test up\n</code></pre> <p>Every time you perform a change, you should perform an operation to reflect the changes into the application:</p> <ul> <li>if you changed the python requirements, restart the application and re-build the images. This is the slowest process. You can always choose this way but it would waste a lot of time.</li> </ul> <pre><code>./start test down &amp;&amp; ./start test up -- --build\n</code></pre> <ul> <li>if you changed either analyzers, connectors, playbooks or anything that is executed asynchronously by the \"celery\" containers, you just need to restart the application because we leverage Docker bind volumes that will reflect the changes to the containers. This saves the time of the build</li> </ul> <pre><code>./start test down &amp;&amp; ./start test up\n</code></pre> <ul> <li>if you made changes to either the API or anything that is executed only by the application server, changes will be instantly reflected and you don't need to do anything. This is thanks to the Django Development server that is executed instead of <code>uwsgi</code> while using the <code>test</code> mode</li> </ul>"},{"location":"ThreatMatrix/contribute/#note-about-documentation","title":"NOTE about documentation:","text":"<p>If you made any changes to an existing model/serializer/view, please run the following command to generate a new version of the API schema and docs:</p> <pre><code>docker exec -it threatmatrix_uwsgi python manage.py spectacular --file docs/source/schema.yml &amp;&amp; make html\n</code></pre>"},{"location":"ThreatMatrix/contribute/#frontend","title":"Frontend","text":"<p>To start the frontend in \"develop\" mode, you can execute the startup npm script within the folder <code>frontend</code>:</p> <pre><code>cd frontend/\n# Install\nnpm i\n# Start\nDANGEROUSLY_DISABLE_HOST_CHECK=true npm start\n# See https://create-react-app.dev/docs/proxying-api-requests-in-development/#invalid-host-header-errors-after-configuring-proxy for why we use that flag in development mode\n</code></pre> <p>Most of the time you would need to test the changes you made together with the backend. In that case, you would need to run the backend locally too:</p> <pre><code>./start prod up\n</code></pre> <p>Note</p> <ul> <li>Running <code>prod</code> would be faster because you would leverage the official images and you won't need to build the backend locally. In case you would need to test backend changes too at the same time, please use <code>test</code> and refer to the previous section of the documentation.</li> <li>This works thanks to the directive <code>proxy</code> in the <code>frontend/package.json</code> configuration</li> <li>It may happen that the backend build does not work due to incompatibility between the frontend version you are testing with the current complete ThreatMatrix version you are running. In those cases, considering that you don't need to build the frontend together with the backend because you are already testing it separately, we suggest to remove the first build step (the frontend part) from the main Dockerfile temporarily and build ThreatMatrix with only the backend. In this way there won't be conflict issues.</li> </ul>"},{"location":"ThreatMatrix/contribute/#certego-ui","title":"Certego-UI","text":"<p>The ThreatMatrix Frontend is tightly linked to the <code>certego-ui</code> library. Most of the React components are imported from there. Because of this, it may happen that, during development, you would need to work on that library too. To install the <code>certego-ui</code> library, please take a look to npm link and remember to start certego-ui without installing peer dependencies (to avoid conflicts with ThreatMatrix dependencies):</p> <pre><code>git clone https://github.com/certego/certego-ui.git\n# change directory to the folder where you have the cloned the library\ncd certego-ui/\n# install, without peer deps (to use packages of ThreatMatrix)\nnpm i --legacy-peer-deps\n# create link to the project (this will globally install this package)\nsudo npm link\n# compile the library\nnpm start\n</code></pre> <p>Then, open another command line tab, create a link in the <code>frontend</code> to the <code>certego-ui</code> and re-install and re-start the frontend application (see previous section):</p> <pre><code>cd frontend/\nnpm link @certego/certego-ui\n</code></pre> <p>This trick will allow you to see reflected every changes you make in the <code>certego-ui</code> directly in the running <code>frontend</code> application.</p>"},{"location":"ThreatMatrix/contribute/#example-application","title":"Example application","text":"<p>The <code>certego-ui</code> application comes with an example project that showcases the components that you can re-use and import to other projects, like ThreatMatrix:</p> <pre><code># To have the Example application working correctly, be sure to have installed `certego-ui` *without* the `--legacy-peer-deps` option and having it started in another command line\ncd certego-ui/\nnpm i\nnpm start\n# go to another tab\ncd certego-ui/example/\nnpm i\nnpm start\n</code></pre>"},{"location":"ThreatMatrix/contribute/#how-to-add-a-new-plugin","title":"How to add a new Plugin","text":"<p>ThreatMatrix was designed to ease the addition of new plugins. With a simple python script you can integrate your own engine or integrate an external service in a short time.</p> <p>There are two possible cases:</p> <ol> <li>You are creating an entirely new Plugin, meaning that you actually wrote python code</li> <li>You are creating a new Configuration for some code that already exists.</li> </ol> <p>If you are doing the step number <code>2</code>, you can skip this paragraph.</p> <p>First, you need to create the python code that will be actually executed. You can easily take other plugins as example to write this. Then, you have to create a <code>Python Module</code> model. You can do this in the <code>Django Admin</code> page: You have to specify which type of Plugin you wrote, and its python module. Again, you can use as an example an already configured <code>Python Module</code>.</p> <p>Some <code>Python Module</code> requires to update some part of its code in a schedule way: for example <code>Yara</code> requires to update the rule repositories, <code>QuarkEngine</code> to update its database and so on. If the <code>Python Module</code> that you define need this type of behaviour, you have to configure two things:</p> <ul> <li>In the python code, you have to override a method called <code>update</code> and put the updating logic (see other plugins for examples) there.</li> <li>In the model class, you have to add the <code>update_schedule</code> (crontab syntax) that define when the update should be executed.</li> </ul> <p>Some <code>Python Module</code> requires further check to see if the service provider is able to answer requests; for example if you have done too many requests, or the website is currently down for maintenance and so on. If the <code>Python Module</code> that you define need this type of behaviour, you have to configure two things:</p> <ul> <li>In the python code, you can override a method called <code>health_check</code> and put there the custom health check logic. As default, plugins will try to make an HTTP <code>HEAD</code> request to the configured url (the Plugin must have a <code>url</code> attribute).</li> <li>In the model class, you have to add the <code>health_check_schedule</code> (crontab syntax) that define when the health check should be executed.</li> </ul> <p>Press <code>Save and continue editing</code> to, at the moment, manually ad the <code>Parameters</code> that the python code requires (the class attributes that you needed):</p> <ol> <li>*name: Name of the parameter that will be dynamically added to the python class (if is a secret, in the python code a <code>_</code> wil be prepended to the name)</li> <li>*type: data type, <code>string</code>, <code>list</code>, <code>dict</code>, <code>integer</code>, <code>boolean</code>, <code>float</code></li> <li>*description</li> <li>*required: <code>true</code> or <code>false</code>, meaning that a value is necessary to allow the run of the analyzer</li> <li>*is_secret: <code>true</code> or <code>false</code></li> </ol> <p>At this point, you can follow the specific guide for each plugin</p>"},{"location":"ThreatMatrix/contribute/#how-to-add-a-new-analyzer","title":"How to add a new Analyzer","text":"<p>You may want to look at a few existing examples to start to build a new one, such as:</p> <ul> <li>shodan.py, if you are creating an observable analyzer</li> <li>malpedia_scan.py, if you are creating a file analyzer</li> <li>peframe.py, if you are creating a docker based analyzer</li> <li>Please note: If the new analyzer that you are adding is free for the user to use, please add it in the <code>FREE_TO_USE_ANALYZERS</code> playbook. To do this you have to make a migration file; you can use <code>0026_add_mmdb_analyzer_free_to_use</code> as a template.</li> </ul> <p>After having written the new python module, you have to remember to:</p> <ol> <li>Put the module in the <code>file_analyzers</code> or <code>observable_analyzers</code> directory based on what it can analyze</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new analyzer. This is a trick to have tests in the same class of its analyzer.</li> <li>Create the configuration inside django admin in <code>Analyzers_manager/AnalyzerConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Routing key: celery queue that will be used</li> <li>*Soft_time_limit: maximum time for the task execution</li> <li>*Type: <code>observable</code> or <code>file</code></li> <li>*Docker based: if the analyzer run through a docker instance</li> <li>*Maximum tlp: maximum tlp to allow the run on the connector</li> <li>~Observable supported: required if <code>type</code> is <code>observable</code></li> <li>~Supported filetypes: required if <code>type</code> is <code>file</code> and <code>not supported filetypes</code> is empty</li> <li>Run hash: if the analyzer supports hash as inputs</li> <li>~Run hash type: required if <code>run hash</code> is <code>True</code></li> <li>~Not supported filetypes: required if <code>type</code> is <code>file</code> and <code>supported filetypes</code> is empty</li>"},{"location":"ThreatMatrix/contribute/#integrating-a-docker-based-analyzer","title":"Integrating a docker based analyzer","text":"<p>If the analyzer you wish to integrate doesn't exist as a public API or python package, it should be integrated with its own docker image which can be queried from the main Django app.</p> <ul> <li>It should follow the same design principle as the other such existing integrations, unless there's very good reason not to.</li> <li>The dockerfile should be placed at <code>./integrations/&lt;analyzer_name&gt;/Dockerfile</code>.</li> <li>Two docker-compose files <code>compose.yml</code> for production and <code>compose-tests.yml</code> for testing should be placed under <code>./integrations/&lt;analyzer_name&gt;</code>.</li> <li>If your docker-image uses any environment variables, add them in the <code>docker/env_file_integrations_template</code>.</li> <li>Rest of the steps remain same as given under \"How to add a new analyzer\".</li> </ul>"},{"location":"ThreatMatrix/contribute/#how-to-add-a-new-connector","title":"How to add a new Connector","text":"<p>You may want to look at a few existing examples to start to build a new one:</p> <ul> <li>misp.py</li> <li>opencti.py</li> </ul> <p>After having written the new python module, you have to remember to:</p> <ol> <li>Put the module in the <code>connectors</code> directory</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new connector. This is a trick to have tests in the same class of its connector.</li> <li>Create the configuration inside django admin in <code>Connectors_manager/ConnectorConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Routing key: celery queue that will be used</li> <li>*Soft_time_limit: maximum time for the task execution</li> <li>*Maximum tlp: maximum tlp to allow the run on the connector</li> <li>*Run on failure: if the connector should be run even if the job fails</li>"},{"location":"ThreatMatrix/contribute/#how-to-add-a-new-ingestor","title":"How to add a new Ingestor","text":"<ol> <li>Put the module in the <code>ingestors</code> directory</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new ingestor. This is a trick to have tests in the same class of its ingestor.</li> <li>Create the configuration inside django admin in <code>Ingestors_manager/IngestorConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Routing key: celery queue that will be used</li> <li>*Soft_time_limit: maximum time for the task execution</li> <li>*Playbook to Execute: Playbook that will be executed on every IOC retrieved</li> <li>*Schedule: Crontab object that describes the schedule of the ingestor. You are able to create a new clicking the <code>plus</code> symbol.</li>"},{"location":"ThreatMatrix/contribute/#how-to-add-a-new-pivot","title":"How to add a new Pivot","text":"<ol> <li>Put the module in the <code>pivots</code> directory</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new pivot. This is a trick to have tests in the same class of its pivot.</li> <li>Create the configuration inside django admin in <code>Pivots_manager/PivotConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Routing key: celery queue that will be used</li> <li>*Soft_time_limit: maximum time for the task execution</li> <li>*Playbook to Execute: Playbook that will be executed in the Job generated by the Pivot</li> <p>Most of the times you don't need to create a new Pivot Module. There are already some base modules that can be extended. The most important ones are the following 2:</p> <ul> <li>1.<code>AnyCompare</code>: use this module if you want to create a custom Pivot from a specific value extracted from the results of the analyzers/connectors. How? you should populate the parameter <code>field_to_compare</code> with the dotted path to the field you would like to extract the value from.</li> <li>2.<code>SelfAnalyzable</code>: use this module if you want to create a custom Pivot that would analyze again the same observable/file.</li> </ul>"},{"location":"ThreatMatrix/contribute/#how-to-add-a-new-visualizer","title":"How to add a new Visualizer","text":""},{"location":"ThreatMatrix/contribute/#configuration","title":"Configuration","text":"<ol> <li>Put the module in the <code>visualizers</code> directory</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new visualizer. This is a trick to have tests in the same class of its visualizer.</li> <li>Create the configuration inside django admin in <code>Visualizers_manager/VisualizerConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Config:     1. *Queue: celery queue that will be used     2. *Soft_time_limit: maximum time for the task execution</li> <li>*Playbook: Playbook that must have run to execute the visualizer</li>"},{"location":"ThreatMatrix/contribute/#python-class","title":"Python class","text":"<p>The visualizers' python code could be not immediate, so a small digression on how it works is necessary. Visualizers have as goal to create a data structure inside the <code>Report</code> that the frontend is able to parse and correctly visualize on the page. To do so, some utility classes have been made:</p> Class Description Visual representation/example VisualizablePage A single page of the final report, made of different levels. Each page added is represented as a new tab in frontend. VisualizableLevel        Each level corresponds to a line in the final frontend visualizations. Every level is made of a        VisualizableHorizontalList.       The dimension of the level can be customized with the size parameter (1 is the biggest, 6 is the smallest).       VisualizableHorizontalList An horizontal list of visualizable elements. In the example there is an horizontal list of vertical lists. VisualizableVerticalList A vertical list made of a name, a title, and the list of elements. VisualizableTable A table of visualizable elements. In the example there is a table of base and vertical lists. VisualizableBool The representation of a boolean value. It can be enabled or disabled with colors. VisualizableTitle The representation of a tuple, composed of a title and a value. VisualizableBase The representation of a base string. Can have a link attached to it and even an icon. The background color can be changed. The title above is composed by two `VisualizableBase` <p>Inside a <code>Visualizer</code> you can retrieve the reports of the analyzers and connectors  that have been specified inside configuration of the Visualizer itself using <code>.analyzer_reports()</code> and <code>.connector_reports()</code>. At this point, you can compose these values as you wish wrapping them with the <code>Visualizable</code> classes mentioned before.</p> <p>The best way to create a visualizer is to define several methods, one for each <code>Visualizable</code> you want to show in the UI, in your new visualizer and decore them with <code>visualizable_error_handler_with_params</code>. This decorator handles exceptions: in case there is a bug during the generation of a Visualizable element, it will be show an error instead of this component and all the other Visualizable are safe and will render correctly. Be careful using it because is a function returning a decorator! This means you need to use a syntax like this:</p> <pre><code>@visualizable_error_handler_with_params(error_name=\"custom visualizable\", error_size=VisualizableSize.S_2)\ndef custom_visualizable(self):\n   ...\n</code></pre> <p>instead of the syntax of other decorators that doesn't need the function call.</p> <p>You may want to look at a few existing examples to start to build a new one:</p> <ul> <li>dns.py</li> <li>yara.py</li> </ul>"},{"location":"ThreatMatrix/contribute/#how-to-share-your-plugin-with-the-community","title":"How to share your plugin with the community","text":"<p>To allow other people to use your configuration, that is now stored in your local database, you have to export it and create a data migration</p> <ol> <li>You can use the django management command <code>dumpplugin</code> to automatically create the migration file for your new analyzer (you will find it under <code>api_app/YOUR_PLUGIN_manager/migrations</code>). The script will create the following models:     1. PythonModule     2. AnalyzerConfig     3. Parameter     4. PluginConfig</li> <li>Example: <code>docker exec -ti threatmatrix_uwsgi python3 manage.py dumpplugin AnalyzerConfig &lt;new_analyzer_name&gt;</code></li> </ol> <p>Add the new analyzer in the lists in the docs: Usage. Also, if the analyzer provides additional optional configuration, add the available options here: Advanced-Usage</p> <p>In the Pull Request remember to provide some real world examples (screenshots and raw JSON results) of some successful executions of the analyzer to let us understand how it would work.</p>"},{"location":"ThreatMatrix/contribute/#how-to-add-a-new-playbook","title":"How to add a new Playbook","text":"<ol> <li>Create the configuration inside django admin in <code>Playbooks_manager/PlaybookConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Description: description of the configuration</li> <li>*Type: list of types that are supported by the playbook</li> <li>*Analyzers: List of analyzers that will be run</li> <li>*Connectors: List of connectors that will be run</li> </ol> </li> </ol>"},{"location":"ThreatMatrix/contribute/#how-to-share-your-playbook-with-the-community","title":"How to share your playbook with the community","text":"<p>To allow other people to use your configuration, that is now stored in your local database, you have to export it and create a data migration You can use the django management command <code>dumpplugin</code> to automatically create the migration file for your new analyzer (you will find it under <code>api_app/playbook_manager/migrations</code>).</p> <p>Example: <code>docker exec -ti threatmatrix_uwsgi python3 manage.py dumpplugin PlaybookConfig &lt;new_analyzer_name&gt;</code></p>"},{"location":"ThreatMatrix/contribute/#how-to-modify-a-plugin","title":"How to modify a plugin","text":"<p>If the changes that you have to make should stay local, you can just change the configuration inside the <code>Django admin</code> page.</p> <p>But if, instead, you want your changes to be usable by every ThreatMatrix user, you have to create a new migration.</p> <p>To do so, you can use the following snippets as an example:</p> <ol> <li>You have to create a new migration file</li> <li>Add as dependency the previous last migration of the package</li> <li>You have to create a forward and a reverse function</li> <li>You have to make the proper changes of the configuration inside these functions (change parameters, secrets, or even delete the configuration)<ol> <li>If changes are made, you have to validate the instance calling <code>.full_clean()</code> and then you can save the instance with <code>.save()</code></li> </ol> </li> </ol>"},{"location":"ThreatMatrix/contribute/#example-how-to-add-a-new-parameter-in-the-configuration-with-a-default-value","title":"Example: how to add a new parameter in the configuration with a default value","text":"<pre><code>def migrate(apps, schema_editor):\n   PythonModule = apps.get_model(\"api_app\", \"PythonModule\")\n   Parameter = apps.get_model(\"api_app\", \"Parameter\")\n   PluginConfig = apps.get_model(\"api_app\", \"PluginConfig\")\n   pm = PythonModule.objects.get(module=\"test.Test\", base_path=\"api_app.connectors_manager.connectors\")\n   p = Parameter(name=\"mynewfield\", type=\"str\", description=\"Test field\", is_secret=False, required=True, python_module=pm)\n   p.full_clean()\n   p.save()\n   for connector in pm.connectorconfigs.all():\n    pc = PluginConfig(value=\"test\", connector_config=connector, python_module=pm, for_organization=False, owner=None, parameter=p)\n    pc.full_clean()\n    pc.save()\n</code></pre>"},{"location":"ThreatMatrix/contribute/#example-how-to-add-a-new-secret-in-the-configuration","title":"Example: how to add a new secret in the configuration","text":"<pre><code>def migrate(apps, schema_editor):\n   PythonModule = apps.get_model(\"api_app\", \"PythonModule\")\n   Parameter = apps.get_model(\"api_app\", \"Parameter\")\n   pm = PythonModule.objects.get(module=\"test.Test\", base_path=\"api_app.connectors_manager.connectors\")\n   p = Parameter(name=\"mynewsecret\", type=\"str\", description=\"Test field\", is_secret=True, required=True, python_module=pm)\n   p.full_clean()\n   p.save()\n</code></pre>"},{"location":"ThreatMatrix/contribute/#example-how-to-delete-a-parameter","title":"Example: how to delete a parameter","text":"<pre><code>def migrate(apps, schema_editor):\n   PythonModule = apps.get_model(\"api_app\", \"PythonModule\")\n   Parameter = apps.get_model(\"api_app\", \"Parameter\")\n   pm = PythonModule.objects.get(module=\"test.Test\", base_path=\"api_app.connectors_manager.connectors\")\n   Parameter.objects.get(name=\"myoldfield\", python_module=pm).delete()\n</code></pre>"},{"location":"ThreatMatrix/contribute/#example-how-to-change-the-default-value-of-a-parameter","title":"Example: how to change the default value of a parameter","text":"<pre><code>def migrate(apps, schema_editor):\n   PythonModule = apps.get_model(\"api_app\", \"PythonModule\")\n   Parameter = apps.get_model(\"api_app\", \"Parameter\")\n   PluginConfig = apps.get_model(\"api_app\", \"PluginConfig\")\n   pm = PythonModule.objects.get(module=\"test.Test\", base_path=\"api_app.connectors_manager.connectors\")\n   p = Parameter.objects.get(name=\"myfield\", python_module=pm)\n   PluginConfig.objects.filter(parameter=p, python_module=pm, for_organization=False, owner=None ).update(value=\"newvalue\")\n</code></pre>"},{"location":"ThreatMatrix/contribute/#modifying-functionalities-of-the-certego-packages","title":"Modifying functionalities of the Certego packages","text":"<p>Since v4, ThreatMatrix leverages some packages from Certego:</p> <ul> <li>certego-saas that integrates some common reusable Django applications and tools that can be used for generic services.</li> <li>certego-ui that contains reusable React components for the UI.</li> </ul> <p>If you need to modify the behavior or add feature to those packages, please follow the same rules for ThreatMatrix and request a Pull Request there. The same maintainers of ThreatMatrix will answer to you.</p> <p>Follow these guides to understand how to start to contribute to them while developing for ThreatMatrix:</p> <ul> <li>certego-saas: create a fork, commit your changes in your local repo, then change the commit hash to the last one you made in the requirements file. Ultimately re-build the project</li> <li>certego-ui: Frontend doc</li> </ul>"},{"location":"ThreatMatrix/contribute/#how-to-test-the-application","title":"How to test the application","text":"<p>ThreatMatrix makes use of the django testing framework and the <code>unittest</code> library for unit testing of the API endpoints and End-to-End testing of the analyzers and connectors.</p>"},{"location":"ThreatMatrix/contribute/#configuration_1","title":"Configuration","text":"<ul> <li>In the encrypted folder <code>tests/test_files.zip</code> (password: \"threatmatrix\") there are some files that you can use for testing purposes.</li> </ul> <ul> <li> <p>With the following environment variables you can customize your tests:</p> <ul> <li><code>DISABLE_LOGGING_TEST</code> -&gt; disable logging to get a clear output</li> <li><code>MOCK_CONNECTIONS</code> -&gt; mock connections to external API to test the analyzers without a real connection or a valid API key</li> </ul> </li> </ul> <ul> <li>If you prefer to use custom inputs for tests, you can change the following environment variables in the environment file based on the data you would like to test:<ul> <li><code>TEST_MD5</code></li> <li><code>TEST_URL</code></li> <li><code>TEST_IP</code></li> <li><code>TEST_DOMAIN</code></li> </ul> </li> </ul>"},{"location":"ThreatMatrix/contribute/#setup-containers","title":"Setup containers","text":"<p>The point here is to launch the code in your environment and not the last official image in Docker Hub. For this, use the <code>test</code> or the <code>ci</code> option when launching the containers with the <code>./start</code> script.</p> <ul> <li>Use the <code>test</code> option to actually execute tests that simulate a real world environment without mocking connections.</li> <li>Use the <code>ci</code> option to execute tests in a CI environment where connections are mocked.</li> </ul> <pre><code>$ ./start test up\n$ # which corresponds to the command: docker-compose -f docker/default.yml -f docker/test.override.yml up\n</code></pre>"},{"location":"ThreatMatrix/contribute/#launch-tests","title":"Launch tests","text":"<p>Now that the containers are up, we can launch the test suite.</p>"},{"location":"ThreatMatrix/contribute/#backend_1","title":"Backend","text":""},{"location":"ThreatMatrix/contribute/#run-all-tests","title":"Run all tests","text":"<p>Examples:</p> <pre><code>$ docker exec threatmatrix_uwsgi python3 manage.py test\n</code></pre>"},{"location":"ThreatMatrix/contribute/#run-tests-for-a-particular-plugin","title":"Run tests for a particular plugin","text":"<p>To test a plugin in real environment, i.e. without mocked data, we suggest that you use the GUI of ThreatMatrix directly. Meaning that you have your plugin configured, you have selected a correct observable/file to analyze, and the final report shown in the GUI of ThreatMatrix is exactly what you wanted.</p>"},{"location":"ThreatMatrix/contribute/#run-tests-available-in-a-particular-file","title":"Run tests available in a particular file","text":"<p>Examples:</p> <pre><code>$ docker exec threatmatrix_uwsgi python3 manage.py test tests.api_app tests.test_crons # dotted paths\n</code></pre>"},{"location":"ThreatMatrix/contribute/#frontend_1","title":"Frontend","text":"<p>All the frontend tests must be run from the folder <code>frontend</code>. The tests can contain log messages, you can suppress then with the environment variable <code>SUPPRESS_JEST_LOG=True</code>.</p>"},{"location":"ThreatMatrix/contribute/#run-all-tests_1","title":"Run all tests","text":"<pre><code>npm test\n</code></pre>"},{"location":"ThreatMatrix/contribute/#run-a-specific-component-tests","title":"Run a specific component tests","text":"<pre><code>npm test -- -t &lt;componentPath&gt;\n// example\nnpm test tests/components/auth/Login.test.jsx\n</code></pre>"},{"location":"ThreatMatrix/contribute/#run-a-specific-test","title":"Run a specific test","text":"<pre><code>npm test -- -t '&lt;describeString&gt; &lt;testString&gt;'\n// example\nnpm test -- -t \"Login component User login\"\n</code></pre>"},{"location":"ThreatMatrix/contribute/#create-a-pull-request","title":"Create a pull request","text":""},{"location":"ThreatMatrix/contribute/#remember","title":"Remember!!!","text":"<p>Please create pull requests only for the branch develop. That code will be pushed to master only on a new release.</p> <p>Also remember to pull the most recent changes available in the develop branch before submitting your PR. If your PR has merge conflicts caused by this behavior, it won't be accepted.</p>"},{"location":"ThreatMatrix/contribute/#install-testing-requirements","title":"Install testing requirements","text":"<p>Run <code>pip install -r requirements/test-requirements.txt</code> to install the requirements to validate your code.</p>"},{"location":"ThreatMatrix/contribute/#pass-linting-and-tests","title":"Pass linting and tests","text":"<ol> <li>Run <code>psf/black</code> to lint the files automatically, then <code>flake8</code> to check and <code>isort</code>:</li> </ol> <p>(if you installed <code>pre-commit</code> this is performed automatically at every commit)</p> <pre><code>$ black . --exclude \"migrations|venv\"\n$ flake8 . --show-source --statistics\n$ isort . --profile black --filter-files --skip venv\n</code></pre> <p>if flake8 shows any errors, fix them.</p> <ol> <li>Run the build and start the app using the docker-compose test file. In this way, you would launch the code in your environment and not the last official image in Docker Hub:</li> </ol> <pre><code>$ ./start ci build\n$ ./start ci up\n</code></pre> <ol> <li>Here, we simulate the GitHub CI tests locally by running the following 3 tests:</li> </ol> <pre><code>$ docker exec -ti threatmatrix_uwsgi unzip -P threatmatrix tests/test_files.zip -d test_files\n$ docker exec -ti threatmatrix_uwsgi python manage.py test tests\n</code></pre> <p>Note: ThreatMatrix has dynamic testing suite. This means that no explicit analyzers/connector tests are required after the addition of a new analyzer or connector.</p> <p>If everything is working, before submitting your pull request, please squash your commits into a single one!</p>"},{"location":"ThreatMatrix/contribute/#how-to-squash-commits-to-a-single-one","title":"How to squash commits to a single one","text":"<ul> <li>Run <code>git rebase -i HEAD~[NUMBER OF COMMITS]</code></li> <li>You should see a list of commits, each commit starting with the word \"pick\".</li> <li>Make sure the first commit says \"pick\" and change the rest from \"pick\" to \"squash\". -- This will squash each commit into the previous commit, which will continue until every commit is squashed into the first commit.</li> <li>Save and close the editor.</li> <li>It will give you the opportunity to change the commit message.</li> <li>Save and close the editor again.</li> <li>Then you have to force push the final, squashed commit: <code>git push --force-with-lease origin</code>.</li> </ul> <p>Squashing commits can be a tricky process but once you figure it out, it's really helpful and keeps our repo concise and clean.</p>"},{"location":"ThreatMatrix/contribute/#debug-application-problems","title":"Debug application problems","text":"<p>Keep in mind that, if any errors arise during development, you would need to check the application logs to better understand what is happening so you can easily address the problem.</p> <p>This is the reason why it is important to add tons of logs in the application...if they are not available in time of needs you would cry really a lot.</p> <p>Where are ThreatMatrix logs? With a default installation of ThreatMatrix, you would be able to get the application data from the following paths in your OS:</p> <ul> <li><code>/var/lib/docker/volumes/threat_matrix_generic_logs/_data/django</code>: Django Application logs</li> <li><code>/var/lib/docker/volumes/threat_matrix_generic_logs/_data/uwsgi</code>: Uwsgi application server logs</li> <li><code>/var/lib/docker/volumes/threat_matrix_nginx_logs/_data/</code>: Nginx Web Server Logs</li> </ul>"},{"location":"ThreatMatrix/installation/","title":"Installation","text":""},{"location":"ThreatMatrix/installation/#requirements","title":"Requirements","text":"<p>The project leverages <code>docker compose</code> with a custom Bash script and you need to have the following packages installed in your machine:</p> <ul> <li>docker - v19.03.0+</li> <li>docker-compose - v2.3.4+</li> </ul> <p>In some systems you could find pre-installed older versions. Please check this and install a supported version before attempting the installation. Otherwise it would fail. Note: We've added a new Bash script <code>initialize.sh</code> that will check compatibility with your system and attempt to install the required dependencies.</p> <p>Note</p> <ul> <li>The project uses public docker images that are available on Docker Hub</li> <li>ThreatMatrix is tested and supported to work in a Debian distro. More precisely we suggest using Ubuntu. Other Linux-based OS should work but that has not been tested much. It may also run on Windows, but that is not officially supported.</li> <li>ThreatMatrix does not support ARM at the moment. We'll fix this with the next v6.0.5 release <li>Before installing remember that you must comply with the LICENSE and the Legal Terms</li> <p>Warning</p> The <code>start</code> script requires a `bash` version &gt; 4 to run.  Note that macOS is shipped with an older version of <code>bash</code>. Please ensure to upgrade before running the script."},{"location":"ThreatMatrix/installation/#tldr","title":"TL;DR","text":"<p>Obviously we strongly suggest reading through all the page to configure ThreatMatrix in the most appropriate way.</p> <p>However, if you feel lazy, you could just install and test ThreatMatrix with the following steps. <code>docker</code> will be run with <code>sudo</code> if permissions/roles have not been set.</p> <pre><code># clone the ThreatMatrix project repository\ngit clone https://github.com/khulnasoft/ThreatMatrix\ncd ThreatMatrix/\n\n# run helper script to verify installed dependencies and configure basic stuff\n./initialize.sh\n\n# start the app\n./start prod up\n# now the application is running on http://localhost:80\n\n# create a super user\nsudo docker exec -ti threatmatrix_uwsgi python3 manage.py createsuperuser\n\n# now you can login with the created user from http://localhost:80/login\n\n# Have fun!\n</code></pre> <p>Warning</p> The first time you start ThreatMatrix, a lot of database migrations are being applied. This requires some time. If you get 500 status code errors in the GUI, just wait few minutes and then refresh the page."},{"location":"ThreatMatrix/installation/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<p>These are our recommendations for dedicated deployments of ThreatMatrix:</p> <ul> <li>Basic Installation in a VM: 2 CPU, 4GB RAM, 20GB Disk</li> <li>Intensive Usage (hundreds of analysis in a hour) in a single VM: 8CPU, 16GB RAM and 80GB Disk.</li> </ul> <p>Please remember that every environment has its own peculiarities so these numbers must not be taken as the holy grail.</p> <p>What should be done is a comprehensive evaluation of the environment where the application will deployed.</p> <p>For more complex environments, a Docker Swarm / Kubernetes cluster is recommended.</p> <p>ThreatMatrix's maintainers are available to offer paid consultancy and mentorship about that.</p>"},{"location":"ThreatMatrix/installation/#deployment-components","title":"Deployment Components","text":"<p>ThreatMatrix is composed of various different technologies, namely:</p> <ul> <li>React: Frontend, using CRA and certego-ui</li> <li>Django: Backend</li> <li>PostgreSQL: Database</li> <li>Redis: Message Broker</li> <li>Celery: Task Queue</li> <li>Nginx: Reverse proxy for the Django API and web asssets.</li> <li>Uwsgi: Application Server</li> <li>Daphne: Asgi Server for WebSockets</li> <li>Elastic Search (optional): Auto-sync indexing of analysis' results.</li> <li>Kibana (optional): GUI for Elastic Search. We provide a saved configuration with dashboards and visualizations.</li> <li>Flower (optional): Celery Management Web Interface</li> </ul> <p>All these components are managed via <code>docker compose</code>.</p>"},{"location":"ThreatMatrix/installation/#deployment-preparation","title":"Deployment Preparation","text":"<ul> <li>Environment configuration (required)</li> <li>Database configuration (required)</li> <li>Web server configuration (optional)</li> <li>Analyzers configuration (optional)</li> </ul> <p>Open a terminal and execute below commands to construct new environment files from provided templates.</p> <pre><code>./initialize.sh\n</code></pre>"},{"location":"ThreatMatrix/installation/#environment-configuration-required","title":"Environment configuration (required)","text":"<p>In the <code>docker/env_file_app</code>, configure different variables as explained below.</p> <p>REQUIRED variables to run the image:</p> <ul> <li><code>DB_HOST</code>, <code>DB_PORT</code>, <code>DB_USER</code>, <code>DB_PASSWORD</code>: PostgreSQL configuration (The DB credentals should match the ones in the <code>env_file_postgres</code>). If you like, you can configure the connection to an external PostgreSQL instance in the same variables. Then, to avoid to run PostgreSQL locally, please run ThreatMatrix with the option <code>--use-external-database</code>. Otherwise, <code>DB_HOST</code> must be <code>postgres</code> to have the app properly communicate with the PostgreSQL container.</li> <li> <p><code>DJANGO_SECRET</code>: random 50 chars key, must be unique. If you do not provide one, Intel Owl will automatically set a secret key and use the same for each run. The key is generated by <code>initialize.sh</code> script.</p> <p>Strongly recommended variable to set:</p> </li> </ul> <ul> <li><code>THREATMATRIX_WEB_CLIENT_DOMAIN</code> (example: <code>localhost</code>/<code>mywebsite.com</code>): the web domain of your instance, this is used for generating links to analysis results.</li> </ul> <p>Optional configuration:</p> <ul> <li><code>OLD_JOBS_RETENTION_DAYS</code>: Database retention for analysis results (default: 14 days). Change this if you want to keep your old analysis longer in the database.</li> </ul>"},{"location":"ThreatMatrix/installation/#other-optional-configuration-to-enable-specific-services-features","title":"Other optional configuration to enable specific services / features","text":"<p>Configuration required to enable integration with Slack:</p> <ul> <li><code>SLACK_TOKEN</code>: Slack token of your Slack application that will be used to send/receive notifications</li> <li><code>DEFAULT_SLACK_CHANNEL</code>: ID of the Slack channel you want to post the message to</li> </ul> <p>Configuration required to have InteOwl sending Emails (registration requests, mail verification, password reset/change, etc)</p> <ul> <li><code>DEFAULT_FROM_EMAIL</code>: email address used for automated correspondence from the site manager (example: <code>noreply@mydomain.com</code>)</li> <li><code>DEFAULT_EMAIL</code>: email address used for correspondence with users (example: <code>info@mydomain.com</code>)</li> <li><code>EMAIL_HOST</code>: the host to use for sending email with SMTP</li> <li><code>EMAIL_HOST_USER</code>: username to use for the SMTP server defined in EMAIL_HOST</li> <li><code>EMAIL_HOST_PASSWORD</code>: password to use for the SMTP server defined in EMAIL_HOST. This setting is used in conjunction with EMAIL_HOST_USER when authenticating to the SMTP server.</li> <li><code>EMAIL_PORT</code>: port to use for the SMTP server defined in EMAIL_HOST.</li> <li><code>EMAIL_USE_TLS</code>: whether to use an explicit TLS (secure) connection when talking to the SMTP server, generally used on port 587.</li> <li><code>EMAIL_USE_SSL</code>: whether to use an implicit TLS (secure) connection when talking to the SMTP server, generally used on port 465.</li> </ul>"},{"location":"ThreatMatrix/installation/#database-configuration-required-if-running-postgresql-locally","title":"Database configuration (required if running PostgreSQL locally)","text":"<p>If you use a local PostgreSQL instance (this is the default), in the <code>env_file_postgres</code> you have to configure different variables as explained below.</p> <p>Required variables:</p> <ul> <li><code>POSTGRES_PASSWORD</code> (same as <code>DB_PASSWORD</code>)</li> <li><code>POSTGRES_USER</code> (same as <code>DB_USER</code>)</li> <li><code>POSTGRES_DB</code> (default: <code>threat_matrix_db</code>)</li> </ul>"},{"location":"ThreatMatrix/installation/#logrotate-configuration-strongly-recommended","title":"Logrotate configuration (strongly recommended)","text":"<p>If you want to have your logs rotated correctly, we suggest you to add the configuration for the system Logrotate. To do that you can leverage the <code>initialize.sh</code> script. Otherwise, if you have skipped that part, you can manually install logrotate by launching the following script:</p> <pre><code>cd ./docker/scripts\n./install_logrotate.sh\n</code></pre> <p>We decided to do not leverage Django Rotation Configuration because it caused problematic concurrency issues, leading to logs that are not rotated correctly and to apps that do not log anymore. Logrotate configuration is more stable.</p>"},{"location":"ThreatMatrix/installation/#crontab-configuration-recommended-for-advanced-deployments","title":"Crontab configuration (recommended for advanced deployments)","text":"<p>We added few Crontab configurations that could be installed in the host machine at system level to solve some possible edge-case issues:</p> <ul> <li>Memory leaks: Once a week it is suggested to do a full restart of the application to clean-up the memory used by the application. Practical experience suggest us to do that to solve some recurrent memory issues in Celery. A cron called <code>application_restart</code> has been added for this purpose (it uses the absolute path of <code>start</code> script in the container). This cron assumes that you have executed ThreatMatrix with the parameters <code>--all_analyzers</code>. If you didn't, feel free to change the cron as you wish.</li> </ul> <p>This configuration is optional but strongly recommended for people who want to have a production grade deployment. To install it you need to run the following script in each deployed server:</p> <pre><code>cd ./docker/scripts\n./install_crontab.sh\n</code></pre>"},{"location":"ThreatMatrix/installation/#web-server-configuration-required-for-enabling-https","title":"Web server configuration (required for enabling HTTPS)","text":"<p>Intel Owl provides basic configuration for:</p> <ul> <li>Nginx (<code>configuration/nginx/http.conf</code>)</li> <li>Uwsgi (<code>configuration/threat_matrix.ini</code>)</li> </ul> <p>In case you enable HTTPS, remember to set the environment variable <code>HTTPS_ENABLED</code> as \"enabled\" to increment the security of the application.</p> <p>There are 3 options to execute the web server:</p> <ul> <li> <p>HTTP only (default)</p> <p>The project would use the default deployment configuration and HTTP only.</p> </li> </ul> <ul> <li> <p>HTTPS with your own certificate</p> <p>The project provides a template file to configure Nginx to serve HTTPS: <code>configuration/nginx/https.conf</code>.</p> <p>You should change <code>ssl_certificate</code>, <code>ssl_certificate_key</code> and <code>server_name</code> in that file and put those required files in the specified locations.</p> <p>Then you should call the <code>./start</code> script with the parameter <code>--https</code> to leverage the right Docker Compose file for HTTPS.</p> <p>Plus, if you use Flower, you should change in the <code>docker/flower.override.yml</code> the <code>flower_http.conf</code> with <code>flower_https.conf</code>.</p> </li> </ul> <ul> <li> <p>HTTPS with Let's Encrypt</p> <p>We provide a specific docker-compose file that leverages Traefik to allow fast deployments of public-faced and HTTPS-enabled applications.</p> <p>Before using it, you should configure the configuration file <code>docker/traefik.override.yml</code> by changing the email address and the hostname where the application is served. For a detailed explanation follow the official documentation: Traefix doc.</p> <p>After the configuration is done, you can add the option <code>--traefik</code> while executing <code>./start</code></p> </li> </ul>"},{"location":"ThreatMatrix/installation/#run","title":"Run","text":"<p>Important Info</p> ThreatMatrix depends heavily on docker and docker compose so as to hide this complexity from the enduser the project leverages a custom shell script (<code>start</code>) to interface with <code>docker compose</code>.  You may invoke <code>$ ./start --help</code> to get help and usage info.  The CLI provides the primitives to correctly build, run or stop the containers for ThreatMatrix. Therefore,  <ul> <li>It is possible to attach every optional docker container that ThreatMatrix has: multi_queue with traefik enabled while every optional docker analyzer is active.</li> <li>It is possible to insert an optional docker argument that the CLI will pass to <code>docker compose</code></li> </ul> <p>Now that you have completed different configurations, starting the containers is as simple as invoking:</p> <pre><code>$ ./start prod up\n</code></pre> <p>You can add the <code>docker</code> options <code>-d</code> to run the application in the background.</p> <p>Important Info</p> All <code>docker</code> and <code>docker compose</code> specific options must be passed at the end of the script, after a <code>--</code> token. This token indicates the end of ThreatMatrix's options and the beginning of Docker options.  Example:  <pre><code>./start prod up -- -d\n</code></pre> <p>Hint</p> Starting from ThreatMatrix 4.0.0, with the startup script you can select which version of ThreatMatrix you want to run (<code>--version</code>). This  can be helpful to keep using old versions in case of retrocompatibility issues. The <code>--version</code> parameter checks out the Git Repository to the Tag of the version that you have chosen. This means that if you checkout to a v3.x.x version, you won't have the <code>--version</code> parameter anymore so you would need to manually checkout back to the <code>master</code> branch to use newer versions.  <p>Warning</p> If, for any reason, the <code>start</code> script does not work in your environment, we suggest to use plain <code>docker compose</code> and configuring manually all the optional containers you need.  The basic replacement of <code>./start prod up</code> would be:  <pre><code>docker compose --project-directory docker -f docker/default.yml -f docker/postgres.override.yml -f docker/redis.override.yml -f docker/nginx.override.yml -p threatmatrix up\n</code></pre>"},{"location":"ThreatMatrix/installation/#stop","title":"Stop","text":"<p>To stop the application you have to:</p> <ul> <li>if executed without <code>-d</code> parameter: press <code>CTRL+C</code></li> <li>if executed with <code>-d</code> parameter: <code>./start prod down</code></li> </ul>"},{"location":"ThreatMatrix/installation/#cleanup-of-database-and-application","title":"Cleanup of database and application","text":"<p>This is a destructive operation but can be useful to start again the project from scratch</p> <p><code>./start prod down -- -v</code></p>"},{"location":"ThreatMatrix/installation/#after-deployment","title":"After Deployment","text":""},{"location":"ThreatMatrix/installation/#users-creation","title":"Users creation","text":"<p>You may want to run <code>docker exec -ti threatmatrix_uwsgi python3 manage.py createsuperuser</code> after first run to create a superuser. Then you can add other users directly from the Django Admin Interface after having logged with the superuser account. To manage users, organizations and their visibility please refer to this section</p>"},{"location":"ThreatMatrix/installation/#update-and-rebuild","title":"Update and Rebuild","text":""},{"location":"ThreatMatrix/installation/#rebuilding-the-project-creating-custom-docker-build","title":"Rebuilding the project / Creating custom docker build","text":"<p>If you make some code changes and you like to rebuild the project, follow these steps:</p> <ol> <li><code>./start test build -- --tag=&lt;your_tag&gt; .</code> to build the new docker image.</li> <li>Add this new image tag in the <code>docker/test.override.yml</code> file.</li> <li>Start the containers with <code>./start test up -- --build</code>.</li> </ol>"},{"location":"ThreatMatrix/installation/#update-to-the-most-recent-version","title":"Update to the most recent version","text":"<p>To update the project with the most recent available code you have to follow these steps:</p> <pre><code>$ cd &lt;your_threat_matrix_directory&gt; # go into the project directory\n$ git pull # pull new changes\n$ ./start prod down # kill and destroy the currently running ThreatMatrix containers\n$ ./start prod up # restart the ThreatMatrix application\n</code></pre> <p>Note</p> After an upgrade, sometimes a database error in Celery Containers could happen. That could be related to new DB migrations which are not applied by the main Uwsgi Container yet. Do not worry. Wait few seconds for the Uwsgi container to start correctly, then put down the application again and restart it. The problem should be solved. If not, please feel free to open an issue on Github  <p>Note</p> After having upgraded ThreatMatrix, in case the application does not start and you get an error like this:  <pre><code>PermissionError: [Errno 13] Permission denied: '/var/log/threat_matrix/django/authentication.log\n</code></pre>  just run this:  <pre><code>sudo chown -R www-data:www-data /var/lib/docker/volumes/threat_matrix_generic_logs/_data/django\n</code></pre>  and restart ThreatMatrix. It should solve the permissions problem.   <p>Warning</p> Major versions of ThreatMatrix are usually incompatible from one another. Maintainers strive to keep the upgrade between major version easy but it's not always like that. Below you can find the additional process required to upgrade from each major versions."},{"location":"ThreatMatrix/installation/#updating-to-600-from-a-5xx-version","title":"Updating to &gt;=6.0.0 from a 5.x.x version","text":"<p>ThreatMatrix v6 introduced some major changes regarding how the project is started. Before upgrading, some important things should be checked by the administrator:</p> <ul> <li>Docker Compose V1 support has been dropped project-wide. If you are still using a Compose version prior to v2.3.4, please upgrade to a newer version or install Docker Compose V2.</li> <li>ThreatMatrix is now started with the new Bash <code>start</code> script that has the same options as the old Python <code>start.py</code> script but is more manageable and has decreased the overall project dependencies. The <code>start.py</code> script has now been removed. Please use the new <code>start</code> script instead.</li> <li>The default message broker is now Redis. We have replaced Rabbit-MQ for Redis to allow support for Websockets in the application:<ul> <li>This change is transparent if you use our <code>start</code> script to run ThreatMatrix. That would spawn a Redis instance instead of a Rabbit-MQ one locally.</li> <li>If you were using an external broker like AWS SQS or a managed Rabbit-MQ, they are still supported but we suggest to move to a Redis supported service to simplify the architecture (because Redis is now mandatory for Websockets)</li> </ul> </li> <li>Support for multiple jobs with multiple playbooks has been removed. Every Observable or File in the request will be processed by a single playbook.</li> <li>We upgraded the base PostgreSQL image from version 12 to version 16. You have 2 choice:<ul> <li>remove your actual database and start from scratch with a new one</li> <li>maintain your database and do not update Postgres. This could break the application at anytime because we do not support it anymore.</li> <li>if you want to keep your old DB, follow the migration procedure you can find below</li> </ul> </li> </ul> <p>Warning</p> CARE! We are providing this database migration procedure to help the users to migrate to a new PostgreSQL version.  Upgrading PostgreSQL is outside the scope of the ThreatMatrix project so we do not guarantee that everything will work as intended.  In case of doubt, please check the official PostgreSQL documentation.  Upgrade at your own risk.   <p>The database migration procedure is as follows:</p> <ul> <li>You have ThreatMatrix version 5.x.x up and running</li> <li>Bring down the application (you can use the start script or manually concatenate your docker compose configuration )</li> <li>Go inside the docker folder <code>cd docker</code></li> <li>Bring only the postgres 12 container up <code>docker run -d --name threatmatrix_postgres_12 -v threat_matrix_postgres_data:/var/lib/postgresql/data/ --env-file env_file_postgres  library/postgres:12-alpine</code></li> <li>Dump the entire database. You need the user and the database that you configured during startup for this <code>docker exec -t threatmatrix_postgres_12  pg_dump -U &lt;POSTGRES_USER&gt; -d &lt;POSTGRES_DB&gt; --no-owner &gt; /tmp/dump_threatmatrix.sql</code></li> <li>Stop che container <code>docker container stop threatmatrix_postgres_12</code></li> <li>Remove the backup container <code>docker container rm threatmatrix_postgres_12</code></li> <li>Remove the postgres volume <code>docker volume rm threat_matrix_postgres_data</code> &lt;------------- remove old data, this is not exactly necessary because the new postgres has a different volume name</li> <li>Start the intermediary postgres 16 container <code>docker run -d --name threatmatrix_postgres_16 -v threatmatrix_postgres_data:/var/lib/postgresql/data/ --env-file env_file_postgres  library/postgres:16-alpine</code></li> <li>Add the data to the volume <code>cat /tmp/dump_threatmatrix.sql | docker exec -i threatmatrix_postgres_16 psql -U &lt;POSTGRES_USER&gt; -d &lt;POSTGRES_DB&gt;</code></li> <li>Stop the intermediary container <code>docker container stop threatmatrix_postgres_16</code></li> <li>Remove the intermediary container <code>docker container rm threatmatrix_postgres_16</code></li> <li>Update ThreatMatrix to the latest version</li> <li>Bring up the application back again (you can use the start script or manually concatenate your docker compose configuration)</li> </ul>"},{"location":"ThreatMatrix/installation/#updating-to-500-from-a-4xx-version","title":"Updating to &gt;=5.0.0 from a 4.x.x version","text":"<p>ThreatMatrix v5 introduced some major changes regarding how the plugins and their related configuration are managed in the application. Before upgrading, some important things should be checked by the administrator:</p> <ul> <li>A lot of database migrations will need to be applied. Just be patient few minutes once you install the new major release. If you get 500 status code errors in the GUI, just wait few minutes and then refresh the page.</li> <li>We moved away from the old big <code>analyzer_config.json</code> which was storing all the base configuration of the Analyzers to a database model (we did the same for all the other plugins types too). This allows us to manage plugins creation/modification/deletion in a more reliable manner and via the Django Admin Interface. If you have created custom plugins and changed those <code>&lt;plugins&gt;_config.json</code> file manually, you would need to re-create those custom plugins again from the Django Admin Interface. To do that please follow the related new documentation</li> <li>We have REMOVED all the analyzers that we deprecated during the v4 releases cycle. Please substitute them with their respective new names, in case they have a replacement.<ul> <li>REMOVED <code>Pulsedive_Active_IOC</code> analyzer. Please substitute it with the new <code>Pulsedive</code> analyzer.</li> <li>REMOVED <code>Fortiguard</code> analyzer because endpoint does not work anymore. No substitute.</li> <li>REMOVED <code>Rendertron</code> analyzer not working as intended. No substitute.</li> <li>REMOVED <code>ThreatMiner</code>, <code>SecurityTrails</code> and <code>Robtex</code> various analyzers and substituted with new versions.</li> <li>REMOVED <code>Doc_Info_Experimental</code>. Its functionality (XLM Macro parsing) is moved to <code>Doc_Info</code></li> <li>REMOVED <code>Strings_Info_Classic</code>. Please use <code>Strings_Info</code></li> <li>REMOVED <code>Strings_Info_ML</code>. Please use <code>Strings_Info</code> and set the parameter <code>rank_strings</code> to <code>True</code></li> <li>REMOVED all <code>Yara_Scan_&lt;repo&gt;</code> analyzers. They all went merged in the single <code>Yara</code> analyzer</li> <li>REMOVED <code>Darksearch_Query</code> analyzer because the service does not exist anymore. No substitute.</li> <li>REMOVED <code>UnpacMe_EXE_Unpacker</code>. Please use <code>UnpacMe</code></li> <li>REMOVED <code>BoxJS_Scan_JavaScript</code>. Please use <code>BoxJS</code></li> <li>REMOVED all <code>Anomali_Threatstream_&lt;option&gt;</code> analyzers. Now we have a single <code>Anomali_Threatstream</code> analyzer. Use the parameters to select the specific API you need.</li> </ul> </li> </ul>"},{"location":"ThreatMatrix/installation/#updating-to-500-from-a-3xx-version","title":"Updating to &gt;=5.0.0 from a 3.x.x version","text":"<p>This is not supported. Please perform a major upgrade once at a time.</p>"},{"location":"ThreatMatrix/installation/#updating-to-400-from-a-3xx-version","title":"Updating to &gt;=4.0.0 from a 3.x.x version","text":"<p>ThreatMatrix v4 introduced some major changes regarding the permission management, allowing an easier way to manage users and visibility. But that did break the previous available DB. So, to migrate to the new major version you would need to delete your DB. To do that, you would need to delete your volumes and start the application from scratch.</p> <pre><code>python3 start.py prod down -v\n</code></pre> <p>Please be aware that, while this can be an important effort to manage, the v4 ThreatMatrix provides an easier way to add, invite and manage users from the application itself. See the Organization section.</p>"},{"location":"ThreatMatrix/installation/#updating-to-200-from-a-1xx-version","title":"Updating to &gt;=2.0.0 from a 1.x.x version","text":"<p>Users upgrading from previous versions need to manually move <code>env_file_app</code>, <code>env_file_postgres</code> and <code>env_file_integrations</code> files under the new <code>docker</code> directory.</p>"},{"location":"ThreatMatrix/installation/#updating-to-v13x-from-any-prior-version","title":"Updating to &gt;v1.3.x from any prior version","text":"<p>If you are updating to &gt;v1.3.0 from any prior version, you need to execute a helper script so that the old data present in the database doesn't break.</p> <ol> <li> <p>Follow the above updation steps, once the docker containers are up and running execute the following in a new terminal</p> <pre><code>docker exec -ti threatmatrix_uwsgi bash\n</code></pre> <p>to get a shell session inside the ThreatMatrix's container.</p> </li> <li> <p>Now just copy and paste the below command into this new session,</p> <pre><code>curl https://gist.githubusercontent.com/Eshaan7/b111f887fa8b860c762aa38d99ec5482/raw/758517acf87f9b45bd22f06aee57251b1f3b1bbf/update_to_v1.3.0.py | python -\n</code></pre> </li> <li> <p>If you see \"Update successful!\", everything went fine and now you can enjoy the new features!</p> </li> </ol>"},{"location":"ThreatMatrix/introduction/","title":"Introduction","text":"<p> ThreatMatrix Repository</p>"},{"location":"ThreatMatrix/introduction/#introduction","title":"Introduction","text":"<p>ThreatMatrix was designed with the intent to help the community, in particular those researchers that can not afford commercial solutions, in the generation of threat intelligence data, in a simple, scalable and reliable way.</p> <p>Main features:</p> <ul> <li>Provides enrichment of Threat Intel for malware as well as observables (IP, Domain, URL, hash, etc).</li> <li>This application is built to scale out and to speed up the retrieval of threat info.</li> <li>Thanks to the official libraries pythreatmatrix and go-threatmatrix, it can be integrated easily in your stack of security tools to automate common jobs usually performed, for instance, by SOC analysts manually.</li> <li>Intel Owl is composed of:<ul> <li>analyzers that can be run to either retrieve data from external sources (like VirusTotal or AbuseIPDB) or to generate intel from internally available tools (like Yara or Oletools)</li> <li>connectors that can be run to export data to external platforms (like MISP or OpenCTI)</li> <li>visualizers that can be run to create custom visualizations of analyzers results</li> <li>playbooks that are meant to make analysis easily repeatable</li> </ul> </li> <li>API REST written in Django and Python 3.9.</li> <li>Built-in frontend client written in ReactJS, with certego-ui: provides features such as dashboard, visualizations of analysis data, easy to use forms for requesting new analysis, etc.</li> </ul>"},{"location":"ThreatMatrix/introduction/#publications-and-media","title":"Publications and media","text":"<p>To know more about the project and its growth over time, you may be interested in reading the following official blog posts and/or videos:</p> <ul> <li>HelpNetSecurity interview</li> <li>FIRSTCON 24 Fukuoka (Japan)</li> <li>The Honeynet Workshop: Denmark 2024</li> <li>Certego Blog: v6 Announcement (in Italian)</li> <li>HackinBo 2023 Presentation (in Italian)</li> <li>Certego Blog: v.5.0.0 Announcement</li> <li>Youtube demo: ThreatMatrix v4</li> <li>Certego Blog: v.4.0.0 Announcement</li> <li>LimaCharlie sponsorship</li> <li>Tines sponsorship</li> <li>APNIC blog</li> <li>Honeynet Blog: v3.0.0 Announcement</li> <li>Intel Owl on Daily Swig</li> <li>Honeynet Blog: v1.0.0 Announcement</li> <li>Certego Blog: First announcement</li> </ul> <p>Feel free to ask everything it comes to your mind about the project to the author: Matteo Lodi (Twitter).</p> <p>We also have a dedicated twitter account for the project: @threat_matrix.</p>"},{"location":"ThreatMatrix/usage/","title":"Usage","text":"<p>This page includes the most important things to know and understand when using ThreatMatrix.</p>"},{"location":"ThreatMatrix/usage/#how-to-interact-with-threatmatrix","title":"How to interact with ThreatMatrix","text":"<p>Intel Owl main objective is to provide a single API interface to query in order to retrieve threat intelligence at scale.</p> <p>There are multiple ways to interact with the Intel Owl APIs,</p> <ol> <li> <p>Web Interface</p> <ul> <li>Built-in Web interface with dashboard, visualizations of analysis data, easy to use forms for requesting new analysis, tags management and more features</li> </ul> </li> <li> <p>pyThreatMatrix (CLI/SDK)</p> <ul> <li>Official Python client that is available at: PyThreatMatrix,</li> <li>Can be used as a library for your own python projects or...</li> <li>directly via the command line interface.</li> </ul> </li> <li> <p>goThreatMatrix (CLI/SDK)</p> <ul> <li>Official GO client that is available at: go-threatmatrix</li> </ul> </li> </ol> <p>Hint: Tokens Creation</p> The server authentication is managed by API tokens. So, if you want to interact with Intel Owl, you have two ways to do that: <ul> <li>If you are a normal user, you can go to the \"API Access/Sessions\" section of the GUI and create a Token there.</li> <li>If you are an administrator of ThreatMatrix, you can create one or more unprivileged users from the Django Admin Interface and then generate a token for those users. </li> </ul> Afterwards you can leverage the created tokens with the Intel Owl Client."},{"location":"ThreatMatrix/usage/#plugins-framework","title":"Plugins Framework","text":"<p>Plugins are the core modular components of ThreatMatrix that can be easily added, changed and customized. There are several types of plugins:</p> <ul> <li>Analyzers</li> <li>Connectors</li> <li>Pivots</li> <li>Visualizers</li> <li>Ingestors</li> <li>Playbooks</li> </ul>"},{"location":"ThreatMatrix/usage/#analyzers","title":"Analyzers","text":"<p>Analyzers are the most important plugins in ThreatMatrix. They allow to perform data extraction on the observables and/or files that you would like to analyze.</p>"},{"location":"ThreatMatrix/usage/#analyzers-list","title":"Analyzers list","text":"<p>The following is the list of the available analyzers you can run out-of-the-box. You can also navigate the same list via the</p> <ul> <li>Graphical Interface: once your application is up and running, go to the \"Plugins\" section</li> <li>pythreatmatrix: <code>$ pythreatmatrix get-analyzer-config</code></li> </ul>"},{"location":"ThreatMatrix/usage/#file-analyzers","title":"File analyzers:","text":""},{"location":"ThreatMatrix/usage/#internal-tools","title":"Internal tools","text":"<ul> <li><code>APKiD</code>: APKiD identifies many compilers, packers, obfuscators, and other weird stuff from an APK or DEX file.</li> <li><code>BoxJS_Scan_Javascript</code>: Box-JS is a tool for studying JavaScript malware.</li> <li><code>Capa_Info</code>: Capa detects capabilities in executable files</li> <li><code>Capa_Info_Shellcode</code>: Capa detects capabilities in shellcode</li> <li><code>ClamAV</code>: scan a file via the ClamAV AntiVirus Engine. ThreatMatrix automatically keep ClamAV updated with official and unofficial open source signatures</li> <li><code>Doc_Info</code>: static document analysis with new features to analyze XLM macros, encrypted macros and more (combination of Oletools and XLMMacroDeobfuscator)</li> <li><code>ELF_Info</code>: static ELF analysis with pyelftools and telfhash</li> <li><code>File_Info</code>: static generic File analysis (hashes, magic and exiftool)</li> <li><code>Floss</code>: Mandiant Floss Obfuscated String Solver in files</li> <li><code>Hfinger</code>: create fingerprints of malware HTTPS requests using Hfinger</li> <li><code>PE_Info</code>: static PE analysis with pefile</li> <li><code>PEframe_Scan</code>: Perform static analysis on Portable Executable malware and malicious MS Office documents with PeFrame</li> <li><code>Permhash</code>: create hash of manifest permssions found in APK, Android manifest, Chrome extensions or Chrome extension manifest using Permhash</li> <li><code>PDF_Info</code>: static PDF analysis (peepdf + pdfid)</li> <li><code>Qiling_Linux</code>: Qiling qiling linux binary emulation.</li> <li><code>Qiling_Linux_Shellcode</code>: Qiling qiling linux shellcode emulation.</li> <li><code>Qiling_Windows</code>: Qiling qiling windows binary emulation.</li> <li><code>Qiling_Windows_Shellcode</code>: Qiling qiling windows shellcode emulation.</li> <li><code>Quark_Engine</code>: Quark Engine is an Obfuscation-Neglect Android Malware Scoring System.</li> <li><code>Rtf_Info</code>: static RTF analysis (Oletools)</li> <li><code>Signature_Info</code>: PE signature extractor with osslsigncode</li> <li><code>Speakeasy</code>: Mandiant Speakeasy binary emulation</li> <li><code>SpeakEasy_Shellcode</code>: Mandiant Speakeasy shellcode emulation</li> <li><code>Strings_Info</code>: Strings extraction. Leverages Mandiant's Stringsifter</li> <li><code>Suricata</code>: Analyze PCAPs with open IDS signatures with Suricata engine</li> <li><code>Thug_HTML_Info</code>: Perform hybrid dynamic/static analysis on a HTML file using Thug low-interaction honeyclient</li> <li><code>Xlm_Macro_Deobfuscator</code>: XlmMacroDeobfuscator deobfuscate xlm macros</li> <li><code>Yara</code>: scan a file with<ul> <li>ATM malware yara rules</li> <li>bartblaze yara rules</li> <li>community yara rules</li> <li>StrangerealIntel</li> <li>Neo23x0 yara rules</li> <li>Intezer yara rules</li> <li>Inquest yara rules</li> <li>McAfee yara rules</li> <li>Samir Threat Hunting yara rules</li> <li>Stratosphere yara rules</li> <li>Mandiant yara rules</li> <li>ReversingLabs yara rules</li> <li>YARAify rules</li> <li>SIFalcon rules</li> <li>Elastic rules</li> <li>JPCERTCC Yara rules</li> <li>HuntressLab Yara rules</li> <li>elceef Yara Rules</li> <li>dr4k0nia Yara rules</li> <li>Facebook Yara rules</li> <li>edelucia Yara rules</li> <li>LOLDrivers Yara Rules</li> <li>your own added signatures. See Advanced-Usage for more details.</li> </ul> </li> <li><code>Zippy_scan</code> : Zippy: Fast method to classify text as AI or human-generated; takes in <code>lzma</code>,<code>zlib</code>,<code>brotli</code> as input based engines; <code>ensemble</code> being default.</li> <li><code>Blint</code>: Blint is a Binary Linter that checks the security properties and capabilities of your executables. Supported binary formats: - Android (apk, aab) - ELF (GNU, musl) - PE (exe, dll) - Mach-O (x64, arm64)</li> <li><code>Mobsf</code>: MobSF is a static analysis tool that can find insecure code patterns in your Android and iOS source code. Supports Java, Kotlin, Android XML, Swift and Objective C Code.</li> <li><code>DroidLysis</code>: DroidLysis is a pre-analysis tool for Android apps: it performs repetitive and boring tasks we'd typically do at the beginning of any reverse engineering. It disassembles the Android sample, organizes output in directories, and searches for suspicious spots in the code to look at. The output helps the reverse engineer speed up the first few steps of analysis.</li> <li><code>Artifacts</code>: Artifacts is a tool that does APK strings analysis. Useful for first analysis.</li> </ul>"},{"location":"ThreatMatrix/usage/#external-services","title":"External services","text":"<ul> <li><code>CapeSandbox</code>: CAPESandbox automatically scans suspicious files using the CapeSandbox API. Analyzer works for private instances as well.</li> <li><code>Cymru_Hash_Registry_Get_File</code>: Check if a particular file is known to be malware by Team Cymru</li> <li><code>Cuckoo_Scan</code>: scan a file on Cuckoo (this analyzer is disabled by default. You have to change that flag in the config to use it)</li> <li><code>DocGuard_Upload_File</code>: Analyze office files in seconds. DocGuard.</li> <li><code>Dragonfly_Emulation</code>: Emulate malware against Dragonfly sandbox by Certego S.R.L.</li> <li><code>FileScan_Upload_File</code>: Upload your file to extract IoCs from executable files, documents and scripts via FileScan.io API.</li> <li><code>HashLookupServer_Get_File</code>: check if a md5 or sha1 is available in the database of known file hosted by CIRCL</li> <li><code>HybridAnalysis_Get_File</code>: check file hash on HybridAnalysis sandbox reports</li> <li><code>Intezer_Scan</code>: scan a file on Intezer. Register for a free community account here. With TLP <code>CLEAR</code>, in case the hash is not found, you would send the file to the service.</li> <li><code>Malpedia_Scan</code>: scan a binary or a zip file (pwd:infected) against all the yara rules available in Malpedia</li> <li><code>MalwareBazaar_Get_File</code>: Check if a particular malware sample is known to MalwareBazaar</li> <li><code>MISPFIRST_Check_Hash</code>: check a file hash on the FIRST MISP instance</li> <li><code>MISP_Check_Hash</code>: check a file hash on a MISP instance</li> <li><code>MWDB_Scan</code>: mwdblib Retrieve malware file analysis from repository maintained by CERT Polska MWDB. With TLP <code>CLEAR</code>, in case the hash is not found, you would send the file to the service.</li> <li><code>OTX_Check_Hash</code>: check file hash on Alienvault OTX</li> <li><code>SublimeSecurity</code>: Analyze an Email with Sublime Security live flow</li> <li><code>Triage_Scan</code>: leverage Triage sandbox environment to scan various files</li> <li><code>UnpacMe</code>: UnpacMe is an automated malware unpacking service</li> <li><code>Virushee_Scan</code>: Check file hash on Virushee API. With TLP <code>CLEAR</code>, in case the hash is not found, you would send the file to the service.</li> <li><code>VirusTotal_v3_File</code>: check the file hash on VirusTotal. With TLP <code>CLEAR</code>, in case the hash is not found, you would send the file to the service.</li> <li><code>YARAify_File_Scan</code>: scan a file against public and non-public YARA and ClamAV signatures in YARAify public service</li> <li><code>YARAify_File_Search</code>: scan an hash against YARAify database</li> </ul>"},{"location":"ThreatMatrix/usage/#observable-analyzers-ip-domain-url-hash","title":"Observable analyzers (ip, domain, url, hash)","text":""},{"location":"ThreatMatrix/usage/#internal-tools_1","title":"Internal tools","text":"<ul> <li><code>CheckDMARC</code>: An SPF and DMARC DNS records validator for domains.</li> <li><code>DNStwist</code>: Scan a url/domain to find potentially malicious permutations via dns fuzzing. dnstwist repo</li> <li><code>Thug_URL_Info</code>: Perform hybrid dynamic/static analysis on a URL using Thug low-interaction honeyclient</li> <li><code>AILTypoSquatting</code>:AILTypoSquatting is a Python library to generate list of potential typo squatting domains with domain name permutation engine to feed AIL and other systems.</li> </ul>"},{"location":"ThreatMatrix/usage/#external-services_1","title":"External services","text":"<ul> <li><code>AbuseIPDB</code>: check if an ip was reported on AbuseIPDB</li> <li><code>Abusix</code>: get abuse contacts of an IP address from Abusix</li> <li><code>BGP Ranking</code>: BGP-Ranking provides a way to collect such malicious activities, aggregate the information per ASN and provide a ranking model to rank the ASN from the most malicious to the less malicious ASN.</li> <li><code>Anomali_Threatstream_PassiveDNS</code>: Return information from passive dns of Anomali. On Anomali Threatstream PassiveDNS Api.</li> <li><code>Auth0</code>: scan an IP against the Auth0 API</li> <li><code>BinaryEdge</code>: Details about an Host. List of recent events for the specified host, including details of exposed ports and services using IP query and return list of subdomains known from the target domains using domain query</li> <li><code>BitcoinAbuse</code> : Check a BTC address against bitcoinabuse.com, a public database of BTC addresses used by hackers and criminals.</li> <li><code>Censys_Search</code>: scan an IP address against Censys View API</li> <li><code>CheckPhish</code>: CheckPhish can detect phishing and fraudulent sites.</li> <li><code>CIRCLPassiveDNS</code>: scan an observable against the CIRCL Passive DNS DB</li> <li><code>CIRCLPassiveSSL</code>: scan an observable against the CIRCL Passive SSL DB</li> <li><code>Classic_DNS</code>: Retrieve current domain resolution with default DNS</li> <li><code>CloudFlare_DNS</code>: Retrieve current domain resolution with CloudFlare DoH (DNS over HTTPS)</li> <li><code>CloudFlare_Malicious_Detector</code>: Leverages CloudFlare DoH to check if a domain is related to malware</li> <li><code>Crowdsec</code>: check if an IP was reported on Crowdsec Smoke Dataset</li> <li><code>Cymru_Hash_Registry_Get_Observable</code>: Check if a particular hash is available in the malware hash registry of Team Cymru</li> <li><code>DNSDB</code>: scan an observable against the Passive DNS Farsight Database (support both v1 and v2 versions)</li> <li><code>DNS0_EU</code>: Retrieve current domain resolution with DNS0.eu DoH (DNS over HTTPS)</li> <li><code>DNS0_EU_Malicious_Detector</code>: Check if a domain or an url is marked as malicious in DNS0.eu database (Zero service)</li> <li><code>DocGuard_Get</code>: check if an hash was analyzed on DocGuard. DocGuard</li> <li><code>Feodo_Tracker</code>: Feodo Tracker offers various blocklists, helping network owners to protect their users from Dridex and Emotet/Heodo.</li> <li><code>FileScan_Search</code>: Finds reports and uploaded files by various tokens, like hash, filename, verdict, IOCs etc via FileScan.io API.</li> <li><code>FireHol_IPList</code>: check if an IP is in FireHol's IPList</li> <li><code>GoogleSafebrowsing</code>: Scan an observable against GoogleSafeBrowsing DB</li> <li><code>GoogleWebRisk</code>: Scan an observable against WebRisk API (Commercial version of Google Safe Browsing). Check the docs to enable this properly</li> <li><code>Google_DNS</code>: Retrieve current domain resolution with Google DoH (DNS over HTTPS)</li> <li><code>ThreatPot</code>: scan an IP or a domain against the ThreatPot API (requires API key)</li> <li><code>GreyNoise</code>: scan an IP against the Greynoise API (requires API key)</li> <li><code>GreyNoiseCommunity</code>: scan an IP against the Community Greynoise API (requires API key))</li> <li><code>Greynoise_Labs</code>: scan an IP against the Greynoise API (requires authentication token which can be obtained from cookies on Greynoise website after launching the playground from here)</li> <li><code>HashLookupServer_Get_Observable</code>: check if a md5 or sha1 is available in the database of known file hosted by CIRCL</li> <li><code>HoneyDB_Get</code>: HoneyDB IP lookup service</li> <li><code>HoneyDB_Scan_Twitter</code>: scan an IP against HoneyDB.io's Twitter Threat Feed</li> <li><code>Hunter_How</code>: Scans IP and domain against Hunter_How API.</li> <li><code>Hunter_Io</code>: Scans a domain name and returns set of data about the organisation, the email address found and additional information about the people owning those email addresses.</li> <li><code>HybridAnalysis_Get_Observable</code>: search an observable in the HybridAnalysis sandbox reports</li> <li><code>IP2WHOIS</code>: API Docs IP2Location.io IP2WHOIS Domain WHOIS API helps users to obtain domain information and WHOIS record by using a domain name.</li> <li><code>IPQS_Fraud_And_Risk_Scoring</code>: Scan an Observable against IPQualityscore</li> <li><code>InQuest_DFI</code>: Deep File Inspection by InQuest Labs</li> <li><code>InQuest_IOCdb</code>: Indicators of Compromise Database by InQuest Labs</li> <li><code>InQuest_REPdb</code>: Search in InQuest Lab's Reputation Database</li> <li><code>IPApi</code>: Get information about IPs using batch-endpoint and DNS using DNS-endpoint.</li> <li><code>IPInfo</code>: Location Information about an IP</li> <li><code>Ip2location</code>: API Docs IP2Location.io allows users to check IP address location in real time. (Supports both with or without key)</li> <li><code>Intezer_Get</code>: check if an analysis related to a hash is available in Intezer. Register for a free community account here.</li> <li><code>Koodous</code>: koodous API get information about android malware.</li> <li><code>MalwareBazaar_Get_Observable</code>: Check if a particular malware hash is known to MalwareBazaar</li> <li><code>MalwareBazaar_Google_Observable</code>: Check if a particular IP, domain or url is known to MalwareBazaar using google search</li> <li><code>MaxMindGeoIP</code>: extract GeoIP info for an observable</li> <li><code>MISP</code>: scan an observable on a MISP instance</li> <li><code>MISPFIRST</code>: scan an observable on the FIRST MISP instance</li> <li><code>Mmdb_server</code>: Mmdb_server mmdb-server is an open source fast API server to lookup IP addresses for their geographic location, AS number.</li> <li><code>Mnemonic_PassiveDNS</code> : Look up a domain or IP using the Mnemonic PassiveDNS public API.</li> <li><code>MWDB_Get</code>: mwdblib Retrieve malware file analysis by hash from repository maintained by CERT Polska MWDB.</li> <li><code>Netlas</code>: search an IP against Netlas</li> <li><code>ONYPHE</code>: search an observable in ONYPHE</li> <li><code>OpenCTI</code>: scan an observable on an OpenCTI instance</li> <li><code>OTXQuery</code>: scan an observable on Alienvault OTX</li> <li><code>Phishstats</code>: Search PhishStats API to determine if an IP/URL/domain is malicious.</li> <li><code>Phishtank</code>: Search an url against Phishtank API</li> <li><code>PhishingArmy</code>: Search an observable in the PhishingArmy blocklist</li> <li><code>Pulsedive</code>: Scan indicators and retrieve results from Pulsedive's API.</li> <li><code>Quad9_DNS</code>: Retrieve current domain resolution with Quad9 DoH (DNS over HTTPS)</li> <li><code>Quad9_Malicious_Detector</code>: Leverages Quad9 DoH to check if a domain is related to malware</li> <li><code>Robtex</code>: scan a domain/IP against the Robtex Passive DNS DB</li> <li><code>Securitytrails</code>: scan an IP/Domain against Securitytrails API</li> <li><code>Shodan_Honeyscore</code>: scan an IP against Shodan Honeyscore API</li> <li><code>Shodan_Search</code>: scan an IP against Shodan Search API</li> <li><code>Spyse</code>: Scan domains, IPs, emails and CVEs using Spyse's API. Register here.</li> <li><code>SSAPINet</code>: get a screenshot of a web page using screenshotapi.net (external source); additional config options can be added to <code>extra_api_params</code> in the config.</li> <li><code>Stalkphish</code>: Search Stalkphish API to retrieve information about a potential phishing site (IP/URL/domain/Generic).</li> <li><code>Stratosphere_Blacklist</code>: Cross-reference an IP from blacklists maintained by Stratosphere Labs</li> <li><code>TalosReputation</code>: check an IP reputation from Talos</li> <li><code>ThreatFox</code>: search for an IOC in ThreatFox's database</li> <li><code>Threatminer</code>: retrieve data from Threatminer API</li> <li><code>TorNodesDanMeUk</code>: check if an IP is a Tor Node using a list of all Tor nodes provided by dan.me.uk</li> <li><code>TorProject</code>: check if an IP is a Tor Exit Node</li> <li><code>Triage_Search</code>: Search for reports of observables or upload from URL on triage cloud</li> <li><code>Tranco</code>: Check if a domain is in the latest Tranco ranking top sites list</li> <li><code>URLhaus</code>: Query a domain or URL against URLhaus API.</li> <li><code>UrlScan_Search</code>: Search an IP/domain/url/hash against URLScan API</li> <li><code>UrlScan_Submit_Result</code>: Submit &amp; retrieve result of an URL against URLScan API</li> <li><code>Virushee_CheckHash</code>: Search for a previous analysis of a file by its hash (SHA256/SHA1/MD5) on Virushee API.</li> <li><code>VirusTotal_v3_Get_Observable</code>: search an observable in the VirusTotal DB</li> <li><code>Whoisxmlapi</code>: Fetch WHOIS record data, of a domain name, an IP address, or an email address.</li> <li><code>WhoIs_RipeDB_Search</code> : Fetch whois record data of an IP address from Ripe DB using their search API (no API key required)</li> <li><code>XForceExchange</code>: scan an observable on IBM X-Force Exchange</li> <li><code>YARAify_Search</code>: lookup a file hash in Abuse.ch YARAify</li> <li><code>YETI</code> (Your Everyday Threat Intelligence): scan an observable on a YETI instance.</li> <li><code>Zoomeye</code>: Zoomeye Cyberspace Search Engine recording information of devices, websites, services and components etc..</li> <li><code>Validin</code>:Validin investigates historic and current data describing the structure and composition of the internet.</li> <li><code>TweetFeed</code>: TweetFeed collects Indicators of Compromise (IOCs) shared by the infosec community at Twitter.\\r\\nHere you will find malicious URLs, domains, IPs, and SHA256/MD5 hashes.</li> <li><code>HudsonRock</code>: Hudson Rock provides its clients the ability to query a database of over 27,541,128 computers which were compromised through global info-stealer campaigns performed by threat actors.</li> <li><code>CyCat</code>: CyCat or the CYbersecurity Resource CATalogue aims at mapping and documenting, in a single formalism and catalogue available cybersecurity tools, rules, playbooks, processes and controls.</li> <li><code>Vulners</code>: Vulners is the most complete and the only fully correlated security intelligence database, which goes through constant updates and links 200+ data sources in a unified machine-readable format. It contains 8 mln+ entries, including CVEs, advisories, exploits, and IoCs \u2014 everything you need to stay abreast on the latest security threats.</li> </ul>"},{"location":"ThreatMatrix/usage/#generic-analyzers-email-phone-number-etc-anything-really","title":"Generic analyzers (email, phone number, etc.; anything really)","text":"<p>Some analyzers require details other than just IP, URL, Domain, etc. We classified them as <code>generic</code> Analyzers. Since the type of field is not known, there is a format for strings to be followed.</p>"},{"location":"ThreatMatrix/usage/#internal-tools_2","title":"Internal tools","text":"<ul> <li><code>CyberChef</code>: Run a query on a CyberChef server using pre-defined or custom recipes.</li> </ul>"},{"location":"ThreatMatrix/usage/#external-services_2","title":"External services","text":"<ul> <li><code>Anomali_Threatstream_Confidence</code>: Give max, average and minimum confidence of maliciousness for an observable. On Anomali Threatstream Confidence API.</li> <li><code>Anomali_Threatstream_Intelligence</code>: Search for threat intelligence information about an observable. On Anomali Threatstream Intelligence API.</li> <li><code>CRXcavator</code>: scans a chrome extension against crxcavator.io</li> <li><code>Dehashed_Search</code>: Query any observable/keyword against https://dehashed.com's search API.</li> <li><code>EmailRep</code>: search an email address on emailrep.io</li> <li><code>HaveIBeenPwned</code>: HaveIBeenPwned checks if an email address has been involved in a data breach</li> <li><code>IntelX_Intelligent_Search</code>: IntelligenceX is a search engine and data archive. Fetches emails, urls, domains associated with an observable or a generic string.</li> <li><code>IntelX_Phonebook</code>: IntelligenceX is a search engine and data archive. Fetches emails, urls, domains associated with an observable or a generic string.</li> <li><code>IPQS_Fraud_And_Risk_Scoring</code>: Scan an Observable against IPQualityscore</li> <li><code>MISP</code>: scan an observable on a MISP instance</li> <li><code>VirusTotal_v3_Intelligence_Search</code>: Perform advanced queries with VirusTotal Intelligence (requires paid plan)</li> <li><code>WiGLE</code>: Maps and database of 802.11 wireless networks, with statistics, submitted by wardrivers, netstumblers, and net huggers.</li> <li><code>YARAify_Generics</code>: lookup a YARA rule (default), ClamAV rule, imphash, TLSH, telfhash or icon_dash in YARAify</li> <li><code>PhoneInfoga</code> : PhoneInfoga is one of the most advanced tools to scan international phone numbers.</li> <li><code>HudsonRock</code>: Hudson Rock provides its clients the ability to query a database of over 27,541,128 computers which were compromised through global info-stealer campaigns performed by threat actors.</li> </ul>"},{"location":"ThreatMatrix/usage/#optional-analyzers","title":"Optional analyzers","text":"<p>Some analyzers are optional and need to be enabled explicitly.</p>"},{"location":"ThreatMatrix/usage/#connectors","title":"Connectors","text":"<p>Connectors are designed to run after every successful analysis which makes them suitable for automated threat-sharing. They support integration with other SIEM/SOAR projects, specifically aimed at Threat Sharing Platforms.</p>"},{"location":"ThreatMatrix/usage/#connectors-list","title":"Connectors list","text":"<p>The following is the list of the available connectors. You can also navigate the same list via the</p> <ul> <li>Graphical Interface: once your application is up and running, go to the \"Plugins\" section</li> <li>pythreatmatrix: <code>$ pythreatmatrix get-connector-config</code></li> </ul>"},{"location":"ThreatMatrix/usage/#list-of-pre-built-connectors","title":"List of pre-built Connectors","text":"<ul> <li><code>MISP</code>: automatically creates an event on your MISP instance, linking the successful analysis on ThreatMatrix.</li> <li><code>OpenCTI</code>: automatically creates an observable and a linked report on your OpenCTI instance, linking the the successful analysis on ThreatMatrix.</li> <li><code>YETI</code>: YETI = Your Everyday Threat Intelligence. find or create observable on YETI, linking the successful analysis on ThreatMatrix.</li> <li><code>Slack</code>: Send the analysis link to a Slack channel (useful for external notifications)</li> <li><code>EmailSender</code>: Send a generic email.</li> <li><code>AbuseSubmitter</code>: Send an email to request to take down a malicious domain.</li> </ul>"},{"location":"ThreatMatrix/usage/#pivots","title":"Pivots","text":"<p>With ThreatMatrix v5.2.0 we introduced the <code>Pivot</code> Plugin.</p> <p>Pivots are designed to create a job from another job. This plugin allows the user to set certain conditions that trigger the execution of one or more subsequent jobs, strictly connected to the first one.</p> <p>This is a \"SOAR\" feature that allows the users to connect multiple analysis together.</p>"},{"location":"ThreatMatrix/usage/#list-of-pre-built-pivots","title":"List of pre-built Pivots","text":"<ul> <li><code>TakedownRequestToAbuseIp</code>: This Plugin leverages results from DNS resolver analyzers to extract a valid IP address to pivot to the Abusix analyzer.</li> <li><code>AbuseIpToSubmission</code>: This Plugin leverages results from the Abusix analyzer to extract the abuse contacts of an IP address to pivot to the AbuseSubmitter connector.</li> </ul> <p>You can build your own custom Pivot with your custom logic with just few lines of code. See the Contribute section for more info.</p>"},{"location":"ThreatMatrix/usage/#creating-pivots-from-the-gui","title":"Creating Pivots from the GUI","text":"<p>From the GUI, the users can pivot in two ways:</p> <ul> <li>If a Job executed a Visualizer, it is possible to select a field extracted and analyze its value by clicking the \"Pivot\" button (see following image). In this way, the user is able to \"jump\" from one indicator to another.   </li> </ul> <ul> <li>Starting from an already existing Investigation, it is possible to select a Job block and click the \"Pivot\" button to analyze the same observable again, usually choosing another Playbook (see following image)   </li> </ul> <p>In both cases, the user is redirected to the Scan Page that is precompiled with the observable selected. Then the user would be able to select the Playbook to execute in the new job. </p> <p>After the new Job is started, a new Investigation will be created (if it does not already exist) and both the jobs will be added to the same Investigation.</p> <p>In the following image you can find an example of an Investigation composed by 3 pivots generated manually:</p> <ul> <li>leveraging the first way to create a Pivot, the 2 Jobs that analyzed IP addresses have been generated from the first <code>test\\.com</code> Job</li> <li>leveraging the second way to create a Pivot, the second <code>test\\.com</code> analysis had been created with a different Playbook.</li> </ul> <p></p>"},{"location":"ThreatMatrix/usage/#visualizers","title":"Visualizers","text":"<p>With ThreatMatrix v5 we introduced a new plugin type called Visualizers. You can leverage it as a framework to create custom aggregated and simplified visualization of analyzer results.</p> <p>Visualizers are designed to run after the analyzers and the connectors. The visualizer adds logic after the computations, allowing to show the final result in a different way than merely the list of reports.</p> <p>Visualizers can be executed only during <code>Scans</code> through the playbook that has been configured on the visualizer itself.</p> <p>This framework is extremely powerful and allows every user to customize the GUI as they wish. But you know...with great power comes great responsability. To fully leverage this framework, you would need to put some effort in place. You would need to understand which data is useful for you and then write few code lines that would create your own GUI. To simplify the process, take example from the pre-built visualizers listed below and follow the dedicated documentation.</p>"},{"location":"ThreatMatrix/usage/#list-of-pre-built-visualizers","title":"List of pre-built Visualizers","text":"<ul> <li><code>DNS</code>: displays the aggregation of every DNS analyzer report</li> <li><code>Yara</code>: displays the aggregation of every matched rule by the <code>Yara</code> Analyzer</li> <li><code>Domain_Reputation</code>: Visualizer for the Playbook \"Popular_URL_Reputation_Services\"</li> <li><code>IP_Reputation</code>: Visualizer for the Playbook \"Popular_IP_Reputation_Services\"</li> <li><code>Pivot</code>: Visualizer that can be used in a Playbook to show the Pivot execution result. See Pivots for more info.</li> </ul>"},{"location":"ThreatMatrix/usage/#ingestors","title":"Ingestors","text":"<p>With ThreatMatrix v5.1.0 we introduced the <code>Ingestor</code> Plugin.</p> <p>Ingestors allow to automatically insert IOC streams from outside sources to ThreatMatrix itself. Each Ingestor must have a <code>Playbook</code> attached: this will allow to create a <code>Job</code> from every IOC retrieved.</p> <p>Ingestors are system-wide and disabled by default, meaning that only the administrator are able to configure them and enable them. Ingestors can be spammy so be careful about enabling them.</p> <p>A very powerful use is case is to combine Ingestors with Connectors to automatically extract data from external sources, analyze them with ThreatMatrix and push them externally to another platform (like MISP or a SIEM)</p>"},{"location":"ThreatMatrix/usage/#list-of-pre-built-ingestors","title":"List of pre-built Ingestors","text":"<ul> <li><code>ThreatFox</code>: Retrieves daily ioc from <code>https://threatfox.abuse.ch/</code> and analyze them.</li> <li><code>MalwareBazaar</code>: Retrieves hourly samples from <code>https://bazaar.abuse.ch/</code> and analyze them.</li> <li><code>VirusTotal</code>: Perform intelligence queries at hourly intervals from <code>https://www.virustotal.com/</code> (premium api key required), then retrieves the samples and analyzes them.</li> </ul>"},{"location":"ThreatMatrix/usage/#playbooks","title":"Playbooks","text":"<p>Playbooks are designed to be easy to share sequence of running Plugins (Analyzers, Connectors, ...) on a particular kind of observable.</p> <p>If you want to avoid to re-select/re-configure a particular combination of analyzers and connectors together every time, you should create a playbook out of it and use it instead. This is time saver.</p> <p>This is a feature introduced since ThreatMatrix v4.1.0! Please provide feedback about it!</p>"},{"location":"ThreatMatrix/usage/#playbooks-list","title":"Playbooks List","text":"<p>The following is the list of the available pre-built playbooks. You can also navigate the same list via the</p> <ul> <li>Graphical Interface: once your application is up and running, go to the \"Plugins\" section</li> <li>pythreatmatrix: <code>$ pythreatmatrix get-playbook-config</code></li> </ul>"},{"location":"ThreatMatrix/usage/#list-of-pre-built-playbooks","title":"List of pre-built playbooks","text":"<ul> <li><code>FREE_TO_USE_ANALYZERS</code>: A playbook containing all free to use analyzers.</li> <li><code>Sample_Static_Analysis</code>: A playbook containing all analyzers that perform static analysis on files.</li> <li><code>Popular_URL_Reputation_Services</code>: Collection of the most popular and free reputation analyzers for URLs and Domains</li> <li><code>Popular_IP_Reputation_Services</code>: Collection of the most popular and free reputation analyzers for IP addresses</li> <li><code>Dns</code>: A playbook containing all dns providers</li> <li><code>Takedown_Request</code>: Start investigation to request to take down a malicious domain. A mail will be sent to the domain's abuse contacts found</li> <li><code>Abuse_IP</code>: Playbook containing the Abusix analyzer. It is executed after the Takedown_Request playbook</li> <li><code>Send_Abuse_Email</code>: Playbook containing the AbuseSubmitter connector to send an email to request to take down a malicious domain. It is executed after the Abuse_IP playbook</li> </ul>"},{"location":"ThreatMatrix/usage/#playbooks-creation-and-customization","title":"Playbooks creation and customization","text":"<p>You can create new playbooks in different ways, based on the users you want to share them with:</p> <p>If you want to share them to every user in ThreatMatrix, create them via the Django Admin interface at <code>/admin/playbooks_manager/playbookconfig/</code>.</p> <p>If you want share them to yourself or your organization only, you need to leverage the \"Save as Playbook\" button that you can find on the top right of the Job Result Page. In this way, after you have done an analysis, you can save the configuration of the Plugins you executed for re-use with a single click.</p> <p></p> <p>The created Playbook would be available to yourself only. If you want either to share it with your organization or to delete it, you need to go to the \"Plugins\" section and enable it manually by clicking the dedicated button.</p> <p></p>"},{"location":"ThreatMatrix/usage/#generic-plugin-creation-configuration-and-customization","title":"Generic Plugin Creation, Configuration and Customization","text":"<p>If you want to create completely new Plugins (not based on already existing python modules), please refer to the Contribute section. This is usually the case when you want to integrate ThreatMatrix with either a new tool or a new service.</p> <p>On the contrary, if you would like to just customize the already existing plugins, this is the place.</p>"},{"location":"ThreatMatrix/usage/#superuser-customization","title":"SuperUser customization","text":"<p>If you are an ThreatMatrix superuser, you can create, modify, delete analyzers based on already existing modules by changing the configuration values inside the Django Admin interface at:</p> <ul> <li>for analyzers: <code>/admin/analyzers_manager/analyzerconfig/</code>.</li> <li>for connectors: <code>/admin/connectors_manager/connectorconfig/</code>.</li> <li>...and so on for all the Plugin types.</li> </ul> <p>The following are the most important fields that you can change without touching the source code:</p> <ul> <li><code>Name</code>: Name of the analyzer</li> <li><code>Description</code>: Description of the analyzer</li> <li><code>Disabled</code>: you can choose to disable certain analyzers, then they won't appear in the dropdown list and won't run if requested.</li> <li><code>Python Module</code>: Python path of the class that will be executed. This should not be changed most of the times.</li> <li><code>Maximum TLP</code>: see TLP Support</li> <li><code>Soft Time Limit</code>: this is the maximum time (in seconds) of execution for an analyzer. Once reached, the task will be killed (or managed in the code by a custom Exception). Default <code>300</code>.</li> <li><code>Routing Key</code>: this takes effects only when multi-queue is enabled. Choose which celery worker would execute the task: <code>local</code> (ideal for tasks that leverage local applications like Yara), <code>long</code> (ideal for long tasks) or <code>default</code> (ideal for simple webAPI-based analyzers).</li> </ul> <p>For analyzers only:</p> <ul> <li><code>Supported Filetypes</code>: can be populated as a list. If set, if you ask to analyze a file with a different mimetype from the ones you specified, it won't be executed</li> <li><code>Not Supported Filetypes</code>: can be populated as a list. If set, if you ask to analyze a file with a mimetype from the ones you specified, it won't be executed</li> <li><code>Observable Supported</code>: can be populated as a list. If set, if you ask to analyze an observable that is not in this list, it won't be executed. Valid values are: <code>ip</code>, <code>domain</code>, <code>url</code>, <code>hash</code>, <code>generic</code>.</li> </ul> <p>For connectors only:</p> <ul> <li><code>Run on Failure</code> (default: <code>true</code>): if they can be run even if the job has status <code>reported_with_fails</code></li> </ul> <p>For visualizers only:</p> <ul> <li><code>Playbooks</code>: list of playbooks that trigger the specified visualizer execution.</li> </ul> <p>Sometimes, it may happen that you would like to create a new analyzer very similar to an already existing one. Maybe you would like to just change the description and the default parameters. A helpful way to do that without having to copy/pasting the entire configuration, is to click on the analyzer that you want to copy, make the desired changes, and click the <code>save as new</code> button.</p> <p>Warning</p> Changing other keys can break a plugin. In that case, you should think about duplicating the configuration entry or python module with your changes.  <p>Other options can be added at the \"Python module\" level and not at the Plugin level. To do that, go to: <code>admin/api_app/pythonmodule/</code> and select the Python module used by the Plugin that you want to change. For example, the analyzer <code>AbuseIPDB</code> uses the Python module <code>abuseipdb.AbuseIPDB</code>.</p> <p></p> <p>Once there, you'll get this screen:</p> <p></p> <p>There you can change the following values:</p> <ul> <li><code>Update Schedule</code>: if the analyzer require some sort of update (local database, local rules, ...), you can specify the crontab schedule to update them.</li> <li><code>Health Check Schedule</code>: if the analyzer has implemented a Health Check, you can specify the crontab schedule to check whether the service works or not.</li> </ul>"},{"location":"ThreatMatrix/usage/#parameters","title":"Parameters","text":"<p>Each Plugin could have one or more parameters available to be configured. These parameters allow the users to customize the Plugin behavior.</p> <p>There are 2 types of Parameters:</p> <ul> <li>classic Parameters</li> <li><code>Secrets</code>: these parameters usually manage sensitive data, like API keys.</li> </ul> <p>To see the list of these parameters:</p> <ul> <li>You can view the \"Plugin\" Section in ThreatMatrix to have a complete and updated view of all the options available</li> <li>You can view the parameters by exploring the Django Admin Interface:<ul> <li><code>admin/api_app/parameter/</code></li> <li>or at the very end of each Plugin configuration like <code>/admin/analyzers_manager/analyzerconfig/</code></li> </ul> </li> </ul> <p>You can change the Plugin Parameters at 5 different levels:</p> <ul> <li>if you are an ThreatMatrix superuser, you can go in the Django Admin Interface and change the default values of the parameters for every plugin you like. This option would change the default behavior for every user in the platform.</li> <li>if you are either Owner or Admin of an org, you can customize the default values of the parameters for every member of the organization by leveraging the GUI in the \"Organization Config\" section. This overrides the previous option.</li> <li>if you are a normal user, you can customize the default values of the parameters for your analysis only by leveraging the GUI in the \"Plugin config\" section. This overrides the previous option.</li> <li>You can choose to provide runtime configuration when requesting an analysis that will override the previous options. This override is done only for the specific analysis. See Customize analyzer execution at time of request</li> </ul> <p>Playbook Exception</p> Please remember that, if you are executing a Playbook, the \"Runtime configuration\" of the Playbook take precedence over the Plugin Configuration.  <p>Plugin Configuration Order</p> Due to the multiple chances that are given to customize the parameters of the Plugins that are executed, it may be easy to confuse the order and launch Plugins without the awereness of what you are doing.  This is the order to define which values are used for the parameters, starting by the most important element:  - Runtime Configuration at Time of Request. - Runtime Configuration of the Playbook (if a Playbook is used and the Runtime Configuration at Time of Request is empty) - Plugin Configuration of the User - Plugin Configuration of the Organization - Default Plugin Configuration of the Parameter  If you are using the GUI, please remember that you can always check the Parameters before starting a \"Scan\" by clicking at the \"Runtime configuration\" ![img.png](./static/runtime_config.png) button.  Example: ![img.png](./static/runtime_config_2.png)"},{"location":"ThreatMatrix/usage/#enabling-or-disabling-plugins","title":"Enabling or Disabling Plugins","text":"<p>By default, each available plugin is configured as either disabled or not. The majority of them are enabled by default, while others may be disabled to avoid potential problems in the application usability for first time users.</p> <p>Considering the impact that this change could have in the application, the GUI does not allow a normal user to enable/disable any plugin. On the contrary, users with specific privileges may change this configuration:</p> <ul> <li>Org Administrators may leverage the feature documented here to enable/disable plugins for their org. This can be helpful to control users' behavior.</li> <li>ThreatMatrix Superusers (full admin) can go to the Django Admin Interface and enable/disable them from there. This operation does overwrite the Org administrators configuration. To find the plugin to change, they'll need to first choose the section of its type (\"ANALYZERS_MANAGER\", \"CONNECTORS_MANAGER\", etc), then select the chosen plugin, change the flag on that option and save the plugin by pressing the right button.</li> </ul> <p></p> <p></p>"},{"location":"ThreatMatrix/usage/#special-plugins-operations","title":"Special Plugins operations","text":"<p>All plugins, i.e. analyzers and connectors, have <code>kill</code> and <code>retry</code> actions. In addition to that, all docker-based analyzers and connectors have a <code>healthcheck</code> action to check if their associated instances are up or not.</p> <ul> <li> <p>kill:</p> <p>Stop a plugin whose status is <code>running</code>/<code>pending</code>:</p> <ul> <li>GUI: Buttons on reports table on job result page.</li> <li>PyThreatMatrix: <code>ThreatMatrix.kill_analyzer</code> and <code>ThreatMatrix.kill_connector</code> function.</li> <li>CLI: <code>$ pythreatmatrix jobs kill-analyzer &lt;job_id&gt; &lt;analyzer_name&gt;</code> and <code>$ pythreatmatrix jobs kill-connector &lt;job_id&gt; &lt;connector_name&gt;</code></li> <li>API: <code>PATCH /api/job/{job_id}/{plugin_type/{plugin_name}/kill</code> and <code>PATCH /api/job/{job_id}/connector/{connector_name}/kill</code></li> </ul> </li> </ul> <ul> <li> <p>retry:</p> <p>Retry a plugin whose status is <code>failed</code>/<code>killed</code>:</p> <ul> <li>GUI: Buttons on reports table on job result page.</li> <li>PyThreatMatrix: <code>ThreatMatrix.retry_analyzer</code> and <code>ThreatMatrix.retry_connector</code> function,</li> <li>CLI: <code>$ pythreatmatrix jobs retry-analyzer &lt;job_id&gt; &lt;analyzer_name&gt;</code> and <code>$ pythreatmatrix jobs retry-connector &lt;job_id&gt; &lt;connector_name&gt;</code></li> <li>API: <code>PATCH /api/job/{job_id}/{plugin_type}/{plugin_name}/retry</code></li> </ul> </li> </ul> <ul> <li> <p>healthcheck:</p> <p>Check if a plugin is able to connect to its provider:</p> <ul> <li>GUI: Buttons on every plugin table.</li> <li>PyThreatMatrix: <code>ThreatMatrix.analyzer_healthcheck</code> and <code>ThreatMatrix.connector_healthcheck</code> methods.</li> <li>CLI: <code>$ pythreatmatrix analyzer-healthcheck &lt;analyzer_name&gt;</code> and <code>$ pythreatmatrix connector-healthcheck &lt;connector_name&gt;</code></li> <li>API: <code>GET /api/{plugin_type}/{plugin_name}/healthcheck</code></li> </ul> </li> </ul> <ul> <li> <p>pull:</p> <p>Update a plugin with the newest rules/database:</p> <ul> <li>GUI: Buttons on every plugin table.</li> <li>API: <code>POST /api/{plugin_type}/{plugin_name}/pull</code></li> </ul> </li> </ul>"},{"location":"ThreatMatrix/usage/#tlp-support","title":"TLP Support","text":"<p>The Traffic Light Protocol (TLP) is a standard that was created to facilitate greater sharing of potentially sensitive information and more effective collaboration.</p> <p>ThreatMatrix is not a threat intel sharing platform, like the MISP platform. However, ThreatMatrix is able to share analysis results to external platforms (via Connectors) and to send possible privacy related information to external services (via Analyzers).</p> <p>This is why ThreatMatrix does support a customized version of the Traffic Light Protocol (TLP): to allow the user to have a better knowledge of how their data are being shared.</p> <p>Every Analyzer and Connector can be configured with a <code>maximum_tlp</code> value. Based on that value, ThreatMatrix understands if the specific plugin is allowed or not to run (e.g. if <code>maximum_tlp</code> is <code>GREEN</code>, it would run for analysis with TLPs <code>WHITE</code> and <code>GREEN</code> only)</p> <p>These is how every available TLP value behaves once selected for an analysis execution:</p> <ol> <li><code>CLEAR</code>: no restriction (<code>WHITE</code> was replaced by <code>CLEAR</code> in TLP v2.0, but <code>WHITE</code> is supported for retrocompatibility)</li> <li><code>GREEN</code>: disable analyzers that could impact privacy</li> <li><code>AMBER</code> (default): disable analyzers that could impact privacy and limit view permissions to my group</li> <li><code>RED</code>: disable analyzers that could impact privacy, limit view permissions to my group and do not use any external service</li> </ol>"},{"location":"ThreatMatrix/usage/#running-a-plugin","title":"Running a plugin","text":"<p>A plugin can be run when all of the following requirements have been satisfied:</p> <ol> <li>All the required parameters of the plugin have been configured</li> <li>The plugin is not disabled</li> <li>The plugin is not disabled for the user's organization</li> <li>If the plugin has a health check schedule, the last check has to be successful</li> <li>The TLP selected to run the plugin cannot be higher than the maximum TLP configured for that plugin</li> <li>The observable classification or the file mimetype has to be supported by the plugin</li> </ol>"},{"location":"ThreatMatrix/usage/#investigations-framework","title":"Investigations Framework","text":"<p>Investigations are a new framework introduced in ThreatMatrix v6 with the goal to allow the users to connect the analysis they do with each other.</p> <p>In this way the analysts can use ThreatMatrix as the starting point of their \"Investigations\", register their findings, correlate the information found, and collaborate...all in a single place.</p> <p>Things to know about the framework:</p> <ul> <li>an Investigation is a superset of ThreatMatrix Jobs. It can have attached one or more existing ThreatMatrix Jobs</li> <li>an Investigation contains a \"description\" section that can be changed and updated at anytime with new information from the analysts.</li> <li>modification to the Investigation (description, jobs, etc) can be done by every member of the Organization where the creator of the Investigation belongs. However only they creator can delete an Investigation.</li> </ul>"},{"location":"ThreatMatrix/usage/#create-and-populate-an-investigation","title":"Create and populate an investigation","text":"<p>Investigations are created in 2 ways:</p> <ul> <li>automatically:<ul> <li>if you scan multiple observables at the same time, a new investigation will be created by default and all the observables they will be automatically connected to the same investigation.</li> <li>if you run a Job with a Playbook which contains a Pivot that triggers another Job, a new investigation will be created and both the Jobs will be added to the same investigation. See how you can create a new Pivot manually from the GUI.</li> </ul> </li> <li>manually: by clicking on the button in the \"History\" section you can create an Investigation from scratch without any job attached (see following image)</li> </ul> <p></p> <p>If you want to add a job to an Investigation, you should click to the root block of the Investigation (see following image):</p> <p></p> <p>Once a job has been added, you'll have something like this:</p> <p></p> <p>If you want to remove a Job, you can click on the Job block and click \"Remove branch\". On the contrary, if you just want to see Job Results, you can click in the \"Link\" button. (check next image)</p> <p></p>"},{"location":"ThreatMatrix/usage/#example-output-of-a-complex-investigation","title":"Example output of a complex investigation","text":""},{"location":"ThreatPot/Api-docs/","title":"API Documentation","text":""},{"location":"ThreatPot/Api-docs/#enrichment","title":"<code>enrichment</code>","text":"<p>Handle enrichment requests for a specific observable (domain or IP address).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object containing query parameters.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response indicating whether the observable was found,</p> <p>and if so, the corresponding IOC.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>@api_view([GET])\n@authentication_classes([CookieTokenAuthentication])\n@permission_classes([IsAuthenticated])\ndef enrichment_view(request):\n    \"\"\"\n    Handle enrichment requests for a specific observable (domain or IP address).\n\n    Args:\n        request: The incoming request object containing query parameters.\n\n    Returns:\n        Response: A JSON response indicating whether the observable was found,\n        and if so, the corresponding IOC.\n    \"\"\"\n    observable_name = request.query_params.get(\"query\")\n    logger.info(f\"Enrichment view requested for: {str(observable_name)}\")\n    serializer = EnrichmentSerializer(data=request.query_params, context={\"request\": request})\n    serializer.is_valid(raise_exception=True)\n\n    source_ip = str(request.META[\"REMOTE_ADDR\"])\n    request_source = Statistics(source=source_ip, view=viewType.ENRICHMENT_VIEW.value)\n    request_source.save()\n\n    return Response(serializer.data, status=status.HTTP_200_OK)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#feeds","title":"<code>feeds</code>","text":"<p>Handle requests for IOC feeds with specific parameters and format the response accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <code>feed_type</code> <code>str</code> <p>Type of feed (e.g., log4j, cowrie, etc.).</p> required <code>attack_type</code> <code>str</code> <p>Type of attack (e.g., all, specific attack types).</p> required <code>age</code> <code>str</code> <p>Age of the data to filter (e.g., recent, persistent).</p> required <code>format_</code> <code>str</code> <p>Desired format of the response (e.g., json, csv, txt).</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>The HTTP response with formatted IOC data.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>@api_view([GET])\ndef feeds(request, feed_type, attack_type, age, format_):\n    \"\"\"\n    Handle requests for IOC feeds with specific parameters and format the response accordingly.\n\n    Args:\n        request: The incoming request object.\n        feed_type (str): Type of feed (e.g., log4j, cowrie, etc.).\n        attack_type (str): Type of attack (e.g., all, specific attack types).\n        age (str): Age of the data to filter (e.g., recent, persistent).\n        format_ (str): Desired format of the response (e.g., json, csv, txt).\n\n    Returns:\n        Response: The HTTP response with formatted IOC data.\n    \"\"\"\n    logger.info(f\"request /api/feeds with params: feed type: {feed_type}, \" f\"attack_type: {attack_type}, Age: {age}, format: {format_}\")\n\n    iocs_queryset = get_queryset(request, feed_type, attack_type, age, format_)\n    return feeds_response(request, iocs_queryset, feed_type, format_)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#feeds_pagination","title":"<code>feeds_pagination</code>","text":"<p>Handle requests for paginated IOC feeds based on query parameters.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>The paginated HTTP response with IOC data.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>@api_view([GET])\ndef feeds_pagination(request):\n    \"\"\"\n    Handle requests for paginated IOC feeds based on query parameters.\n\n    Args:\n        request: The incoming request object.\n\n    Returns:\n        Response: The paginated HTTP response with IOC data.\n    \"\"\"\n    params = request.query_params\n    logger.info(f\"request /api/feeds with params: {params}\")\n\n    paginator = CustomPageNumberPagination()\n    iocs_queryset = get_queryset(\n        request,\n        params[\"feed_type\"],\n        params[\"attack_type\"],\n        params[\"age\"],\n        \"json\",\n    )\n    iocs = paginator.paginate_queryset(iocs_queryset, request)\n    resp_data = feeds_response(request, iocs, params[\"feed_type\"], \"json\", dict_only=True)\n    return paginator.get_paginated_response(resp_data)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#statistics","title":"<code>Statistics</code>","text":"<p>               Bases: <code>ViewSet</code></p> <p>A viewset for viewing and editing statistics related to feeds and enrichment data.</p> <p>Provides actions to retrieve statistics about the sources and downloads of feeds, as well as statistics on enrichment data.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>class StatisticsViewSet(viewsets.ViewSet):\n    \"\"\"\n    A viewset for viewing and editing statistics related to feeds and enrichment data.\n\n    Provides actions to retrieve statistics about the sources and downloads of feeds,\n    as well as statistics on enrichment data.\n    \"\"\"\n\n    @action(detail=True, methods=[\"GET\"])\n    def feeds(self, request, pk=None):\n        \"\"\"\n        Retrieve feed statistics, including the number of sources and downloads.\n\n        Args:\n            request: The incoming request object.\n            pk (str): The type of statistics to retrieve (e.g., \"sources\", \"downloads\").\n\n        Returns:\n            Response: A JSON response containing the requested statistics.\n        \"\"\"\n        if pk == \"sources\":\n            annotations = {\n                \"Sources\": Count(\n                    \"source\",\n                    distinct=True,\n                    filter=Q(view=viewType.FEEDS_VIEW.value),\n                )\n            }\n        elif pk == \"downloads\":\n            annotations = {\"Downloads\": Count(\"source\", filter=Q(view=viewType.FEEDS_VIEW.value))}\n        else:\n            logger.error(\"this is impossible. check the code\")\n            return HttpResponseServerError()\n        return self.__aggregation_response_static_statistics(annotations)\n\n    @action(detail=True, methods=[\"get\"])\n    def enrichment(self, request, pk=None):\n        \"\"\"\n        Retrieve enrichment statistics, including the number of sources and requests.\n\n        Args:\n            request: The incoming request object.\n            pk (str): The type of statistics to retrieve (e.g., \"sources\", \"requests\").\n\n        Returns:\n            Response: A JSON response containing the requested statistics.\n        \"\"\"\n        if pk == \"sources\":\n            annotations = {\n                \"Sources\": Count(\n                    \"source\",\n                    distinct=True,\n                    filter=Q(view=viewType.ENRICHMENT_VIEW.value),\n                )\n            }\n        elif pk == \"requests\":\n            annotations = {\"Requests\": Count(\"source\", filter=Q(view=viewType.ENRICHMENT_VIEW.value))}\n        else:\n            logger.error(\"this is impossible. check the code\")\n            return HttpResponseServerError()\n        return self.__aggregation_response_static_statistics(annotations)\n\n    @action(detail=False, methods=[\"get\"])\n    def feeds_types(self, request):\n        \"\"\"\n        Retrieve statistics for different types of feeds, including Log4j, Cowrie,\n        and general honeypots.\n\n        Args:\n            request: The incoming request object.\n\n        Returns:\n            Response: A JSON response containing the feed type statistics.\n        \"\"\"\n        # FEEDS\n        annotations = {\n            \"Log4j\": Count(\"name\", distinct=True, filter=Q(log4j=True)),\n            \"Cowrie\": Count(\"name\", distinct=True, filter=Q(cowrie=True)),\n        }\n        # feed_type for each general honeypot in the list\n        generalHoneypots = GeneralHoneypot.objects.all().filter(active=True)\n        for hp in generalHoneypots:\n            annotations[hp.name] = Count(\"name\", Q(general_honeypot__name__iexact=hp.name.lower()))\n        return self.__aggregation_response_static_ioc(annotations)\n\n    def __aggregation_response_static_statistics(self, annotations: dict) -&gt; Response:\n        \"\"\"\n        Helper method to generate statistics response based on annotations.\n\n        Args:\n            annotations (dict): Dictionary containing the annotations for the query.\n\n        Returns:\n            Response: A JSON response containing the aggregated statistics.\n        \"\"\"\n        delta, basis = self.__parse_range(self.request)\n        qs = Statistics.objects.filter(request_date__gte=delta).annotate(date=Trunc(\"request_date\", basis)).values(\"date\").annotate(**annotations)\n        return Response(qs)\n\n    def __aggregation_response_static_ioc(self, annotations: dict) -&gt; Response:\n        \"\"\"\n        Helper method to generate IOC response based on annotations.\n\n        Args:\n            annotations (dict): Dictionary containing the annotations for the query.\n\n        Returns:\n            Response: A JSON response containing the aggregated IOC data.\n        \"\"\"\n        delta, basis = self.__parse_range(self.request)\n\n        qs = (\n            IOC.objects.filter(last_seen__gte=delta)\n            .exclude(general_honeypot__active=False)\n            .annotate(date=Trunc(\"last_seen\", basis))\n            .values(\"date\")\n            .annotate(**annotations)\n        )\n        return Response(qs)\n\n    @staticmethod\n    def __parse_range(request):\n        \"\"\"\n        Parse the range parameter from the request query string to determine the time range for the query.\n\n        Args:\n            request: The incoming request object.\n\n        Returns:\n            tuple: A tuple containing the delta time and basis for the query range.\n        \"\"\"\n        try:\n            range_str = request.GET[\"range\"]\n        except KeyError:\n            # default\n            range_str = \"7d\"\n\n        return parse_humanized_range(range_str)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#docs.Submodules.ThreatPot.api.views.StatisticsViewSet.__aggregation_response_static_ioc","title":"<code>__aggregation_response_static_ioc(annotations)</code>","text":"<p>Helper method to generate IOC response based on annotations.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>Dictionary containing the annotations for the query.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>A JSON response containing the aggregated IOC data.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>def __aggregation_response_static_ioc(self, annotations: dict) -&gt; Response:\n    \"\"\"\n    Helper method to generate IOC response based on annotations.\n\n    Args:\n        annotations (dict): Dictionary containing the annotations for the query.\n\n    Returns:\n        Response: A JSON response containing the aggregated IOC data.\n    \"\"\"\n    delta, basis = self.__parse_range(self.request)\n\n    qs = (\n        IOC.objects.filter(last_seen__gte=delta)\n        .exclude(general_honeypot__active=False)\n        .annotate(date=Trunc(\"last_seen\", basis))\n        .values(\"date\")\n        .annotate(**annotations)\n    )\n    return Response(qs)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#docs.Submodules.ThreatPot.api.views.StatisticsViewSet.__aggregation_response_static_statistics","title":"<code>__aggregation_response_static_statistics(annotations)</code>","text":"<p>Helper method to generate statistics response based on annotations.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>Dictionary containing the annotations for the query.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>A JSON response containing the aggregated statistics.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>def __aggregation_response_static_statistics(self, annotations: dict) -&gt; Response:\n    \"\"\"\n    Helper method to generate statistics response based on annotations.\n\n    Args:\n        annotations (dict): Dictionary containing the annotations for the query.\n\n    Returns:\n        Response: A JSON response containing the aggregated statistics.\n    \"\"\"\n    delta, basis = self.__parse_range(self.request)\n    qs = Statistics.objects.filter(request_date__gte=delta).annotate(date=Trunc(\"request_date\", basis)).values(\"date\").annotate(**annotations)\n    return Response(qs)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#docs.Submodules.ThreatPot.api.views.StatisticsViewSet.__parse_range","title":"<code>__parse_range(request)</code>  <code>staticmethod</code>","text":"<p>Parse the range parameter from the request query string to determine the time range for the query.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the delta time and basis for the query range.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>@staticmethod\ndef __parse_range(request):\n    \"\"\"\n    Parse the range parameter from the request query string to determine the time range for the query.\n\n    Args:\n        request: The incoming request object.\n\n    Returns:\n        tuple: A tuple containing the delta time and basis for the query range.\n    \"\"\"\n    try:\n        range_str = request.GET[\"range\"]\n    except KeyError:\n        # default\n        range_str = \"7d\"\n\n    return parse_humanized_range(range_str)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#docs.Submodules.ThreatPot.api.views.StatisticsViewSet.enrichment","title":"<code>enrichment(request, pk=None)</code>","text":"<p>Retrieve enrichment statistics, including the number of sources and requests.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <code>pk</code> <code>str</code> <p>The type of statistics to retrieve (e.g., \"sources\", \"requests\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response containing the requested statistics.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>@action(detail=True, methods=[\"get\"])\ndef enrichment(self, request, pk=None):\n    \"\"\"\n    Retrieve enrichment statistics, including the number of sources and requests.\n\n    Args:\n        request: The incoming request object.\n        pk (str): The type of statistics to retrieve (e.g., \"sources\", \"requests\").\n\n    Returns:\n        Response: A JSON response containing the requested statistics.\n    \"\"\"\n    if pk == \"sources\":\n        annotations = {\n            \"Sources\": Count(\n                \"source\",\n                distinct=True,\n                filter=Q(view=viewType.ENRICHMENT_VIEW.value),\n            )\n        }\n    elif pk == \"requests\":\n        annotations = {\"Requests\": Count(\"source\", filter=Q(view=viewType.ENRICHMENT_VIEW.value))}\n    else:\n        logger.error(\"this is impossible. check the code\")\n        return HttpResponseServerError()\n    return self.__aggregation_response_static_statistics(annotations)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#docs.Submodules.ThreatPot.api.views.StatisticsViewSet.feeds","title":"<code>feeds(request, pk=None)</code>","text":"<p>Retrieve feed statistics, including the number of sources and downloads.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <code>pk</code> <code>str</code> <p>The type of statistics to retrieve (e.g., \"sources\", \"downloads\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response containing the requested statistics.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>@action(detail=True, methods=[\"GET\"])\ndef feeds(self, request, pk=None):\n    \"\"\"\n    Retrieve feed statistics, including the number of sources and downloads.\n\n    Args:\n        request: The incoming request object.\n        pk (str): The type of statistics to retrieve (e.g., \"sources\", \"downloads\").\n\n    Returns:\n        Response: A JSON response containing the requested statistics.\n    \"\"\"\n    if pk == \"sources\":\n        annotations = {\n            \"Sources\": Count(\n                \"source\",\n                distinct=True,\n                filter=Q(view=viewType.FEEDS_VIEW.value),\n            )\n        }\n    elif pk == \"downloads\":\n        annotations = {\"Downloads\": Count(\"source\", filter=Q(view=viewType.FEEDS_VIEW.value))}\n    else:\n        logger.error(\"this is impossible. check the code\")\n        return HttpResponseServerError()\n    return self.__aggregation_response_static_statistics(annotations)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#docs.Submodules.ThreatPot.api.views.StatisticsViewSet.feeds_types","title":"<code>feeds_types(request)</code>","text":"<p>Retrieve statistics for different types of feeds, including Log4j, Cowrie, and general honeypots.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response containing the feed type statistics.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>@action(detail=False, methods=[\"get\"])\ndef feeds_types(self, request):\n    \"\"\"\n    Retrieve statistics for different types of feeds, including Log4j, Cowrie,\n    and general honeypots.\n\n    Args:\n        request: The incoming request object.\n\n    Returns:\n        Response: A JSON response containing the feed type statistics.\n    \"\"\"\n    # FEEDS\n    annotations = {\n        \"Log4j\": Count(\"name\", distinct=True, filter=Q(log4j=True)),\n        \"Cowrie\": Count(\"name\", distinct=True, filter=Q(cowrie=True)),\n    }\n    # feed_type for each general honeypot in the list\n    generalHoneypots = GeneralHoneypot.objects.all().filter(active=True)\n    for hp in generalHoneypots:\n        annotations[hp.name] = Count(\"name\", Q(general_honeypot__name__iexact=hp.name.lower()))\n    return self.__aggregation_response_static_ioc(annotations)\n</code></pre>"},{"location":"ThreatPot/Api-docs/#general_honeypot_list","title":"<code>general_honeypot_list</code>","text":"<p>Retrieve a list of all general honeypots, optionally filtering by active status.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object containing query parameters.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response containing the list of general honeypots.</p> Source code in <code>docs/Submodules/ThreatPot/api/views.py</code> <pre><code>@api_view([GET])\ndef general_honeypot_list(request):\n    \"\"\"\n    Retrieve a list of all general honeypots, optionally filtering by active status.\n\n    Args:\n        request: The incoming request object containing query parameters.\n\n    Returns:\n        Response: A JSON response containing the list of general honeypots.\n    \"\"\"\n\n    logger.info(f\"Requested general honeypots list from {request.user}.\")\n    active = request.query_params.get(\"onlyActive\")\n    honeypots = []\n    generalHoneypots = GeneralHoneypot.objects.all()\n    if active == \"true\":\n        generalHoneypots = generalHoneypots.filter(active=True)\n        logger.info(\"Requested only active general honeypots\")\n    honeypots.extend([hp.name for hp in generalHoneypots])\n\n    logger.info(f\"General honeypots: {honeypots}\")\n    return Response(honeypots)\n</code></pre>"},{"location":"ThreatPot/Contribute/","title":"Contribute","text":""},{"location":"ThreatPot/Contribute/#general-guidance","title":"General Guidance","text":"<p>Please refer to ThreatMatrix Documentation for everything missing here.</p>"},{"location":"ThreatPot/Contribute/#rules","title":"Rules","text":"<p>ThreatPot welcomes contributors from anywhere and from any kind of education or skill level. We strive to create a community of developers that is welcoming, friendly and right.</p> <p>For this reason it is important to follow some easy rules based on a simple but important concept: Respect.</p> <ul> <li>Before starting to work on an issue, you need to get the approval of one of the maintainers. Therefore please ask to be assigned to an issue. If you do not that but you still raise a PR for that issue, your PR can be rejected. This is a form of respect for both the maintainers and the other contributors who could have already started to work on the same problem.</li> </ul> <ul> <li>When you ask to be assigned to an issue, it means that you are ready to work on it. When you get assigned, take the lock and then you disappear, you are not respecting the maintainers and the other contributors who could be able to work on that. So, after having been assigned, you have a week of time to deliver your first draft PR. After that time has passed without any notice, you will be unassigned.</li> </ul> <ul> <li>Before asking questions regarding how the project works, please read through all the documentation and install the project on your own local machine to try it and understand how it basically works. This is a form of respect to the maintainers.</li> </ul> <ul> <li>Once you started working on an issue and you have some work to share and discuss with us, please raise a draft PR early with incomplete changes. This way you can continue working on the same and we can track your progress and actively review and help. This is a form of respect to you and to the maintainers.</li> </ul> <ul> <li>When creating a PR, please read through the sections that you will find in the PR template and compile it appropriately. If you do not, your PR can be rejected. This is a form of respect to the maintainers.</li> </ul>"},{"location":"ThreatPot/Contribute/#code-style","title":"Code Style","text":"<p>Keeping to a consistent code style throughout the project makes it easier to contribute and collaborate. We make use of <code>psf/black</code> and isort for code formatting and <code>flake8</code> for style guides.</p>"},{"location":"ThreatPot/Contribute/#how-to-start-setup-project-and-development-instance","title":"How to start (Setup project and development instance)","text":"<p>To start with the development setup, make sure you go through all the steps in Installation Guide and properly installed it.</p> <p>Please create a new branch based on the develop branch that contains the most recent changes. This is mandatory.</p> <p><code>git checkout -b myfeature develop</code></p> <p>Then we strongly suggest to configure pre-commit to force linters on every commits you perform:</p> <pre><code># create virtualenv to host pre-commit installation\npython3 -m venv venv\nsource venv/bin/activate\n# from the project base directory\npip install pre-commit\npre-commit install -c .github/.pre-commit-config.yaml\n</code></pre> <p>Remember that whenever you make changes, you need to rebuild the docker image to see the reflected changes.</p>"},{"location":"ThreatPot/Contribute/#note-about-documentation","title":"NOTE about documentation:","text":"<p>If you made any changes to an existing model/serializer/view, please run the following command to generate a new version of the API schema and docs:</p> <pre><code>docker exec -it threatpot_uwsgi python manage.py spectacular --file docs/source/schema.yml &amp;&amp; make html\n</code></pre>"},{"location":"ThreatPot/Contribute/#frontend","title":"Frontend","text":"<p>To start the frontend in \"develop\" mode, you can execute the startup npm script within the folder <code>frontend</code>:</p> <pre><code>cd frontend/\n# Install\nnpm i\n# Start\nDANGEROUSLY_DISABLE_HOST_CHECK=true npm start\n# See https://create-react-app.dev/docs/proxying-api-requests-in-development/#invalid-host-header-errors-after-configuring-proxy for why we use that flag in development mode\n</code></pre> <p>Most of the time you would need to test the changes you made together with the backend. In that case, you would need to run the backend locally too:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"ThreatPot/Contribute/#certego-ui","title":"Certego-UI","text":"<p>The ThreatPot Frontend is tightly linked to the <code>certego-ui</code> library. Most of the React components are imported from there. Because of this, it may happen that, during development, you would need to work on that library too. To install the <code>certego-ui</code> library, please take a look to npm link and remember to start certego-ui without installing peer dependencies (to avoid conflicts with ThreatPot dependencies):</p> <pre><code>git clone https://github.com/certego/certego-ui.git\n# change directory to the folder where you have the cloned the library\ncd certego-ui/\n# install, without peer deps (to use packages of ThreatPot)\nnpm i --legacy-peer-deps\n# create link to the project (this will globally install this package)\nsudo npm link\n# compile the library\nnpm start\n</code></pre> <p>Then, open another command line tab, create a link in the <code>frontend</code> to the <code>certego-ui</code> and re-install and re-start the frontend application (see previous section):</p> <pre><code>cd frontend/\nnpm link @certego/certego-ui\n</code></pre> <p>This trick will allow you to see reflected every changes you make in the <code>certego-ui</code> directly in the running <code>frontend</code> application.</p>"},{"location":"ThreatPot/Contribute/#example-application","title":"Example application","text":"<p>The <code>certego-ui</code> application comes with an example project that showcases the components that you can re-use and import to other projects, like ThreatPot:</p> <pre><code># To have the Example application working correctly, be sure to have installed `certego-ui` *without* the `--legacy-peer-deps` option and having it started in another command line\ncd certego-ui/\nnpm i\nnpm start\n# go to another tab\ncd certego-ui/example/\nnpm i\nnpm start\n</code></pre>"},{"location":"ThreatPot/Contribute/#create-a-pull-request","title":"Create a pull request","text":""},{"location":"ThreatPot/Contribute/#remember","title":"Remember!!!","text":"<p>Please create pull requests only for the branch develop. That code will be pushed to master only on a new release.</p> <p>Also remember to pull the most recent changes available in the develop branch before submitting your PR. If your PR has merge conflicts caused by this behavior, it won't be accepted.</p>"},{"location":"ThreatPot/Contribute/#tests","title":"Tests","text":""},{"location":"ThreatPot/Contribute/#backend","title":"Backend","text":""},{"location":"ThreatPot/Contribute/#install-testing-requirements","title":"Install testing requirements","text":"<p>You have to install <code>pre-commit</code> to have your code adjusted and fixed with the available linters:</p> <pre><code>pip install pre-commit\npre-commit install -c .github/.pre-commit-config.yaml\n</code></pre> <p>Once done that, you won't have to think about linters anymore.</p>"},{"location":"ThreatPot/Contribute/#run-all-tests","title":"Run all tests","text":"<pre><code>docker exec threatpot_uwsgi python3 manage.py test\n</code></pre>"},{"location":"ThreatPot/Contribute/#frontend_1","title":"Frontend","text":"<p>All the frontend tests must be run from the folder <code>frontend</code>. The tests can contain log messages, you can suppress then with the environment variable <code>SUPPRESS_JEST_LOG=True</code>.</p>"},{"location":"ThreatPot/Contribute/#run-all-tests_1","title":"Run all tests","text":"<pre><code>npm test\n</code></pre>"},{"location":"ThreatPot/Contribute/#run-a-specific-component-tests","title":"Run a specific component tests","text":"<pre><code>npm test -- -t &lt;componentPath&gt;\n// example\nnpm test tests/components/auth/Login.test.jsx\n</code></pre>"},{"location":"ThreatPot/Contribute/#run-a-specific-test","title":"Run a specific test","text":"<pre><code>npm test -- -t '&lt;describeString&gt; &lt;testString&gt;'\n// example\nnpm test -- -t \"Login component User login\"\n</code></pre> <p>if you get any errors, fix them. Once you make sure that everything is working fine, please squash all of our commits into a single one and finally create a pull request.</p>"},{"location":"ThreatPot/Installation/","title":"Installation","text":""},{"location":"ThreatPot/Installation/#requirements","title":"Requirements","text":"<p>For requirements, please refer to ThreatMatrix requirements which are the same</p> <p>Note that ThreatPot needs a running instance of ElasticSearch of a T-POT to function. In <code>docker/env_file</code>, set the variable <code>ELASTIC_ENDPOINT</code> with the URL of your Elasticsearch T-POT.</p> <p>If you don't have one, you can make the following changes to make GreeyBear spin up it's own ElasticSearch instance. (...Care! This option would require enough RAM to run the additional containers. Suggested is &gt;=16GB):</p> <ol> <li>In <code>docker/env_file</code>, set the variable <code>ELASTIC_ENDPOINT</code> to <code>http://elasticsearch:9200</code>.</li> <li>Add <code>:docker/elasticsearch.yml</code> to the last defined <code>COMPOSE_FILE</code> variable or uncomment the <code># local development with elasticsearch container</code> block in <code>.env</code> file.</li> </ol>"},{"location":"ThreatPot/Installation/#installation-steps","title":"Installation steps","text":"<p>Start by cloning the project</p> <pre><code># clone the Greedybear project repository\ngit clone https://github.com/honeynet/ThreatPot\ncd ThreatPot/\n\n# construct environment files from templates\ncp .env_template .env\ncd docker/\ncp env_file_template env_file\ncp env_file_postgres_template env_file_postgres\n</code></pre> <p>Now you can start by building the image using docker-compose and run the project.</p> <pre><code># build the image locally\ndocker-compose build\n\n# start the app\ndocker-compose up\n\n# now the app is running on http://localhost:80\n\n# shut down the application\ndocker-compose down\n</code></pre> <p>Note: To create a superuser run the following:</p> <pre><code>docker exec -ti threatpot_uwsgi python3 manage.py createsuperuser\n</code></pre> <p>The app administrator can enable/disable the extraction of source IPs for specific honeypots from the Django Admin. This is used for honeypots that are not specifically implemented to extract additional information (so not Log4Pot and Cowrie).</p>"},{"location":"ThreatPot/Installation/#environment-configuration","title":"Environment configuration","text":"<p>In the <code>env_file</code>, configure different variables as explained below.</p> <p>Required variable to set:</p> <ul> <li><code>DEFAULT_FROM_EMAIL</code>: email address used for automated correspondence from the site manager (example: <code>noreply@mydomain.com</code>)</li> <li><code>DEFAULT_EMAIL</code>: email address used for correspondence with users (example: <code>info@mydomain.com</code>)</li> <li><code>EMAIL_HOST</code>: the host to use for sending email with SMTP</li> <li><code>EMAIL_HOST_USER</code>: username to use for the SMTP server defined in EMAIL_HOST</li> <li><code>EMAIL_HOST_PASSWORD</code>: password to use for the SMTP server defined in EMAIL_HOST. This setting is used in conjunction with EMAIL_HOST_USER when authenticating to the SMTP server.</li> <li><code>EMAIL_PORT</code>: port to use for the SMTP server defined in EMAIL_HOST.</li> <li><code>EMAIL_USE_TLS</code>: whether to use an explicit TLS (secure) connection when talking to the SMTP server, generally used on port 587.</li> <li><code>EMAIL_USE_SSL</code>: whether to use an implicit TLS (secure) connection when talking to the SMTP server, generally used on port 465.</li> </ul> <p>Optional configuration:</p> <ul> <li><code>SLACK_TOKEN</code>: Slack token of your Slack application that will be used to send/receive notifications</li> <li><code>DEFAULT_SLACK_CHANNEL</code>: ID of the Slack channel you want to post the message to</li> </ul>"},{"location":"ThreatPot/Installation/#elasticsearch-compatibility","title":"ElasticSearch compatibility.","text":"<p>Greedybear leverages a python client for interacting with ElasticSearch which requires to be at the exact major version of the related T-POT ElasticSearch instance. This means that there could problems if those versions do not match.</p> <p>The actual version of the client installed is the 8.15.0 which allows to run TPOT version from 22.04.0 to 24.04.0 without any problems (and some later ones...we regularly check T-POT releases but we could miss one or two here.)</p> <p>If you want to have compatibility with previous versions, you need to change the <code>elasticsearch-dsl</code> version here and re-build locally the project.</p>"},{"location":"ThreatPot/Installation/#update-and-re-build","title":"Update and Re-build","text":""},{"location":"ThreatPot/Installation/#rebuilding-the-project-creating-custom-docker-build","title":"Rebuilding the project / Creating custom docker build","text":"<p>If you make some code changes and you like to rebuild the project, follow these steps:</p> <ol> <li>Be sure that your <code>.env</code> file has a <code>COMPOSE_FILE</code> variable which mounts the <code>docker/local.override.yml</code> compose file.</li> <li><code>docker-compose build</code> to build the new docker image.</li> <li>Start the containers with <code>docker-compose up</code>.</li> </ol>"},{"location":"ThreatPot/Installation/#update-to-the-most-recent-version","title":"Update to the most recent version","text":"<p>To update the project with the most recent available code you have to follow these steps:</p> <pre><code>$ cd &lt;your_threat_pot_directory&gt; # go into the project directory\n$ git pull # pull new repository changes\n$ docker pull khulnasoft/threatpot:prod # pull new docker images\n$ docker-compose down # stop and destroy the currently running ThreatPot containers\n$ docker-compose up # restart the ThreatPot application\n</code></pre>"},{"location":"ThreatPot/Introduction/","title":"Introduction","text":"<p> ThreatPot Repository</p>"},{"location":"ThreatPot/Introduction/#introduction","title":"Introduction","text":"<p>The project goal is to extract data of the attacks detected by a TPOT or a cluster of them and to generate some feeds that can be used to prevent and detect attacks.</p> <p>Official announcement here.</p>"},{"location":"ThreatPot/Introduction/#public-feeds","title":"Public feeds","text":"<p>There are public feeds provided by The Honeynet Project in this site: threatpot.honeynet.org. Example</p> <p>To check all the available feeds, Please refer to our usage guide</p> <p>Please do not perform too many requests to extract feeds or you will be banned.</p> <p>If you want to be updated regularly, please download the feeds only once every 10 minutes (this is the time between each internal update).</p>"},{"location":"ThreatPot/Usage/","title":"Usage","text":""},{"location":"ThreatPot/Usage/#user-management","title":"User management","text":""},{"location":"ThreatPot/Usage/#registration","title":"Registration","text":"<p>Since Greedybear v1.1.0 we added a Registration Page that can be used to manage Registration requests when providing ThreatPot as a Service.</p> <p>After an user registration, an email is sent to the user to verify their email address. If necessary, there are buttons on the login page to resend the verification email and to reset the password.</p> <p>Once the user has verified their email, they would be manually vetted before being allowed to use the ThreatPot platform. The registration requests would be handled in the Django Admin page by admins. If you have ThreatPot deployed on an AWS instance you can use the SES service.</p> <p>In a development environment the emails that would be sent are written to the standard output.</p>"},{"location":"ThreatPot/Usage/#amazon-ses","title":"Amazon SES","text":"<p>If you like, you could use Amazon SES for sending automated emails.</p> <p>First, you need to configure the environment variable <code>AWS_SES</code> to <code>True</code> to enable it. Then you have to add some credentials for AWS: if you have ThreatPot deployed on the AWS infrastructure, you can use IAM credentials: to allow that just set <code>AWS_IAM_ACCESS</code> to <code>True</code>. If that is not the case, you have to set both <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>.</p> <p>Additionally, if you are not using the default AWS region of us-east-1, you need to specify your <code>AWS_REGION</code>. You can customize the AWS Region location of you services by changing the environment variable <code>AWS_REGION</code>. Default is <code>eu-central-1</code>.</p>"},{"location":"ThreatPot/Usage/#feeds","title":"Feeds","text":"<p>ThreatPot is created with the aim to collect the information from the TPOTs and generate some actionable feeds, so that they can be easily accessible and act as valuable information to prevent and detect attacks.</p> <p>The feeds are reachable through the following URL:</p> <pre><code>https://&lt;threatpot_site&gt;/api/feeds/&lt;feed_type&gt;/&lt;attack_type&gt;/&lt;age&gt;.&lt;format&gt;\n</code></pre> <p>The available feed_type are:</p> <ul> <li><code>log4j</code>: attacks detected from the Log4pot.</li> <li><code>cowrie</code>: attacks detected from the Cowrie Honeypot.</li> <li><code>all</code>: get all types at once</li> <li>The following honeypot feeds exist (for extraction of (only) the source IPs):<ul> <li><code>heralding</code></li> <li><code>ciscoasa</code></li> <li><code>honeytrap</code></li> <li><code>dionaea</code></li> <li><code>conpot</code></li> <li><code>adbhoney</code></li> <li><code>tanner</code></li> <li><code>citrixhoneypot</code></li> <li><code>mailoney</code></li> <li><code>ipphoney</code></li> <li><code>ddospot</code></li> <li><code>elasticpot</code></li> <li><code>dicompot</code></li> <li><code>redishoneypot</code></li> <li><code>sentrypeer</code></li> <li><code>glutton</code></li> </ul> </li> </ul> <p>The available attack_type are:</p> <ul> <li><code>scanner</code>: IP addresses captured by the honeypots while performing attacks</li> <li><code>payload_request</code>: IP addresses and domains extracted from payloads that would have been executed after a speficic attack would have been successful</li> <li><code>all</code>: get all types at once</li> </ul> <p>The available age are:</p> <ul> <li><code>recent</code>: most recent IOCs seen in the last 3 days</li> <li><code>persistent</code>: these IOCs are the ones that were seen regularly by the honeypots. This feeds will start empty once no prior data was collected and will become bigger over time.</li> </ul> <p>The available formats are:</p> <ul> <li><code>txt</code>: plain text (just one line for each IOC)</li> <li><code>csv</code>: CSV-like file (just one line for each IOC)</li> <li><code>json</code>: JSON file with additional information regarding the IOCs</li> </ul> <p>Check the API specification or the to get all the details about how to use the available APIs.</p>"},{"location":"ThreatPot/Usage/#enrichment","title":"Enrichment","text":"<p>ThreatPot provides an easy-to-query API to get the information available in GB regarding the queried observable (domain or IP address).</p> <pre><code>https://&lt;threatpot_site&gt;/api/enrichment?query=&lt;observable&gt;\n</code></pre> <p>This \"Enrichment\" API is protected through authentication. Please reach out Matteo Lodi or another member of The Honeynet Project if you are interested in gain access to this API.</p> <p>If you would like to leverage this API without the need of writing even a line of code and together with a lot of other awesome tools, consider using ThreatMatrix.</p>"},{"location":"pythreatmatrix/","title":"Quickstart","text":"<p> PyThreatMatrix Repository</p>"},{"location":"pythreatmatrix/#welcome-to-pythreatmatrixs-documentation","title":"Welcome to PyThreatMatrix's documentation!","text":""},{"location":"pythreatmatrix/#robust-python-sdk-and-command-line-client-for-interacting-with-threatmatrix-api","title":"Robust Python SDK and Command Line Client for interacting with ThreatMatrix API.","text":""},{"location":"pythreatmatrix/#installation","title":"Installation","text":"<pre><code>pip install pythreatmatrix\n</code></pre>"},{"location":"pythreatmatrix/#usage-as-cli","title":"Usage as CLI","text":"<pre><code> pythreatmatrix\n Usage: pythreatmatrix [OPTIONS] COMMAND [ARGS]...\n\n Options:\n -d, --debug  Set log level to DEBUG\n --version    Show the version and exit.\n -h, --help   Show this message and exit.\n\n Commands:\n analyse                Send new analysis request\n analyzer-healthcheck   Send healthcheck request for an analyzer...\n config                 Set or view config variables\n connector-healthcheck  Send healthcheck request for a connector\n get-analyzer-config    Get current state of `analyzer_config.json` from...\n get-connector-config   Get current state of `connector_config.json` from...\n get-playbook-config    Get current state of `playbook_config.json` from...\n jobs                   Manage Jobs\n tags                   Manage tags\n</code></pre>"},{"location":"pythreatmatrix/#configuration","title":"Configuration:","text":"<p>You can use <code>set</code> to set the config variables and <code>get</code> to view them.</p> <pre><code>pythreatmatrix config set -k 4bf03f20add626e7138f4023e4cf52b8 -u \"http://localhost:80\"\npythreatmatrix config get\n</code></pre>"},{"location":"pythreatmatrix/#hint","title":"Hint","text":"<p>The CLI would is well-documented which will help you navigate various commands easily. Invoke <code>pythreatmatrix -h</code> or <code>pythreatmatrix &lt;command&gt; -h</code> to get help.</p>"},{"location":"pythreatmatrix/#usage-as-sdklibrary","title":"Usage as SDK/library","text":"<pre><code> from pythreatmatrix import ThreatMatrix, ThreatMatrixClientException\n obj = ThreatMatrix(\n    \"4bf03f20add626e7138f4023e4cf52b8\",\n    \"http://localhost:80\",\n    None,\n )\n \"\"\"\n obj = ThreatMatrix(\n    \"&lt;your_api_key&gt;\",\n    \"&lt;your_threatmatrix_instance_url&gt;\",\n    \"optional&lt;path_to_pem_file&gt;\"\n    \"optional&lt;proxies&gt;\"\n )\n \"\"\"\n\n try:\n    ans = obj.get_analyzer_configs()\n    print(ans)\n except ThreatMatrixClientException as e:\n    print(\"Oh no! Error: \", e)\n</code></pre>"},{"location":"pythreatmatrix/#tip","title":"Tip","text":"<p>We very much recommend going through the :class:<code>pythreatmatrix.pythreatmatrix.ThreatMatrix</code> docs.</p>"},{"location":"pythreatmatrix/#index","title":"Index","text":"<pre><code>.. toctree::\n   :maxdepth: 2\n   :caption: Usage\n\n   pythreatmatrix\n</code></pre> <pre><code>  .. toctree::\n   :maxdepth: 2\n   :caption: Development\n\n   tests\n</code></pre>"},{"location":"pythreatmatrix/Tests/","title":"Tests","text":""},{"location":"pythreatmatrix/Tests/#configuration","title":"Configuration","text":"<p>Some tests require file samples, which can be found in the encrypted folder <code>tests/test_files.zip</code> (password: \"infected\"). Unzip the archive in <code>tests/test_files</code> folder before running the tests.</p> <p>Please remember that these are dangerous malware! They come encrypted and locked for a reason! Do NOT run them unless you are absolutely sure of what you are doing! They are to be used only for launching specific tests that require them (<code>__send_analysis_request</code>)</p> <ul> <li> <p>With the following constants in <code>__init__.py</code>, you can customize your tests:</p> <ul> <li>MOCKING_CONNECTIONS: Mock connections to external API to test functions without a real connection or a valid API Key.</li> </ul> </li> </ul> <ul> <li> <p>If you prefer to use custom inputs for tests, you can change the following constants:</p> <ul> <li>TEST_JOB_ID</li> <li>TEST_HASH</li> <li>TEST_URL</li> <li>TEST_IP</li> <li>TEST_DOMAIN</li> <li>TEST_GENERIC</li> <li>TEST_FILE</li> <li>TEST_FILE_HASH</li> </ul> </li> </ul>"},{"location":"pythreatmatrix/Tests/#launch-tests","title":"Launch Tests","text":"<ul> <li>The test requirements are specified in the <code>test-requirements.txt</code> file. Install them using,</li> </ul> <p>.. code-block:: bash</p> <pre><code>$ pip3 install -r test-requirements.txt\n</code></pre> <ul> <li>Launch the tests using <code>tox</code>:</li> </ul> <p>.. code-block:: bash</p> <pre><code>$ tox\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/","title":"ThreatMatrixClass","text":""},{"location":"pythreatmatrix/ThreatMatrixClass/#threatmatrix-class","title":"ThreatMatrix Class","text":"Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>class ThreatMatrix:\n    logger: logging.Logger\n\n    def __init__(\n        self,\n        token: str,\n        instance_url: str,\n        certificate: str = None,\n        proxies: dict = None,\n        logger: logging.Logger = None,\n        cli: bool = False,\n    ):\n        self.token = token\n        self.instance = instance_url\n        self.certificate = certificate\n        if logger:\n            self.logger = logger\n        else:\n            self.logger = logging.getLogger(__name__)\n        if proxies and not isinstance(proxies, dict):\n            raise TypeError(\"proxies param must be a dictionary\")\n        self.proxies = proxies\n        self.cli = cli\n\n    @property\n    def session(self) -&gt; requests.Session:\n        \"\"\"\n        Internal use only.\n        \"\"\"\n        if not hasattr(self, \"_session\"):\n            session = requests.Session()\n            if self.certificate is not True:\n                session.verify = self.certificate\n            if self.proxies:\n                session.proxies = self.proxies\n            session.headers.update(\n                {\n                    \"Authorization\": f\"Token {self.token}\",\n                    \"User-Agent\": f\"PyThreatMatrix/{__version__}\",\n                }\n            )\n            self._session = session\n\n        return self._session\n\n    def __make_request(\n        self,\n        method: Literal[\"GET\", \"POST\", \"PUT\", \"PATCH\", \"DELETE\"] = \"GET\",\n        *args,\n        **kwargs,\n    ) -&gt; requests.Response:\n        \"\"\"\n        For internal use only.\n        \"\"\"\n        response: requests.Response = None\n        requests_function_map: Dict[str, Callable] = {\n            \"GET\": self.session.get,\n            \"POST\": self.session.post,\n            \"PUT\": self.session.put,\n            \"PATCH\": self.session.patch,\n            \"DELETE\": self.session.delete,\n        }\n        func = requests_function_map.get(method, None)\n        if not func:\n            raise RuntimeError(f\"Unsupported method name: {method}\")\n\n        try:\n            response = func(*args, **kwargs)\n            self.logger.debug(\n                msg=(response.url, response.status_code, response.content)\n            )\n            response.raise_for_status()\n        except Exception as e:\n            raise ThreatMatrixClientException(e, response=response)\n\n        return response\n\n    def ask_analysis_availability(\n        self,\n        md5: str,\n        analyzers: List[str] = None,\n        check_reported_analysis_too: bool = False,\n        minutes_ago: int = None,\n    ) -&gt; Dict:\n        \"\"\"Search for already available analysis.\\n\n        Endpoint: ``/api/ask_analysis_availability``\n\n        Args:\n            md5 (str): md5sum of the observable or file\n            analyzers (List[str], optional):\n            list of analyzers to trigger.\n            Defaults to `None` meaning automatically select all configured analyzers.\n            check_reported_analysis_too (bool, optional):\n            Check against all existing jobs. Defaults to ``False``.\n            minutes_ago (int, optional):\n            number of minutes to check back for analysis.\n            Default is None so the check does not have any time limits.\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        if not analyzers:\n            analyzers = []\n        data = {\"md5\": md5, \"analyzers\": analyzers}\n        if not check_reported_analysis_too:\n            data[\"running_only\"] = True\n        if minutes_ago:\n            data[\"minutes_ago\"] = int(minutes_ago)\n        url = self.instance + \"/api/ask_analysis_availability\"\n        response = self.__make_request(\"POST\", url=url, data=data)\n        answer = response.json()\n        status, job_id = answer.get(\"status\", None), answer.get(\"job_id\", None)\n        # check sanity cases\n        if not status:\n            raise ThreatMatrixClientException(\n                \"API ask_analysis_availability gave result without status ?\"\n                f\" Response: {answer}\"\n            )\n        if status != \"not_available\" and not job_id:\n            raise ThreatMatrixClientException(\n                \"API ask_analysis_availability gave result without job_id ?\"\n                f\" Response: {answer}\"\n            )\n        return answer\n\n    def send_file_analysis_request(\n        self,\n        filename: str,\n        binary: bytes,\n        tlp: TLPType = \"CLEAR\",\n        analyzers_requested: List[str] = None,\n        connectors_requested: List[str] = None,\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n    ) -&gt; Dict:\n        \"\"\"Send analysis request for a file.\\n\n        Endpoint: ``/api/analyze_file``\n\n        Args:\n\n            filename (str):\n                Filename\n            binary (bytes):\n                File contents as bytes\n            analyzers_requested (List[str], optional):\n                List of analyzers to invoke\n                Defaults to ``[]`` i.e. all analyzers.\n            connectors_requested (List[str], optional):\n                List of specific connectors to invoke.\n                Defaults to ``[]`` i.e. all connectors.\n            tlp (str, optional):\n                TLP for the analysis.\n                (options: ``CLEAR, GREEN, AMBER, RED``).\n            runtime_configuration (Dict, optional):\n                Overwrite configuration for analyzers. Defaults to ``{}``.\n            tags_labels (List[str], optional):\n                List of tag labels to assign (creates non-existing tags)\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        try:\n            if not tlp:\n                tlp = \"CLEAR\"\n            if not analyzers_requested:\n                analyzers_requested = []\n            if not connectors_requested:\n                connectors_requested = []\n            if not tags_labels:\n                tags_labels = []\n            if not runtime_configuration:\n                runtime_configuration = {}\n            data = {\n                \"file_name\": filename,\n                \"analyzers_requested\": analyzers_requested,\n                \"connectors_requested\": connectors_requested,\n                \"tlp\": tlp,\n                \"tags_labels\": tags_labels,\n            }\n            if runtime_configuration:\n                data[\"runtime_configuration\"] = json.dumps(runtime_configuration)\n            files = {\"file\": (filename, binary)}\n            answer = self.__send_analysis_request(data=data, files=files)\n        except Exception as e:\n            raise ThreatMatrixClientException(e)\n        return answer\n\n    def send_file_analysis_playbook_request(\n        self,\n        filename: str,\n        binary: bytes,\n        playbook_requested: str,\n        tlp: TLPType = \"CLEAR\",\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n    ) -&gt; Dict:\n        \"\"\"Send playbook analysis request for a file.\\n\n        Endpoint: ``/api/playbook/analyze_multiple_files``\n\n        Args:\n\n            filename (str):\n                Filename\n            binary (bytes):\n                File contents as bytes\n            playbook_requested (str, optional):\n            tlp (str, optional):\n                TLP for the analysis.\n                (options: ``WHITE, GREEN, AMBER, RED``).\n            runtime_configuration (Dict, optional):\n                Overwrite configuration for analyzers. Defaults to ``{}``.\n            tags_labels (List[str], optional):\n                List of tag labels to assign (creates non-existing tags)\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        try:\n            if not tags_labels:\n                tags_labels = []\n            if not runtime_configuration:\n                runtime_configuration = {}\n            data = {\n                \"playbook_requested\": playbook_requested,\n                \"tags_labels\": tags_labels,\n            }\n            # send this value only if populated,\n            # otherwise the backend would give you 400\n            if tlp:\n                data[\"tlp\"] = tlp\n\n            if runtime_configuration:\n                data[\"runtime_configuration\"] = json.dumps(runtime_configuration)\n            # `files` is wanted to be different from the other\n            # /api/analyze_file endpoint\n            # because the server is using different serializers\n            files = {\"files\": (filename, binary)}\n            answer = self.__send_analysis_request(\n                data=data, files=files, playbook_mode=True\n            )\n        except Exception as e:\n            raise ThreatMatrixClientException(e)\n        return answer\n\n    def send_observable_analysis_request(\n        self,\n        observable_name: str,\n        tlp: TLPType = \"CLEAR\",\n        analyzers_requested: List[str] = None,\n        connectors_requested: List[str] = None,\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n        observable_classification: str = None,\n    ) -&gt; Dict:\n        \"\"\"Send analysis request for an observable.\\n\n        Endpoint: ``/api/analyze_observable``\n\n        Args:\n            observable_name (str):\n                Observable value\n            analyzers_requested (List[str], optional):\n                List of analyzers to invoke\n                Defaults to ``[]`` i.e. all analyzers.\n            connectors_requested (List[str], optional):\n                List of specific connectors to invoke.\n                Defaults to ``[]`` i.e. all connectors.\n            tlp (str, optional):\n                TLP for the analysis.\n                (options: ``CLEAR, GREEN, AMBER, RED``).\n            runtime_configuration (Dict, optional):\n                Overwrite configuration for analyzers. Defaults to ``{}``.\n            tags_labels (List[str], optional):\n                List of tag labels to assign (creates non-existing tags)\n            observable_classification (str):\n                Observable classification, Default to None.\n                By default launch analysis with an automatic classification.\n                (options: ``url, domain, hash, ip, generic``)\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n            ThreatMatrixClientException: on wrong observable_classification\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        try:\n            if not tlp:\n                tlp = \"CLEAR\"\n            if not analyzers_requested:\n                analyzers_requested = []\n            if not connectors_requested:\n                connectors_requested = []\n            if not tags_labels:\n                tags_labels = []\n            if not runtime_configuration:\n                runtime_configuration = {}\n            if not observable_classification:\n                observable_classification = self._get_observable_classification(\n                    observable_name\n                )\n            elif observable_classification not in [\n                \"generic\",\n                \"hash\",\n                \"ip\",\n                \"domain\",\n                \"url\",\n            ]:\n                raise ThreatMatrixClientException(\n                    \"Observable classification only handle\"\n                    \" 'generic', 'hash', 'ip', 'domain' and 'url' \"\n                )\n            data = {\n                \"observable_name\": observable_name,\n                \"observable_classification\": observable_classification,\n                \"analyzers_requested\": analyzers_requested,\n                \"connectors_requested\": connectors_requested,\n                \"tlp\": tlp,\n                \"tags_labels\": tags_labels,\n                \"runtime_configuration\": runtime_configuration,\n            }\n            answer = self.__send_analysis_request(data=data, files=None)\n        except Exception as e:\n            raise ThreatMatrixClientException(e)\n        return answer\n\n    def send_observable_analysis_playbook_request(\n        self,\n        observable_name: str,\n        playbook_requested: str,\n        tlp: TLPType = \"CLEAR\",\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n        observable_classification: str = None,\n    ) -&gt; Dict:\n        \"\"\"Send playbook analysis request for an observable.\\n\n        Endpoint: ``/api/playbook/analyze_multiple_observables``\n\n        Args:\n            observable_name (str):\n                Observable value\n            playbook_requested str:\n            tlp (str, optional):\n                TLP for the analysis.\n                (options: ``WHITE, GREEN, AMBER, RED``).\n            runtime_configuration (Dict, optional):\n                Overwrite configuration for analyzers. Defaults to ``{}``.\n            tags_labels (List[str], optional):\n                List of tag labels to assign (creates non-existing tags)\n            observable_classification (str):\n                Observable classification, Default to None.\n                By default launch analysis with an automatic classification.\n                (options: ``url, domain, hash, ip, generic``)\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n            ThreatMatrixClientException: on wrong observable_classification\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        try:\n            if not tags_labels:\n                tags_labels = []\n            if not runtime_configuration:\n                runtime_configuration = {}\n            if not observable_classification:\n                observable_classification = self._get_observable_classification(\n                    observable_name\n                )\n            elif observable_classification not in [\n                \"generic\",\n                \"hash\",\n                \"ip\",\n                \"domain\",\n                \"url\",\n            ]:\n                raise ThreatMatrixClientException(\n                    \"Observable classification only handle\"\n                    \" 'generic', 'hash', 'ip', 'domain' and 'url' \"\n                )\n            data = {\n                \"observables\": [[observable_classification, observable_name]],\n                \"playbook_requested\": playbook_requested,\n                \"tags_labels\": tags_labels,\n                \"runtime_configuration\": runtime_configuration,\n            }\n            # send this value only if populated,\n            # otherwise the backend would give you 400\n            if tlp:\n                data[\"tlp\"] = tlp\n            answer = self.__send_analysis_request(\n                data=data, files=None, playbook_mode=True\n            )\n        except Exception as e:\n            raise ThreatMatrixClientException(e)\n        return answer\n\n    def send_analysis_batch(self, rows: List[Dict]):\n        \"\"\"\n        Send multiple analysis requests.\n        Can be mix of observable or file analysis requests.\n\n        Used by the pythreatmatrix CLI.\n\n        Args:\n            rows (List[Dict]):\n                Each row should be a dictionary with keys,\n                `value`, `type`, `check`, `tlp`,\n                `analyzers_list`, `connectors_list`, `runtime_config`\n                `tags_list`.\n        \"\"\"\n        for obj in rows:\n            try:\n                runtime_config = obj.get(\"runtime_config\", {})\n                if runtime_config:\n                    with open(runtime_config) as fp:\n                        runtime_config = json.load(fp)\n\n                analyzers_list = obj.get(\"analyzers_list\", [])\n                connectors_list = obj.get(\"connectors_list\", [])\n                if isinstance(analyzers_list, str):\n                    analyzers_list = analyzers_list.split(\",\")\n                if isinstance(connectors_list, str):\n                    connectors_list = connectors_list.split(\",\")\n\n                self._new_analysis_cli(\n                    obj[\"value\"],\n                    obj[\"type\"],\n                    obj.get(\"check\", None),\n                    obj.get(\"tlp\", \"WHITE\"),\n                    analyzers_list,\n                    connectors_list,\n                    runtime_config,\n                    obj.get(\"tags_list\", []),\n                    obj.get(\"should_poll\", False),\n                )\n            except ThreatMatrixClientException as e:\n                self.logger.fatal(str(e))\n\n    def __send_analysis_request(self, data=None, files=None, playbook_mode=False):\n        \"\"\"\n        Internal use only.\n        \"\"\"\n        response = None\n        answer = {}\n        if files is None:\n            url = self.instance + \"/api/analyze_observable\"\n            if playbook_mode:\n                url = self.instance + \"/api/playbook/analyze_multiple_observables\"\n            args = {\"json\": data}\n        else:\n            url = self.instance + \"/api/analyze_file\"\n            if playbook_mode:\n                url = self.instance + \"/api/playbook/analyze_multiple_files\"\n            args = {\"data\": data, \"files\": files}\n        try:\n            response = self.session.post(url, **args)\n            self.logger.debug(\n                msg={\n                    \"url\": response.url,\n                    \"code\": response.status_code,\n                    \"request\": response.request.headers,\n                    \"headers\": response.headers,\n                    \"body\": response.json(),\n                }\n            )\n            answer = response.json()\n            if playbook_mode:\n                # right now, we are only supporting single input result\n                answers = answer.get(\"results\", [])\n                if answers:\n                    answer = answers[0]\n\n            warnings = answer.get(\"warnings\", [])\n            errors = answer.get(\"errors\", {})\n            if self.cli:\n                info_log = f\"\"\"New Job running..\n                    ID: {answer.get('job_id')} | \n                    Status: [u blue]{answer.get('status')}[/].\n                    Got {len(warnings)} warnings:\n                    [i yellow]{warnings if warnings else None}[/]\n                    Got {len(errors)} errors:\n                    [i red]{errors if errors else None}[/]\n                \"\"\"\n            else:\n                info_log = (\n                    f\"New Job running.. ID: {answer.get('job_id')} \"\n                    f\"| Status: {answer.get('status')}.\"\n                    f\" Got {len(warnings)} warnings:\"\n                    f\" {warnings if warnings else None}\"\n                    f\" Got {len(errors)} errors:\"\n                    f\" {errors if errors else None}\"\n                )\n            self.logger.info(info_log)\n            response.raise_for_status()\n        except Exception as e:\n            raise ThreatMatrixClientException(e, response=response)\n        return answer\n\n    def create_tag(self, label: str, color: str):\n        \"\"\"Creates new tag by sending a POST Request\n        Endpoint: ``/api/tags``\n\n        Args:\n            label ([str]): [Label of the tag to be created]\n            color ([str]): [Color of the tag to be created]\n        \"\"\"\n        url = self.instance + \"/api/tags\"\n        data = {\"label\": label, \"color\": color}\n        response = self.__make_request(\"POST\", url=url, data=data)\n        return response.json()\n\n    def edit_tag(self, tag_id: Union[int, str], label: str, color: str):\n        \"\"\"Edits existing tag by sending PUT request\n        Endpoint: ``api/tags``\n\n        Args:\n            id ([int]): [Id of the existing tag]\n            label ([str]): [Label of the tag to be created]\n            color ([str]): [Color of the tag to be created]\n        \"\"\"\n        url = self.instance + \"/api/tags/\" + str(tag_id)\n        data = {\"label\": label, \"color\": color}\n        response = self.__make_request(\"PUT\", url=url, data=data)\n        return response.json()\n\n    def get_all_tags(self) -&gt; List[Dict[str, str]]:\n        \"\"\"\n        Fetch list of all tags.\\n\n        Endpoint: ``/api/tags``\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            List[Dict[str, str]]: List of tags\n        \"\"\"\n        url = self.instance + \"/api/tags\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_all_jobs(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Fetch list of all jobs.\\n\n        Endpoint: ``/api/jobs``\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Dict: Dict with 3 keys: \"count\", \"total_pages\", \"results\"\n        \"\"\"\n        url = self.instance + \"/api/jobs\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_tag_by_id(self, tag_id: Union[int, str]) -&gt; Dict[str, str]:\n        \"\"\"Fetch tag info by ID.\\n\n        Endpoint: ``/api/tag/{tag_id}``\n\n        Args:\n            tag_id (Union[int, str]): Tag ID\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, str]: Dict with 3 keys: `id`, `label` and `color`.\n        \"\"\"\n\n        url = self.instance + \"/api/tags/\" + str(tag_id)\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_job_by_id(self, job_id: Union[int, str]) -&gt; Dict[str, Any]:\n        \"\"\"Fetch job info by ID.\n        Endpoint: ``/api/jobs/{job_id}``\n\n        Args:\n            job_id (Union[int, str]): Job ID\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url = self.instance + \"/api/jobs/\" + str(job_id)\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    @staticmethod\n    def get_md5(\n        to_hash: AnyStr,\n        type_=\"observable\",\n    ) -&gt; str:\n        \"\"\"Returns md5sum of given observable or file object.\n\n        Args:\n            to_hash (AnyStr):\n                either an observable string, file contents as bytes or path to a file\n            type_ (Union[\"observable\", \"binary\", \"file\"], optional):\n                `observable`, `binary`, `file`. Defaults to \"observable\".\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            str: md5sum\n        \"\"\"\n        md5 = \"\"\n        if type_ == \"observable\":\n            md5 = hashlib.md5(str(to_hash).lower().encode(\"utf-8\")).hexdigest()\n        elif type_ == \"binary\":\n            md5 = hashlib.md5(to_hash).hexdigest()\n        elif type_ == \"file\":\n            path = pathlib.Path(to_hash)\n            if not path.exists():\n                raise ThreatMatrixClientException(f\"{to_hash} does not exists\")\n            binary = path.read_bytes()\n            md5 = hashlib.md5(binary).hexdigest()\n        return md5\n\n    def _new_analysis_cli(\n        self,\n        obj: str,\n        type_: str,\n        check,\n        tlp: TLPType = None,\n        analyzers_list: List[str] = None,\n        connectors_list: List[str] = None,\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n        should_poll: bool = False,\n        minutes_ago: int = None,\n    ) -&gt; None:\n        \"\"\"\n        For internal use by the pythreatmatrix CLI.\n        \"\"\"\n        if not analyzers_list:\n            analyzers_list = []\n        if not connectors_list:\n            connectors_list = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        if not tags_labels:\n            tags_labels = []\n        self.logger.info(\n            f\"\"\"Requesting analysis..\n            {type_}: [blue]{obj}[/]\n            analyzers: [i green]{analyzers_list if analyzers_list else 'none'}[/]\n            connectors: [i green]{connectors_list if connectors_list else 'none'}[/]\n            tags: [i green]{tags_labels}[/]\n            \"\"\"\n        )\n        # 1st step: ask analysis availability\n        if check != \"force-new\":\n            md5 = self.get_md5(obj, type_=type_)\n\n            resp = self.ask_analysis_availability(\n                md5,\n                analyzers_list,\n                True if check == \"reported\" else False,\n                minutes_ago,\n            )\n            status, job_id = resp.get(\"status\", None), resp.get(\"job_id\", None)\n            if status != \"not_available\":\n                self.logger.info(\n                    f\"\"\"Found existing analysis!\n                Job: #{job_id}\n                status: [u blue]{status}[/]\n\n                [i]Hint: use [#854442]--check force-new[/] to perform new scan anyway[/]\n                    \"\"\"\n                )\n                return\n        # 2nd step: send new analysis request\n        if type_ == \"observable\":\n            resp2 = self.send_observable_analysis_request(\n                observable_name=obj,\n                tlp=tlp,\n                analyzers_requested=analyzers_list,\n                connectors_requested=connectors_list,\n                runtime_configuration=runtime_configuration,\n                tags_labels=tags_labels,\n            )\n        else:\n            path = pathlib.Path(obj)\n            resp2 = self.send_file_analysis_request(\n                filename=path.name,\n                binary=path.read_bytes(),\n                tlp=tlp,\n                analyzers_requested=analyzers_list,\n                connectors_requested=connectors_list,\n                runtime_configuration=runtime_configuration,\n                tags_labels=tags_labels,\n            )\n        # 3rd step: poll for result\n        if should_poll:\n            if resp2[\"status\"] != \"accepted\":\n                self.logger.fatal(\"Can't poll a failed job\")\n            # import poll function\n            from .cli._jobs_utils import _poll_for_job_cli\n\n            job_id = resp2[\"job_id\"]\n            _ = _poll_for_job_cli(self, job_id)\n            self.logger.info(\n                f\"\"\"\n        Polling finished.\n        Execute [i blue]pythreatmatrix jobs view {job_id}[/] to view the result\n                \"\"\"\n            )\n\n    def _new_analysis_playbook_cli(\n        self,\n        obj: str,\n        type_: str,\n        playbook: str,\n        tlp: TLPType = None,\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n        should_poll: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        For internal use by the pythreatmatrix CLI.\n        \"\"\"\n        if not runtime_configuration:\n            runtime_configuration = {}\n        if not tags_labels:\n            tags_labels = []\n\n        self.logger.info(\n            f\"\"\"Requesting analysis..\n            {type_}: [blue]{obj}[/]\n            playbook: [i green]{playbook}[/]\n            tags: [i green]{tags_labels}[/]\n            \"\"\"\n        )\n\n        # 1st step, make request\n        if type_ == \"observable\":\n            resp = self.send_observable_analysis_playbook_request(\n                observable_name=obj,\n                playbook_requested=playbook,\n                tlp=tlp,\n                runtime_configuration=runtime_configuration,\n                tags_labels=tags_labels,\n            )\n        else:\n            path = pathlib.Path(obj)\n            resp = self.send_file_analysis_playbook_request(\n                filename=path.name,\n                binary=path.read_bytes(),\n                playbook_requested=playbook,\n                tlp=tlp,\n                runtime_configuration=runtime_configuration,\n                tags_labels=tags_labels,\n            )\n\n        # 2nd step: poll for result\n        if should_poll:\n            if resp.get(\"status\", \"\") != \"accepted\":\n                self.logger.fatal(\"Can't poll a failed job\")\n            # import poll function\n            from .cli._jobs_utils import _poll_for_job_cli\n\n            job_id = resp.get(\"job_id\", 0)\n            _ = _poll_for_job_cli(self, job_id)\n            self.logger.info(\n                f\"\"\"\n                    Polling finished.\n                    Execute [i blue]pythreatmatrix jobs view {job_id}[/] to view the result\n                \"\"\"\n            )\n\n    def _get_observable_classification(self, value: str) -&gt; str:\n        \"\"\"Returns observable classification for the given value.\\n\n        Only following types are supported:\n        ip, domain, url, hash (md5, sha1, sha256), generic (if no match)\n\n        Args:\n            value (str):\n                observable value\n\n        Raises:\n            ThreatMatrixClientException:\n                if value type is not recognized\n\n        Returns:\n            str: one of `ip`, `url`, `domain`, `hash` or 'generic'.\n        \"\"\"\n        try:\n            ipaddress.ip_address(value)\n        except ValueError:\n            if re.match(\n                r\"^(?:htt|ft|tc)ps?://[a-z\\d-]{1,63}(?:\\.[a-z\\d-]{1,63})+\"\n                r\"(?:/[a-z\\d-]{1,63})*(?:\\.\\w+)?\",\n                value,\n            ):\n                classification = \"url\"\n            elif re.match(r\"^(\\.)?[a-z\\d-]{1,63}(\\.[a-z\\d-]{1,63})+$\", value):\n                classification = \"domain\"\n            elif (\n                re.match(r\"^[a-f\\d]{32}$\", value)\n                or re.match(r\"^[a-f\\d]{40}$\", value)\n                or re.match(r\"^[a-f\\d]{64}$\", value)\n                or re.match(r\"^[A-F\\d]{32}$\", value)\n                or re.match(r\"^[A-F\\d]{40}$\", value)\n                or re.match(r\"^[A-F\\d]{64}$\", value)\n            ):\n                classification = \"hash\"\n            else:\n                classification = \"generic\"\n                self.logger.warning(\n                    \"Couldn't detect observable classification, setting as 'generic'...\"\n                )\n        else:\n            # its a simple IP\n            classification = \"ip\"\n\n        return classification\n\n    def download_sample(self, job_id: int) -&gt; bytes:\n        \"\"\"\n        Download file sample from job.\\n\n        Method: GET\n        Endpoint: ``/api/jobs/{job_id}/download_sample``\n\n        Args:\n            job_id (int):\n                id of job to download sample from\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bytes: Raw file data.\n        \"\"\"\n\n        url = self.instance + f\"/api/jobs/{job_id}/download_sample\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.content\n\n    def kill_running_job(self, job_id: int) -&gt; bool:\n        \"\"\"Send kill_running_job request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/kill``\n\n        Args:\n            job_id (int):\n                id of job to kill\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: killed or not\n        \"\"\"\n\n        url = self.instance + f\"/api/jobs/{job_id}/kill\"\n        response = self.__make_request(\"PATCH\", url=url)\n        killed = response.status_code == 204\n        return killed\n\n    def delete_job_by_id(self, job_id: int) -&gt; bool:\n        \"\"\"Send delete job request.\\n\n        Method: DELETE\n        Endpoint: ``/api/jobs/{job_id}``\n\n        Args:\n            job_id (int):\n                id of job to kill\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: deleted or not\n        \"\"\"\n        url = self.instance + \"/api/jobs/\" + str(job_id)\n        response = self.__make_request(\"DELETE\", url=url)\n        deleted = response.status_code == 204\n        return deleted\n\n    def delete_tag_by_id(self, tag_id: int) -&gt; bool:\n        \"\"\"Send delete tag request.\\n\n        Method: DELETE\n        Endpoint: ``/api/tags/{tag_id}``\n\n        Args:\n            tag_id (int):\n                id of tag to delete\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: deleted or not\n        \"\"\"\n\n        url = self.instance + \"/api/tags/\" + str(tag_id)\n        response = self.__make_request(\"DELETE\", url=url)\n        deleted = response.status_code == 204\n        return deleted\n\n    def __run_plugin_action(\n        self, job_id: int, plugin_type: str, plugin_name: str, plugin_action: str\n    ) -&gt; bool:\n        \"\"\"Internal method for kill/retry for analyzer/connector\"\"\"\n        response = None\n        url = (\n            self.instance\n            + f\"/api/jobs/{job_id}/{plugin_type}/{plugin_name}/{plugin_action}\"\n        )\n        response = self.__make_request(\"PATCH\", url=url)\n        success = response.status_code == 204\n        return success\n\n    def kill_analyzer(self, job_id: int, analyzer_name: str) -&gt; bool:\n        \"\"\"Send kill running/pending analyzer request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/analyzer/{analyzer_name}/kill``\n\n        Args:\n            job_id (int):\n                id of job\n            analyzer_name (str):\n                name of analyzer to kill\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: killed or not\n        \"\"\"\n\n        killed = self.__run_plugin_action(\n            job_id=job_id,\n            plugin_name=analyzer_name,\n            plugin_type=\"analyzer\",\n            plugin_action=\"kill\",\n        )\n        return killed\n\n    def kill_connector(self, job_id: int, connector_name: str) -&gt; bool:\n        \"\"\"Send kill running/pending connector request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/connector/{connector_name}/kill``\n\n        Args:\n            job_id (int):\n                id of job\n            connector_name (str):\n                name of connector to kill\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: killed or not\n        \"\"\"\n\n        killed = self.__run_plugin_action(\n            job_id=job_id,\n            plugin_name=connector_name,\n            plugin_type=\"connector\",\n            plugin_action=\"kill\",\n        )\n        return killed\n\n    def retry_analyzer(self, job_id: int, analyzer_name: str) -&gt; bool:\n        \"\"\"Send retry failed/killed analyzer request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/analyzer/{analyzer_name}/retry``\n\n        Args:\n            job_id (int):\n                id of job\n            analyzer_name (str):\n                name of analyzer to retry\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: success or not\n        \"\"\"\n\n        success = self.__run_plugin_action(\n            job_id=job_id,\n            plugin_name=analyzer_name,\n            plugin_type=\"analyzer\",\n            plugin_action=\"retry\",\n        )\n        return success\n\n    def retry_connector(self, job_id: int, connector_name: str) -&gt; bool:\n        \"\"\"Send retry failed/killed connector request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/connector/{connector_name}/retry``\n\n        Args:\n            job_id (int):\n                id of job\n            connector_name (str):\n                name of connector to retry\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: success or not\n        \"\"\"\n\n        success = self.__run_plugin_action(\n            job_id=job_id,\n            plugin_name=connector_name,\n            plugin_type=\"connector\",\n            plugin_action=\"retry\",\n        )\n        return success\n\n    def analyzer_healthcheck(self, analyzer_name: str) -&gt; Optional[bool]:\n        \"\"\"Send analyzer(docker-based) health check request.\\n\n        Method: GET\n        Endpoint: ``/api/analyzer/{analyzer_name}/healthcheck``\n\n        Args:\n            analyzer_name (str):\n                name of analyzer\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: success or not\n        \"\"\"\n\n        url = self.instance + f\"/api/analyzer/{analyzer_name}/healthcheck\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json().get(\"status\", None)\n\n    def connector_healthcheck(self, connector_name: str) -&gt; Optional[bool]:\n        \"\"\"Send connector health check request.\\n\n        Method: GET\n        Endpoint: ``/api/connector/{connector_name}/healthcheck``\n\n        Args:\n            connector_name (str):\n                name of connector\n\n        Raises:\n            ThreatMatrixClientException: on client/HTTP error\n\n        Returns:\n            Bool: success or not\n        \"\"\"\n        url = self.instance + f\"/api/connector/{connector_name}/healthcheck\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json().get(\"status\", None)\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.session","title":"<code>session: requests.Session</code>  <code>property</code>","text":"<p>Internal use only.</p>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.__make_request","title":"<code>__make_request(method='GET', *args, **kwargs)</code>","text":"<p>For internal use only.</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def __make_request(\n    self,\n    method: Literal[\"GET\", \"POST\", \"PUT\", \"PATCH\", \"DELETE\"] = \"GET\",\n    *args,\n    **kwargs,\n) -&gt; requests.Response:\n    \"\"\"\n    For internal use only.\n    \"\"\"\n    response: requests.Response = None\n    requests_function_map: Dict[str, Callable] = {\n        \"GET\": self.session.get,\n        \"POST\": self.session.post,\n        \"PUT\": self.session.put,\n        \"PATCH\": self.session.patch,\n        \"DELETE\": self.session.delete,\n    }\n    func = requests_function_map.get(method, None)\n    if not func:\n        raise RuntimeError(f\"Unsupported method name: {method}\")\n\n    try:\n        response = func(*args, **kwargs)\n        self.logger.debug(\n            msg=(response.url, response.status_code, response.content)\n        )\n        response.raise_for_status()\n    except Exception as e:\n        raise ThreatMatrixClientException(e, response=response)\n\n    return response\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.__run_plugin_action","title":"<code>__run_plugin_action(job_id, plugin_type, plugin_name, plugin_action)</code>","text":"<p>Internal method for kill/retry for analyzer/connector</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def __run_plugin_action(\n    self, job_id: int, plugin_type: str, plugin_name: str, plugin_action: str\n) -&gt; bool:\n    \"\"\"Internal method for kill/retry for analyzer/connector\"\"\"\n    response = None\n    url = (\n        self.instance\n        + f\"/api/jobs/{job_id}/{plugin_type}/{plugin_name}/{plugin_action}\"\n    )\n    response = self.__make_request(\"PATCH\", url=url)\n    success = response.status_code == 204\n    return success\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.__send_analysis_request","title":"<code>__send_analysis_request(data=None, files=None, playbook_mode=False)</code>","text":"<p>Internal use only.</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def __send_analysis_request(self, data=None, files=None, playbook_mode=False):\n    \"\"\"\n    Internal use only.\n    \"\"\"\n    response = None\n    answer = {}\n    if files is None:\n        url = self.instance + \"/api/analyze_observable\"\n        if playbook_mode:\n            url = self.instance + \"/api/playbook/analyze_multiple_observables\"\n        args = {\"json\": data}\n    else:\n        url = self.instance + \"/api/analyze_file\"\n        if playbook_mode:\n            url = self.instance + \"/api/playbook/analyze_multiple_files\"\n        args = {\"data\": data, \"files\": files}\n    try:\n        response = self.session.post(url, **args)\n        self.logger.debug(\n            msg={\n                \"url\": response.url,\n                \"code\": response.status_code,\n                \"request\": response.request.headers,\n                \"headers\": response.headers,\n                \"body\": response.json(),\n            }\n        )\n        answer = response.json()\n        if playbook_mode:\n            # right now, we are only supporting single input result\n            answers = answer.get(\"results\", [])\n            if answers:\n                answer = answers[0]\n\n        warnings = answer.get(\"warnings\", [])\n        errors = answer.get(\"errors\", {})\n        if self.cli:\n            info_log = f\"\"\"New Job running..\n                ID: {answer.get('job_id')} | \n                Status: [u blue]{answer.get('status')}[/].\n                Got {len(warnings)} warnings:\n                [i yellow]{warnings if warnings else None}[/]\n                Got {len(errors)} errors:\n                [i red]{errors if errors else None}[/]\n            \"\"\"\n        else:\n            info_log = (\n                f\"New Job running.. ID: {answer.get('job_id')} \"\n                f\"| Status: {answer.get('status')}.\"\n                f\" Got {len(warnings)} warnings:\"\n                f\" {warnings if warnings else None}\"\n                f\" Got {len(errors)} errors:\"\n                f\" {errors if errors else None}\"\n            )\n        self.logger.info(info_log)\n        response.raise_for_status()\n    except Exception as e:\n        raise ThreatMatrixClientException(e, response=response)\n    return answer\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.analyzer_healthcheck","title":"<code>analyzer_healthcheck(analyzer_name)</code>","text":"<p>Send analyzer(docker-based) health check request.</p> <p>Method: GET Endpoint: <code>/api/analyzer/{analyzer_name}/healthcheck</code></p> <p>Parameters:</p> Name Type Description Default <code>analyzer_name</code> <code>str</code> <p>name of analyzer</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>Optional[bool]</code> <p>success or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def analyzer_healthcheck(self, analyzer_name: str) -&gt; Optional[bool]:\n    \"\"\"Send analyzer(docker-based) health check request.\\n\n    Method: GET\n    Endpoint: ``/api/analyzer/{analyzer_name}/healthcheck``\n\n    Args:\n        analyzer_name (str):\n            name of analyzer\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: success or not\n    \"\"\"\n\n    url = self.instance + f\"/api/analyzer/{analyzer_name}/healthcheck\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json().get(\"status\", None)\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.ask_analysis_availability","title":"<code>ask_analysis_availability(md5, analyzers=None, check_reported_analysis_too=False, minutes_ago=None)</code>","text":"<p>Search for already available analysis.</p> <p>Endpoint: <code>/api/ask_analysis_availability</code></p> <p>Parameters:</p> Name Type Description Default <code>md5</code> <code>str</code> <p>md5sum of the observable or file</p> required <code>analyzers</code> <code>List[str]</code> <code>None</code> <code>check_reported_analysis_too</code> <code>bool</code> <code>False</code> <code>minutes_ago</code> <code>int</code> <code>None</code> <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def ask_analysis_availability(\n    self,\n    md5: str,\n    analyzers: List[str] = None,\n    check_reported_analysis_too: bool = False,\n    minutes_ago: int = None,\n) -&gt; Dict:\n    \"\"\"Search for already available analysis.\\n\n    Endpoint: ``/api/ask_analysis_availability``\n\n    Args:\n        md5 (str): md5sum of the observable or file\n        analyzers (List[str], optional):\n        list of analyzers to trigger.\n        Defaults to `None` meaning automatically select all configured analyzers.\n        check_reported_analysis_too (bool, optional):\n        Check against all existing jobs. Defaults to ``False``.\n        minutes_ago (int, optional):\n        number of minutes to check back for analysis.\n        Default is None so the check does not have any time limits.\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    if not analyzers:\n        analyzers = []\n    data = {\"md5\": md5, \"analyzers\": analyzers}\n    if not check_reported_analysis_too:\n        data[\"running_only\"] = True\n    if minutes_ago:\n        data[\"minutes_ago\"] = int(minutes_ago)\n    url = self.instance + \"/api/ask_analysis_availability\"\n    response = self.__make_request(\"POST\", url=url, data=data)\n    answer = response.json()\n    status, job_id = answer.get(\"status\", None), answer.get(\"job_id\", None)\n    # check sanity cases\n    if not status:\n        raise ThreatMatrixClientException(\n            \"API ask_analysis_availability gave result without status ?\"\n            f\" Response: {answer}\"\n        )\n    if status != \"not_available\" and not job_id:\n        raise ThreatMatrixClientException(\n            \"API ask_analysis_availability gave result without job_id ?\"\n            f\" Response: {answer}\"\n        )\n    return answer\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.connector_healthcheck","title":"<code>connector_healthcheck(connector_name)</code>","text":"<p>Send connector health check request.</p> <p>Method: GET Endpoint: <code>/api/connector/{connector_name}/healthcheck</code></p> <p>Parameters:</p> Name Type Description Default <code>connector_name</code> <code>str</code> <p>name of connector</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>Optional[bool]</code> <p>success or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def connector_healthcheck(self, connector_name: str) -&gt; Optional[bool]:\n    \"\"\"Send connector health check request.\\n\n    Method: GET\n    Endpoint: ``/api/connector/{connector_name}/healthcheck``\n\n    Args:\n        connector_name (str):\n            name of connector\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: success or not\n    \"\"\"\n    url = self.instance + f\"/api/connector/{connector_name}/healthcheck\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json().get(\"status\", None)\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.create_tag","title":"<code>create_tag(label, color)</code>","text":"<p>Creates new tag by sending a POST Request Endpoint: <code>/api/tags</code></p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>[str]</code> <p>[Label of the tag to be created]</p> required <code>color</code> <code>[str]</code> <p>[Color of the tag to be created]</p> required Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def create_tag(self, label: str, color: str):\n    \"\"\"Creates new tag by sending a POST Request\n    Endpoint: ``/api/tags``\n\n    Args:\n        label ([str]): [Label of the tag to be created]\n        color ([str]): [Color of the tag to be created]\n    \"\"\"\n    url = self.instance + \"/api/tags\"\n    data = {\"label\": label, \"color\": color}\n    response = self.__make_request(\"POST\", url=url, data=data)\n    return response.json()\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.delete_job_by_id","title":"<code>delete_job_by_id(job_id)</code>","text":"<p>Send delete job request.</p> <p>Method: DELETE Endpoint: <code>/api/jobs/{job_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job to kill</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>deleted or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def delete_job_by_id(self, job_id: int) -&gt; bool:\n    \"\"\"Send delete job request.\\n\n    Method: DELETE\n    Endpoint: ``/api/jobs/{job_id}``\n\n    Args:\n        job_id (int):\n            id of job to kill\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: deleted or not\n    \"\"\"\n    url = self.instance + \"/api/jobs/\" + str(job_id)\n    response = self.__make_request(\"DELETE\", url=url)\n    deleted = response.status_code == 204\n    return deleted\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.delete_tag_by_id","title":"<code>delete_tag_by_id(tag_id)</code>","text":"<p>Send delete tag request.</p> <p>Method: DELETE Endpoint: <code>/api/tags/{tag_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>tag_id</code> <code>int</code> <p>id of tag to delete</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>deleted or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def delete_tag_by_id(self, tag_id: int) -&gt; bool:\n    \"\"\"Send delete tag request.\\n\n    Method: DELETE\n    Endpoint: ``/api/tags/{tag_id}``\n\n    Args:\n        tag_id (int):\n            id of tag to delete\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: deleted or not\n    \"\"\"\n\n    url = self.instance + \"/api/tags/\" + str(tag_id)\n    response = self.__make_request(\"DELETE\", url=url)\n    deleted = response.status_code == 204\n    return deleted\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.download_sample","title":"<code>download_sample(job_id)</code>","text":"<p>Download file sample from job.</p> <p>Method: GET Endpoint: <code>/api/jobs/{job_id}/download_sample</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job to download sample from</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bytes</code> <code>bytes</code> <p>Raw file data.</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def download_sample(self, job_id: int) -&gt; bytes:\n    \"\"\"\n    Download file sample from job.\\n\n    Method: GET\n    Endpoint: ``/api/jobs/{job_id}/download_sample``\n\n    Args:\n        job_id (int):\n            id of job to download sample from\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bytes: Raw file data.\n    \"\"\"\n\n    url = self.instance + f\"/api/jobs/{job_id}/download_sample\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.content\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.edit_tag","title":"<code>edit_tag(tag_id, label, color)</code>","text":"<p>Edits existing tag by sending PUT request Endpoint: <code>api/tags</code></p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>[int]</code> <p>[Id of the existing tag]</p> required <code>label</code> <code>[str]</code> <p>[Label of the tag to be created]</p> required <code>color</code> <code>[str]</code> <p>[Color of the tag to be created]</p> required Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def edit_tag(self, tag_id: Union[int, str], label: str, color: str):\n    \"\"\"Edits existing tag by sending PUT request\n    Endpoint: ``api/tags``\n\n    Args:\n        id ([int]): [Id of the existing tag]\n        label ([str]): [Label of the tag to be created]\n        color ([str]): [Color of the tag to be created]\n    \"\"\"\n    url = self.instance + \"/api/tags/\" + str(tag_id)\n    data = {\"label\": label, \"color\": color}\n    response = self.__make_request(\"PUT\", url=url, data=data)\n    return response.json()\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.get_all_jobs","title":"<code>get_all_jobs()</code>","text":"<p>Fetch list of all jobs.</p> <p>Endpoint: <code>/api/jobs</code></p> <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>List[Dict[str, Any]]</code> <p>Dict with 3 keys: \"count\", \"total_pages\", \"results\"</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def get_all_jobs(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Fetch list of all jobs.\\n\n    Endpoint: ``/api/jobs``\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Dict: Dict with 3 keys: \"count\", \"total_pages\", \"results\"\n    \"\"\"\n    url = self.instance + \"/api/jobs\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.get_all_tags","title":"<code>get_all_tags()</code>","text":"<p>Fetch list of all tags.</p> <p>Endpoint: <code>/api/tags</code></p> <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: List of tags</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def get_all_tags(self) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Fetch list of all tags.\\n\n    Endpoint: ``/api/tags``\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        List[Dict[str, str]]: List of tags\n    \"\"\"\n    url = self.instance + \"/api/tags\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.get_job_by_id","title":"<code>get_job_by_id(job_id)</code>","text":"<p>Fetch job info by ID. Endpoint: <code>/api/jobs/{job_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>Union[int, str]</code> <p>Job ID</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def get_job_by_id(self, job_id: Union[int, str]) -&gt; Dict[str, Any]:\n    \"\"\"Fetch job info by ID.\n    Endpoint: ``/api/jobs/{job_id}``\n\n    Args:\n        job_id (Union[int, str]): Job ID\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url = self.instance + \"/api/jobs/\" + str(job_id)\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.get_md5","title":"<code>get_md5(to_hash, type_='observable')</code>  <code>staticmethod</code>","text":"<p>Returns md5sum of given observable or file object.</p> <p>Parameters:</p> Name Type Description Default <code>to_hash</code> <code>AnyStr</code> <p>either an observable string, file contents as bytes or path to a file</p> required <code>type_</code> <code>Union[observable, binary, file]</code> <p><code>observable</code>, <code>binary</code>, <code>file</code>. Defaults to \"observable\".</p> <code>'observable'</code> <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>md5sum</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>@staticmethod\ndef get_md5(\n    to_hash: AnyStr,\n    type_=\"observable\",\n) -&gt; str:\n    \"\"\"Returns md5sum of given observable or file object.\n\n    Args:\n        to_hash (AnyStr):\n            either an observable string, file contents as bytes or path to a file\n        type_ (Union[\"observable\", \"binary\", \"file\"], optional):\n            `observable`, `binary`, `file`. Defaults to \"observable\".\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        str: md5sum\n    \"\"\"\n    md5 = \"\"\n    if type_ == \"observable\":\n        md5 = hashlib.md5(str(to_hash).lower().encode(\"utf-8\")).hexdigest()\n    elif type_ == \"binary\":\n        md5 = hashlib.md5(to_hash).hexdigest()\n    elif type_ == \"file\":\n        path = pathlib.Path(to_hash)\n        if not path.exists():\n            raise ThreatMatrixClientException(f\"{to_hash} does not exists\")\n        binary = path.read_bytes()\n        md5 = hashlib.md5(binary).hexdigest()\n    return md5\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.get_tag_by_id","title":"<code>get_tag_by_id(tag_id)</code>","text":"<p>Fetch tag info by ID.</p> <p>Endpoint: <code>/api/tag/{tag_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>tag_id</code> <code>Union[int, str]</code> <p>Tag ID</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Dict with 3 keys: <code>id</code>, <code>label</code> and <code>color</code>.</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def get_tag_by_id(self, tag_id: Union[int, str]) -&gt; Dict[str, str]:\n    \"\"\"Fetch tag info by ID.\\n\n    Endpoint: ``/api/tag/{tag_id}``\n\n    Args:\n        tag_id (Union[int, str]): Tag ID\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, str]: Dict with 3 keys: `id`, `label` and `color`.\n    \"\"\"\n\n    url = self.instance + \"/api/tags/\" + str(tag_id)\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.kill_analyzer","title":"<code>kill_analyzer(job_id, analyzer_name)</code>","text":"<p>Send kill running/pending analyzer request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/analyzer/{analyzer_name}/kill</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job</p> required <code>analyzer_name</code> <code>str</code> <p>name of analyzer to kill</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>killed or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def kill_analyzer(self, job_id: int, analyzer_name: str) -&gt; bool:\n    \"\"\"Send kill running/pending analyzer request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/analyzer/{analyzer_name}/kill``\n\n    Args:\n        job_id (int):\n            id of job\n        analyzer_name (str):\n            name of analyzer to kill\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: killed or not\n    \"\"\"\n\n    killed = self.__run_plugin_action(\n        job_id=job_id,\n        plugin_name=analyzer_name,\n        plugin_type=\"analyzer\",\n        plugin_action=\"kill\",\n    )\n    return killed\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.kill_connector","title":"<code>kill_connector(job_id, connector_name)</code>","text":"<p>Send kill running/pending connector request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/connector/{connector_name}/kill</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job</p> required <code>connector_name</code> <code>str</code> <p>name of connector to kill</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>killed or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def kill_connector(self, job_id: int, connector_name: str) -&gt; bool:\n    \"\"\"Send kill running/pending connector request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/connector/{connector_name}/kill``\n\n    Args:\n        job_id (int):\n            id of job\n        connector_name (str):\n            name of connector to kill\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: killed or not\n    \"\"\"\n\n    killed = self.__run_plugin_action(\n        job_id=job_id,\n        plugin_name=connector_name,\n        plugin_type=\"connector\",\n        plugin_action=\"kill\",\n    )\n    return killed\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.kill_running_job","title":"<code>kill_running_job(job_id)</code>","text":"<p>Send kill_running_job request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/kill</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job to kill</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>killed or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def kill_running_job(self, job_id: int) -&gt; bool:\n    \"\"\"Send kill_running_job request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/kill``\n\n    Args:\n        job_id (int):\n            id of job to kill\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: killed or not\n    \"\"\"\n\n    url = self.instance + f\"/api/jobs/{job_id}/kill\"\n    response = self.__make_request(\"PATCH\", url=url)\n    killed = response.status_code == 204\n    return killed\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.retry_analyzer","title":"<code>retry_analyzer(job_id, analyzer_name)</code>","text":"<p>Send retry failed/killed analyzer request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/analyzer/{analyzer_name}/retry</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job</p> required <code>analyzer_name</code> <code>str</code> <p>name of analyzer to retry</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>success or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def retry_analyzer(self, job_id: int, analyzer_name: str) -&gt; bool:\n    \"\"\"Send retry failed/killed analyzer request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/analyzer/{analyzer_name}/retry``\n\n    Args:\n        job_id (int):\n            id of job\n        analyzer_name (str):\n            name of analyzer to retry\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: success or not\n    \"\"\"\n\n    success = self.__run_plugin_action(\n        job_id=job_id,\n        plugin_name=analyzer_name,\n        plugin_type=\"analyzer\",\n        plugin_action=\"retry\",\n    )\n    return success\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.retry_connector","title":"<code>retry_connector(job_id, connector_name)</code>","text":"<p>Send retry failed/killed connector request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/connector/{connector_name}/retry</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job</p> required <code>connector_name</code> <code>str</code> <p>name of connector to retry</p> required <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>success or not</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def retry_connector(self, job_id: int, connector_name: str) -&gt; bool:\n    \"\"\"Send retry failed/killed connector request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/connector/{connector_name}/retry``\n\n    Args:\n        job_id (int):\n            id of job\n        connector_name (str):\n            name of connector to retry\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Bool: success or not\n    \"\"\"\n\n    success = self.__run_plugin_action(\n        job_id=job_id,\n        plugin_name=connector_name,\n        plugin_type=\"connector\",\n        plugin_action=\"retry\",\n    )\n    return success\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.send_analysis_batch","title":"<code>send_analysis_batch(rows)</code>","text":"<p>Send multiple analysis requests. Can be mix of observable or file analysis requests.</p> <p>Used by the pythreatmatrix CLI.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>List[Dict]</code> <p>Each row should be a dictionary with keys, <code>value</code>, <code>type</code>, <code>check</code>, <code>tlp</code>, <code>analyzers_list</code>, <code>connectors_list</code>, <code>runtime_config</code> <code>tags_list</code>.</p> required Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def send_analysis_batch(self, rows: List[Dict]):\n    \"\"\"\n    Send multiple analysis requests.\n    Can be mix of observable or file analysis requests.\n\n    Used by the pythreatmatrix CLI.\n\n    Args:\n        rows (List[Dict]):\n            Each row should be a dictionary with keys,\n            `value`, `type`, `check`, `tlp`,\n            `analyzers_list`, `connectors_list`, `runtime_config`\n            `tags_list`.\n    \"\"\"\n    for obj in rows:\n        try:\n            runtime_config = obj.get(\"runtime_config\", {})\n            if runtime_config:\n                with open(runtime_config) as fp:\n                    runtime_config = json.load(fp)\n\n            analyzers_list = obj.get(\"analyzers_list\", [])\n            connectors_list = obj.get(\"connectors_list\", [])\n            if isinstance(analyzers_list, str):\n                analyzers_list = analyzers_list.split(\",\")\n            if isinstance(connectors_list, str):\n                connectors_list = connectors_list.split(\",\")\n\n            self._new_analysis_cli(\n                obj[\"value\"],\n                obj[\"type\"],\n                obj.get(\"check\", None),\n                obj.get(\"tlp\", \"WHITE\"),\n                analyzers_list,\n                connectors_list,\n                runtime_config,\n                obj.get(\"tags_list\", []),\n                obj.get(\"should_poll\", False),\n            )\n        except ThreatMatrixClientException as e:\n            self.logger.fatal(str(e))\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.send_file_analysis_playbook_request","title":"<code>send_file_analysis_playbook_request(filename, binary, playbook_requested, tlp='CLEAR', runtime_configuration=None, tags_labels=None)</code>","text":"<p>Send playbook analysis request for a file.</p> <p>Endpoint: <code>/api/playbook/analyze_multiple_files</code></p> <p>Args:</p> <pre><code>filename (str):\n    Filename\nbinary (bytes):\n    File contents as bytes\nplaybook_requested (str, optional):\ntlp (str, optional):\n    TLP for the analysis.\n    (options: ``WHITE, GREEN, AMBER, RED``).\nruntime_configuration (Dict, optional):\n    Overwrite configuration for analyzers. Defaults to ``{}``.\ntags_labels (List[str], optional):\n    List of tag labels to assign (creates non-existing tags)\n</code></pre> <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def send_file_analysis_playbook_request(\n    self,\n    filename: str,\n    binary: bytes,\n    playbook_requested: str,\n    tlp: TLPType = \"CLEAR\",\n    runtime_configuration: Dict = None,\n    tags_labels: List[str] = None,\n) -&gt; Dict:\n    \"\"\"Send playbook analysis request for a file.\\n\n    Endpoint: ``/api/playbook/analyze_multiple_files``\n\n    Args:\n\n        filename (str):\n            Filename\n        binary (bytes):\n            File contents as bytes\n        playbook_requested (str, optional):\n        tlp (str, optional):\n            TLP for the analysis.\n            (options: ``WHITE, GREEN, AMBER, RED``).\n        runtime_configuration (Dict, optional):\n            Overwrite configuration for analyzers. Defaults to ``{}``.\n        tags_labels (List[str], optional):\n            List of tag labels to assign (creates non-existing tags)\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    try:\n        if not tags_labels:\n            tags_labels = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        data = {\n            \"playbook_requested\": playbook_requested,\n            \"tags_labels\": tags_labels,\n        }\n        # send this value only if populated,\n        # otherwise the backend would give you 400\n        if tlp:\n            data[\"tlp\"] = tlp\n\n        if runtime_configuration:\n            data[\"runtime_configuration\"] = json.dumps(runtime_configuration)\n        # `files` is wanted to be different from the other\n        # /api/analyze_file endpoint\n        # because the server is using different serializers\n        files = {\"files\": (filename, binary)}\n        answer = self.__send_analysis_request(\n            data=data, files=files, playbook_mode=True\n        )\n    except Exception as e:\n        raise ThreatMatrixClientException(e)\n    return answer\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.send_file_analysis_request","title":"<code>send_file_analysis_request(filename, binary, tlp='CLEAR', analyzers_requested=None, connectors_requested=None, runtime_configuration=None, tags_labels=None)</code>","text":"<p>Send analysis request for a file.</p> <p>Endpoint: <code>/api/analyze_file</code></p> <p>Args:</p> <pre><code>filename (str):\n    Filename\nbinary (bytes):\n    File contents as bytes\nanalyzers_requested (List[str], optional):\n    List of analyzers to invoke\n    Defaults to ``[]`` i.e. all analyzers.\nconnectors_requested (List[str], optional):\n    List of specific connectors to invoke.\n    Defaults to ``[]`` i.e. all connectors.\ntlp (str, optional):\n    TLP for the analysis.\n    (options: ``CLEAR, GREEN, AMBER, RED``).\nruntime_configuration (Dict, optional):\n    Overwrite configuration for analyzers. Defaults to ``{}``.\ntags_labels (List[str], optional):\n    List of tag labels to assign (creates non-existing tags)\n</code></pre> <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def send_file_analysis_request(\n    self,\n    filename: str,\n    binary: bytes,\n    tlp: TLPType = \"CLEAR\",\n    analyzers_requested: List[str] = None,\n    connectors_requested: List[str] = None,\n    runtime_configuration: Dict = None,\n    tags_labels: List[str] = None,\n) -&gt; Dict:\n    \"\"\"Send analysis request for a file.\\n\n    Endpoint: ``/api/analyze_file``\n\n    Args:\n\n        filename (str):\n            Filename\n        binary (bytes):\n            File contents as bytes\n        analyzers_requested (List[str], optional):\n            List of analyzers to invoke\n            Defaults to ``[]`` i.e. all analyzers.\n        connectors_requested (List[str], optional):\n            List of specific connectors to invoke.\n            Defaults to ``[]`` i.e. all connectors.\n        tlp (str, optional):\n            TLP for the analysis.\n            (options: ``CLEAR, GREEN, AMBER, RED``).\n        runtime_configuration (Dict, optional):\n            Overwrite configuration for analyzers. Defaults to ``{}``.\n        tags_labels (List[str], optional):\n            List of tag labels to assign (creates non-existing tags)\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    try:\n        if not tlp:\n            tlp = \"CLEAR\"\n        if not analyzers_requested:\n            analyzers_requested = []\n        if not connectors_requested:\n            connectors_requested = []\n        if not tags_labels:\n            tags_labels = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        data = {\n            \"file_name\": filename,\n            \"analyzers_requested\": analyzers_requested,\n            \"connectors_requested\": connectors_requested,\n            \"tlp\": tlp,\n            \"tags_labels\": tags_labels,\n        }\n        if runtime_configuration:\n            data[\"runtime_configuration\"] = json.dumps(runtime_configuration)\n        files = {\"file\": (filename, binary)}\n        answer = self.__send_analysis_request(data=data, files=files)\n    except Exception as e:\n        raise ThreatMatrixClientException(e)\n    return answer\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.send_observable_analysis_playbook_request","title":"<code>send_observable_analysis_playbook_request(observable_name, playbook_requested, tlp='CLEAR', runtime_configuration=None, tags_labels=None, observable_classification=None)</code>","text":"<p>Send playbook analysis request for an observable.</p> <p>Endpoint: <code>/api/playbook/analyze_multiple_observables</code></p> <p>Parameters:</p> Name Type Description Default <code>observable_name</code> <code>str</code> <p>Observable value</p> required <code>playbook_requested</code> <code>str</code> required <code>tlp</code> <code>str</code> <p>TLP for the analysis. (options: <code>WHITE, GREEN, AMBER, RED</code>).</p> <code>'CLEAR'</code> <code>runtime_configuration</code> <code>Dict</code> <p>Overwrite configuration for analyzers. Defaults to <code>{}</code>.</p> <code>None</code> <code>tags_labels</code> <code>List[str]</code> <p>List of tag labels to assign (creates non-existing tags)</p> <code>None</code> <code>observable_classification</code> <code>str</code> <p>Observable classification, Default to None. By default launch analysis with an automatic classification. (options: <code>url, domain, hash, ip, generic</code>)</p> <code>None</code> <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <code>ThreatMatrixClientException</code> <p>on wrong observable_classification</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def send_observable_analysis_playbook_request(\n    self,\n    observable_name: str,\n    playbook_requested: str,\n    tlp: TLPType = \"CLEAR\",\n    runtime_configuration: Dict = None,\n    tags_labels: List[str] = None,\n    observable_classification: str = None,\n) -&gt; Dict:\n    \"\"\"Send playbook analysis request for an observable.\\n\n    Endpoint: ``/api/playbook/analyze_multiple_observables``\n\n    Args:\n        observable_name (str):\n            Observable value\n        playbook_requested str:\n        tlp (str, optional):\n            TLP for the analysis.\n            (options: ``WHITE, GREEN, AMBER, RED``).\n        runtime_configuration (Dict, optional):\n            Overwrite configuration for analyzers. Defaults to ``{}``.\n        tags_labels (List[str], optional):\n            List of tag labels to assign (creates non-existing tags)\n        observable_classification (str):\n            Observable classification, Default to None.\n            By default launch analysis with an automatic classification.\n            (options: ``url, domain, hash, ip, generic``)\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n        ThreatMatrixClientException: on wrong observable_classification\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    try:\n        if not tags_labels:\n            tags_labels = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        if not observable_classification:\n            observable_classification = self._get_observable_classification(\n                observable_name\n            )\n        elif observable_classification not in [\n            \"generic\",\n            \"hash\",\n            \"ip\",\n            \"domain\",\n            \"url\",\n        ]:\n            raise ThreatMatrixClientException(\n                \"Observable classification only handle\"\n                \" 'generic', 'hash', 'ip', 'domain' and 'url' \"\n            )\n        data = {\n            \"observables\": [[observable_classification, observable_name]],\n            \"playbook_requested\": playbook_requested,\n            \"tags_labels\": tags_labels,\n            \"runtime_configuration\": runtime_configuration,\n        }\n        # send this value only if populated,\n        # otherwise the backend would give you 400\n        if tlp:\n            data[\"tlp\"] = tlp\n        answer = self.__send_analysis_request(\n            data=data, files=None, playbook_mode=True\n        )\n    except Exception as e:\n        raise ThreatMatrixClientException(e)\n    return answer\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClass/#docs.Submodules.pythreatmatrix.pythreatmatrix.ThreatMatrix.send_observable_analysis_request","title":"<code>send_observable_analysis_request(observable_name, tlp='CLEAR', analyzers_requested=None, connectors_requested=None, runtime_configuration=None, tags_labels=None, observable_classification=None)</code>","text":"<p>Send analysis request for an observable.</p> <p>Endpoint: <code>/api/analyze_observable</code></p> <p>Parameters:</p> Name Type Description Default <code>observable_name</code> <code>str</code> <p>Observable value</p> required <code>analyzers_requested</code> <code>List[str]</code> <p>List of analyzers to invoke Defaults to <code>[]</code> i.e. all analyzers.</p> <code>None</code> <code>connectors_requested</code> <code>List[str]</code> <p>List of specific connectors to invoke. Defaults to <code>[]</code> i.e. all connectors.</p> <code>None</code> <code>tlp</code> <code>str</code> <p>TLP for the analysis. (options: <code>CLEAR, GREEN, AMBER, RED</code>).</p> <code>'CLEAR'</code> <code>runtime_configuration</code> <code>Dict</code> <p>Overwrite configuration for analyzers. Defaults to <code>{}</code>.</p> <code>None</code> <code>tags_labels</code> <code>List[str]</code> <p>List of tag labels to assign (creates non-existing tags)</p> <code>None</code> <code>observable_classification</code> <code>str</code> <p>Observable classification, Default to None. By default launch analysis with an automatic classification. (options: <code>url, domain, hash, ip, generic</code>)</p> <code>None</code> <p>Raises:</p> Type Description <code>ThreatMatrixClientException</code> <p>on client/HTTP error</p> <code>ThreatMatrixClientException</code> <p>on wrong observable_classification</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/pythreatmatrix.py</code> <pre><code>def send_observable_analysis_request(\n    self,\n    observable_name: str,\n    tlp: TLPType = \"CLEAR\",\n    analyzers_requested: List[str] = None,\n    connectors_requested: List[str] = None,\n    runtime_configuration: Dict = None,\n    tags_labels: List[str] = None,\n    observable_classification: str = None,\n) -&gt; Dict:\n    \"\"\"Send analysis request for an observable.\\n\n    Endpoint: ``/api/analyze_observable``\n\n    Args:\n        observable_name (str):\n            Observable value\n        analyzers_requested (List[str], optional):\n            List of analyzers to invoke\n            Defaults to ``[]`` i.e. all analyzers.\n        connectors_requested (List[str], optional):\n            List of specific connectors to invoke.\n            Defaults to ``[]`` i.e. all connectors.\n        tlp (str, optional):\n            TLP for the analysis.\n            (options: ``CLEAR, GREEN, AMBER, RED``).\n        runtime_configuration (Dict, optional):\n            Overwrite configuration for analyzers. Defaults to ``{}``.\n        tags_labels (List[str], optional):\n            List of tag labels to assign (creates non-existing tags)\n        observable_classification (str):\n            Observable classification, Default to None.\n            By default launch analysis with an automatic classification.\n            (options: ``url, domain, hash, ip, generic``)\n\n    Raises:\n        ThreatMatrixClientException: on client/HTTP error\n        ThreatMatrixClientException: on wrong observable_classification\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    try:\n        if not tlp:\n            tlp = \"CLEAR\"\n        if not analyzers_requested:\n            analyzers_requested = []\n        if not connectors_requested:\n            connectors_requested = []\n        if not tags_labels:\n            tags_labels = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        if not observable_classification:\n            observable_classification = self._get_observable_classification(\n                observable_name\n            )\n        elif observable_classification not in [\n            \"generic\",\n            \"hash\",\n            \"ip\",\n            \"domain\",\n            \"url\",\n        ]:\n            raise ThreatMatrixClientException(\n                \"Observable classification only handle\"\n                \" 'generic', 'hash', 'ip', 'domain' and 'url' \"\n            )\n        data = {\n            \"observable_name\": observable_name,\n            \"observable_classification\": observable_classification,\n            \"analyzers_requested\": analyzers_requested,\n            \"connectors_requested\": connectors_requested,\n            \"tlp\": tlp,\n            \"tags_labels\": tags_labels,\n            \"runtime_configuration\": runtime_configuration,\n        }\n        answer = self.__send_analysis_request(data=data, files=None)\n    except Exception as e:\n        raise ThreatMatrixClientException(e)\n    return answer\n</code></pre>"},{"location":"pythreatmatrix/ThreatMatrixClientException/","title":"ThreatMatrixClientException","text":""},{"location":"pythreatmatrix/ThreatMatrixClientException/#threatmatrixclientexception-class","title":"ThreatMatrixClientException Class","text":"<p>               Bases: <code>RequestException</code></p> Source code in <code>docs/Submodules/pythreatmatrix/pythreatmatrix/exceptions.py</code> <pre><code>class ThreatMatrixClientException(RequestException):\n    @property\n    def error_detail(self) -&gt; typing.Union[typing.Dict, typing.AnyStr]:\n        content = None\n        try:\n            content = self.response.json()\n            detail = content.get(\"detail\", None)\n            if detail:\n                content = detail\n        except json.JSONDecodeError:\n            content = self.response.content\n        except Exception:\n            pass\n\n        return content\n\n    def __str__(self):\n        err_msg = super().__str__()\n        detail = self.error_detail\n        return err_msg + f\". Details: {detail}\"\n</code></pre>"}]}